{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d448e8dd-54df-457a-b0e6-a08d27d18317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.float16, attn_implementation='sdpa').cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4bbafda-78b6-42b1-afd5-b51d1ca67c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b56af5e8-c81e-4b5a-9067-0cfe11216980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17256 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17256])\n"
     ]
    }
   ],
   "source": [
    "sample_text = open(\"snapkv.txt\", \"r\").read()\n",
    "encoded_tokens = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "for key in encoded_tokens:\n",
    "    encoded_tokens[key] = encoded_tokens[key].cuda()\n",
    "print(encoded_tokens.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4831a57a-0037-41a5-acda-4107cc712580",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_window = 48\n",
    "template_window = 48\n",
    "max_tokens = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c32c0de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# perform qk calculation and get indices\n",
    "# this version will not update in inference mode\n",
    "class SnapKVCluster():\n",
    "    def __init__(self, query_window = 64, template_window = 64, max_capacity_prompt = 256 + 64, kernel_size = 5, pooling = 'avgpool'):\n",
    "        self.query_window = query_window\n",
    "        self.template_window = template_window\n",
    "        self.max_capacity_prompt = max_capacity_prompt\n",
    "        assert self.max_capacity_prompt - self.query_window > 0\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def update_kv(self, key_states, query_states, value_states, attention_mask, num_key_value_groups):\n",
    "        # check if prefix phase\n",
    "        assert key_states.shape[-2] == query_states.shape[-2]\n",
    "        bsz, num_heads, q_len, head_dim = query_states.shape\n",
    "        if q_len < self.max_capacity_prompt:\n",
    "            return key_states, value_states\n",
    "        else:\n",
    "            attn_weights = torch.matmul(query_states[..., -self.query_window:, :], key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "            mask = torch.full((self.query_window, self.query_window), torch.finfo(attn_weights.dtype).min, device=attn_weights.device)\n",
    "            mask_cond = torch.arange(mask.size(-1), device=attn_weights.device)\n",
    "            mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "            mask = mask.to(attn_weights.device)\n",
    "            attention_mask = mask[None, None, :, :]\n",
    "\n",
    "            attn_weights[:, :, -self.query_window:, -self.query_window:] += attention_mask\n",
    "            attn_weights = torch.where(torch.isinf(attn_weights), torch.tensor(-1e9, dtype=attn_weights.dtype), attn_weights)\n",
    "            # print(\"Weights before softmax: \", attn_weights)\n",
    "            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "            attn_weights_sum = attn_weights[:, :, -self.query_window:, :-self.query_window].sum(dim = -2)\n",
    "            if self.pooling == 'avgpool':\n",
    "                attn_cache = F.avg_pool1d(attn_weights_sum, kernel_size = self.kernel_size, padding=self.kernel_size//2, stride=1)\n",
    "            elif self.pooling == 'maxpool':\n",
    "                attn_cache = F.max_pool1d(attn_weights_sum, kernel_size = self.kernel_size, padding=self.kernel_size//2, stride=1)\n",
    "            else:\n",
    "                raise ValueError('Pooling method not supported')\n",
    "            return attn_cache\n",
    "\n",
    "snapkv_cluster = SnapKVCluster(query_window=query_window, template_window=template_window, max_capacity_prompt=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "224651ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "747fb760-2028-4933-b5ed-1719bac27b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from transformers.cache_utils import Cache\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, repeat_kv\n",
    "\n",
    "def sdpa_forward(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_value: Optional[Cache] = None,\n",
    "    output_attentions: bool = False,\n",
    "    use_cache: bool = False,\n",
    ") -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "    # print(\"Running custom forward function\")\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "    query_states = self.q_proj(hidden_states)\n",
    "    key_states = self.k_proj(hidden_states)\n",
    "    value_states = self.v_proj(hidden_states)\n",
    "\n",
    "    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    kv_seq_len = key_states.shape[-2]\n",
    "    if past_key_value is not None:\n",
    "        kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n",
    "    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
    "\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "\n",
    "    if past_key_value is not None:\n",
    "        cache_kwargs = {\"sin\": sin, \"cos\": cos}  # Specific to RoPE models\n",
    "        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "    key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "    value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "    if attention_mask is not None:\n",
    "        if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
    "            )\n",
    "\n",
    "    # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n",
    "    # Reference: https://github.com/pytorch/pytorch/issues/112577.\n",
    "    if query_states.device.type == \"cuda\" and attention_mask is not None:\n",
    "        query_states = query_states.contiguous()\n",
    "        key_states = key_states.contiguous()\n",
    "        value_states = value_states.contiguous()\n",
    "\n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query_states,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        attn_mask=attention_mask,\n",
    "        dropout_p=self.attention_dropout if self.training else 0.0,\n",
    "        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "        is_causal=self.is_causal and attention_mask is None and q_len > 1,\n",
    "    )\n",
    "\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
    "\n",
    "    attn_output = self.o_proj(attn_output)\n",
    "\n",
    "    attn_cache = snapkv_cluster.update_kv(key_states, query_states, value_states, attention_mask, self.num_key_value_groups)\n",
    "\n",
    "    return attn_output, attn_cache, past_key_value\n",
    "\n",
    "for i in range(len(model.model.layers)):\n",
    "    model.model.layers[i].self_attn.forward = sdpa_forward.__get__(model.model.layers[i].self_attn, type(model.model.layers[i].self_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4794931b-a04c-4e8c-b6f5-08d1b5e540fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding tokens took:  0.019971504982095212\n",
      "Encoded tokens shape:  torch.Size([17256])\n",
      "tensor([   70, 13842,   281,  ...,  2526,  5605, 19926])\n",
      "Last chunk size:  1904\n",
      "Query tokens shape:  torch.Size([48])\n",
      "Chunks shape:  [torch.Size([1907]), torch.Size([1907]), torch.Size([1907]), torch.Size([1907]), torch.Size([1907]), torch.Size([1907]), torch.Size([1907]), torch.Size([1907]), torch.Size([1904])]\n",
      "Template tokens shape:  torch.Size([48])\n",
      "Generated padded tensors:  torch.Size([9, 2048])  and attention masks:  torch.Size([9, 2048])  in  0.02226422500098124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output key:  logits\n",
      "Output key:  past_key_values\n",
      "Output key:  attentions\n",
      "Forward pass completed in:  0.6381797170033678\n",
      "Output attentions:  (tensor([[[0.0324, 0.0432, 0.0540,  ..., 0.1277, 0.0955, 0.0791],\n",
      "         [0.0220, 0.0291, 0.0361,  ..., 0.3423, 0.2666, 0.2192],\n",
      "         [0.0331, 0.0442, 0.0553,  ..., 0.1398, 0.0977, 0.0832],\n",
      "         ...,\n",
      "         [0.0299, 0.0401, 0.0504,  ..., 0.0488, 0.0367, 0.0331],\n",
      "         [0.0175, 0.0239, 0.0304,  ..., 0.2347, 0.2291, 0.1967],\n",
      "         [0.0235, 0.0313, 0.0389,  ..., 0.0786, 0.0652, 0.0571]],\n",
      "\n",
      "        [[0.0327, 0.0435, 0.0544,  ..., 0.1050, 0.0772, 0.0588],\n",
      "         [0.0225, 0.0297, 0.0369,  ..., 0.2961, 0.2368, 0.1899],\n",
      "         [0.0338, 0.0451, 0.0564,  ..., 0.1066, 0.0730, 0.0557],\n",
      "         ...,\n",
      "         [0.0302, 0.0406, 0.0510,  ..., 0.0500, 0.0373, 0.0290],\n",
      "         [0.0183, 0.0251, 0.0320,  ..., 0.0343, 0.0284, 0.0217],\n",
      "         [0.0242, 0.0323, 0.0401,  ..., 0.0793, 0.0619, 0.0465]],\n",
      "\n",
      "        [[0.0331, 0.0442, 0.0552,  ..., 0.1338, 0.1087, 0.0711],\n",
      "         [0.0229, 0.0303, 0.0376,  ..., 0.4133, 0.3621, 0.2715],\n",
      "         [0.0348, 0.0464, 0.0581,  ..., 0.1578, 0.1250, 0.0701],\n",
      "         ...,\n",
      "         [0.0309, 0.0415, 0.0522,  ..., 0.0480, 0.0364, 0.0201],\n",
      "         [0.0181, 0.0248, 0.0316,  ..., 0.0977, 0.0917, 0.0854],\n",
      "         [0.0245, 0.0326, 0.0405,  ..., 0.0684, 0.0523, 0.0324]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0397, 0.0529, 0.0661,  ..., 0.1801, 0.1427, 0.1050],\n",
      "         [0.0258, 0.0341, 0.0424,  ..., 0.5967, 0.5181, 0.4302],\n",
      "         [0.0443, 0.0592, 0.0740,  ..., 0.1724, 0.1281, 0.0934],\n",
      "         ...,\n",
      "         [0.0322, 0.0432, 0.0544,  ..., 0.0476, 0.0363, 0.0288],\n",
      "         [0.0196, 0.0269, 0.0345,  ..., 0.0648, 0.0584, 0.0503],\n",
      "         [0.0245, 0.0326, 0.0405,  ..., 0.0784, 0.0627, 0.0519]],\n",
      "\n",
      "        [[0.0339, 0.0452, 0.0565,  ..., 0.2228, 0.1791, 0.1487],\n",
      "         [0.0227, 0.0300, 0.0372,  ..., 0.6426, 0.5596, 0.4851],\n",
      "         [0.0364, 0.0486, 0.0609,  ..., 0.1475, 0.0950, 0.0538],\n",
      "         ...,\n",
      "         [0.0304, 0.0409, 0.0514,  ..., 0.0532, 0.0321, 0.0159],\n",
      "         [0.0180, 0.0247, 0.0315,  ..., 0.0596, 0.0521, 0.0462],\n",
      "         [0.0237, 0.0316, 0.0392,  ..., 0.0748, 0.0506, 0.0322]],\n",
      "\n",
      "        [[0.0334, 0.0444, 0.0555,  ..., 0.1797, 0.1578, 0.1260],\n",
      "         [0.0223, 0.0294, 0.0365,  ..., 0.4878, 0.4375, 0.3499],\n",
      "         [0.0337, 0.0450, 0.0563,  ..., 0.1932, 0.1771, 0.1405],\n",
      "         ...,\n",
      "         [0.0297, 0.0398, 0.0501,  ..., 0.0544, 0.0505, 0.0389],\n",
      "         [0.0173, 0.0237, 0.0302,  ..., 0.2069, 0.0278, 0.0198],\n",
      "         [0.0236, 0.0314, 0.0390,  ..., 0.0913, 0.0698, 0.0531]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward1>), tensor([[[8.2970e-04, 1.2550e-03, 1.8377e-03,  ..., 3.2935e-01,\n",
      "          3.1592e-01, 2.8906e-01],\n",
      "         [1.4076e-03, 1.7662e-03, 2.0676e-03,  ..., 5.1514e-01,\n",
      "          4.4360e-01, 3.7207e-01],\n",
      "         [1.3852e-04, 1.8132e-04, 2.1887e-04,  ..., 4.7705e-01,\n",
      "          4.5972e-01, 3.9844e-01],\n",
      "         ...,\n",
      "         [2.0275e-03, 2.7409e-03, 3.5038e-03,  ..., 3.9282e-01,\n",
      "          3.4546e-01, 1.7859e-01],\n",
      "         [1.5235e-04, 2.2006e-04, 2.4629e-04,  ..., 2.0312e-01,\n",
      "          1.9116e-01, 1.3513e-01],\n",
      "         [1.4472e-04, 2.6131e-04, 4.4513e-04,  ..., 2.0349e-01,\n",
      "          1.9934e-01, 1.7908e-01]],\n",
      "\n",
      "        [[8.8501e-04, 1.3647e-03, 2.0428e-03,  ..., 7.0679e-02,\n",
      "          5.9296e-02, 4.9591e-02],\n",
      "         [1.4639e-03, 1.8463e-03, 2.1706e-03,  ..., 2.4475e-01,\n",
      "          1.9116e-01, 1.5210e-01],\n",
      "         [1.5068e-04, 1.9729e-04, 2.3830e-04,  ..., 6.9763e-02,\n",
      "          5.9662e-02, 4.3762e-02],\n",
      "         ...,\n",
      "         [2.2240e-03, 3.0098e-03, 3.8471e-03,  ..., 1.1749e-01,\n",
      "          9.9487e-02, 7.3120e-02],\n",
      "         [1.3566e-04, 2.0182e-04, 2.2984e-04,  ..., 1.3196e-01,\n",
      "          1.2598e-01, 1.1969e-01],\n",
      "         [1.4329e-04, 3.0708e-04, 5.5599e-04,  ..., 1.7261e-01,\n",
      "          1.6772e-01, 1.5906e-01]],\n",
      "\n",
      "        [[8.9407e-04, 1.3714e-03, 2.0370e-03,  ..., 1.3354e-01,\n",
      "          1.2524e-01, 1.1127e-01],\n",
      "         [1.4896e-03, 1.8749e-03, 2.1992e-03,  ..., 4.0845e-01,\n",
      "          3.5083e-01, 2.6196e-01],\n",
      "         [1.4925e-04, 1.9562e-04, 2.3651e-04,  ..., 1.4587e-01,\n",
      "          1.3489e-01, 1.1456e-01],\n",
      "         ...,\n",
      "         [2.3994e-03, 3.2482e-03, 4.1542e-03,  ..., 3.3838e-01,\n",
      "          3.0273e-01, 2.5122e-01],\n",
      "         [1.4853e-04, 2.1899e-04, 2.4748e-04,  ..., 1.9861e-01,\n",
      "          1.8982e-01, 1.7773e-01],\n",
      "         [1.5056e-04, 3.0708e-04, 5.4026e-04,  ..., 1.9385e-01,\n",
      "          1.8958e-01, 1.7993e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.0796e-03, 1.5993e-03, 2.2926e-03,  ..., 4.1870e-01,\n",
      "          4.1162e-01, 3.7085e-01],\n",
      "         [1.5078e-03, 1.9064e-03, 2.2488e-03,  ..., 6.7676e-01,\n",
      "          6.3818e-01, 5.1904e-01],\n",
      "         [1.4901e-04, 1.9670e-04, 2.3925e-04,  ..., 4.1089e-01,\n",
      "          4.0137e-01, 3.6426e-01],\n",
      "         ...,\n",
      "         [3.6507e-03, 4.9248e-03, 6.2828e-03,  ..., 4.6704e-01,\n",
      "          4.4971e-01, 3.6963e-01],\n",
      "         [1.3447e-04, 1.9979e-04, 2.2840e-04,  ..., 2.6099e-01,\n",
      "          2.5513e-01, 2.3901e-01],\n",
      "         [1.7059e-04, 3.8171e-04, 6.8998e-04,  ..., 2.6562e-01,\n",
      "          2.6221e-01, 2.4902e-01]],\n",
      "\n",
      "        [[1.0338e-03, 1.5745e-03, 2.3270e-03,  ..., 2.6611e-01,\n",
      "          2.4768e-01, 2.3975e-01],\n",
      "         [1.5726e-03, 1.9817e-03, 2.3289e-03,  ..., 6.8994e-01,\n",
      "          6.4551e-01, 6.0840e-01],\n",
      "         [1.7977e-04, 2.3532e-04, 2.8467e-04,  ..., 2.8223e-01,\n",
      "          2.7246e-01, 2.6221e-01],\n",
      "         ...,\n",
      "         [2.6264e-03, 3.5515e-03, 4.5433e-03,  ..., 4.6826e-01,\n",
      "          4.4971e-01, 4.2505e-01],\n",
      "         [1.5366e-04, 2.2638e-04, 2.5678e-04,  ..., 2.3352e-01,\n",
      "          2.2693e-01, 2.1924e-01],\n",
      "         [1.5378e-04, 3.4165e-04, 6.2323e-04,  ..., 2.7881e-01,\n",
      "          2.7368e-01, 2.6807e-01]],\n",
      "\n",
      "        [[8.1682e-04, 1.2159e-03, 1.7548e-03,  ..., 2.2815e-01,\n",
      "          3.8879e-02, 3.3356e-02],\n",
      "         [1.3046e-03, 1.6394e-03, 1.9236e-03,  ..., 4.4434e-01,\n",
      "          2.9272e-01, 2.3743e-01],\n",
      "         [1.2898e-04, 1.6880e-04, 2.0385e-04,  ..., 3.3667e-01,\n",
      "          5.4688e-02, 4.5593e-02],\n",
      "         ...,\n",
      "         [2.0943e-03, 2.8343e-03, 3.6240e-03,  ..., 2.3804e-01,\n",
      "          1.2671e-01, 1.0571e-01],\n",
      "         [1.4758e-04, 2.1160e-04, 2.3687e-04,  ..., 1.5479e-01,\n",
      "          1.3330e-01, 1.2671e-01],\n",
      "         [1.4079e-04, 2.4605e-04, 4.1413e-04,  ..., 1.8848e-01,\n",
      "          1.5979e-01, 1.5564e-01]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[6.1274e-04, 8.8549e-04, 1.1215e-03,  ..., 7.3096e-01,\n",
      "          7.2803e-01, 5.2100e-01],\n",
      "         [5.4693e-04, 7.7248e-04, 9.5177e-04,  ..., 5.7764e-01,\n",
      "          5.3809e-01, 3.9429e-01],\n",
      "         [3.0708e-04, 4.4966e-04, 5.9175e-04,  ..., 6.5723e-01,\n",
      "          6.5430e-01, 4.9927e-01],\n",
      "         ...,\n",
      "         [2.6059e-04, 3.3760e-04, 4.0841e-04,  ..., 6.9141e-01,\n",
      "          6.8750e-01, 3.4863e-01],\n",
      "         [8.9455e-04, 1.2331e-03, 1.5926e-03,  ..., 4.4775e-01,\n",
      "          4.2578e-01, 2.4695e-01],\n",
      "         [1.3781e-04, 1.8466e-04, 2.2340e-04,  ..., 3.3472e-01,\n",
      "          3.1519e-01, 2.5732e-01]],\n",
      "\n",
      "        [[7.7581e-04, 1.0748e-03, 1.3466e-03,  ..., 6.8420e-02,\n",
      "          6.6406e-02, 4.2847e-02],\n",
      "         [9.5081e-04, 1.3103e-03, 1.5821e-03,  ..., 8.8928e-02,\n",
      "          7.6172e-02, 4.8462e-02],\n",
      "         [2.8706e-04, 4.1127e-04, 5.3072e-04,  ..., 8.5327e-02,\n",
      "          8.2397e-02, 5.8167e-02],\n",
      "         ...,\n",
      "         [4.4966e-04, 5.8603e-04, 7.0524e-04,  ..., 6.6772e-02,\n",
      "          6.1096e-02, 3.9795e-02],\n",
      "         [1.5945e-03, 2.2354e-03, 2.9278e-03,  ..., 1.6284e-01,\n",
      "          1.3354e-01, 6.9214e-02],\n",
      "         [3.2973e-04, 4.3941e-04, 5.2118e-04,  ..., 1.2415e-01,\n",
      "          8.9172e-02, 7.1960e-02]],\n",
      "\n",
      "        [[6.2847e-04, 8.8978e-04, 1.1396e-03,  ..., 2.0178e-01,\n",
      "          1.9812e-01, 1.9409e-01],\n",
      "         [5.2881e-04, 7.3814e-04, 9.4175e-04,  ..., 2.7026e-01,\n",
      "          2.1155e-01, 1.4465e-01],\n",
      "         [3.0088e-04, 4.5609e-04, 6.1512e-04,  ..., 1.6101e-01,\n",
      "          1.5564e-01, 1.4929e-01],\n",
      "         ...,\n",
      "         [2.8777e-04, 3.8099e-04, 4.6492e-04,  ..., 1.6162e-01,\n",
      "          1.5259e-01, 1.4893e-01],\n",
      "         [1.1721e-03, 1.6069e-03, 2.0809e-03,  ..., 1.6492e-01,\n",
      "          1.3452e-01, 1.1914e-01],\n",
      "         [1.8513e-04, 2.4581e-04, 3.0112e-04,  ..., 1.8542e-01,\n",
      "          1.1621e-01, 8.7891e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[7.8630e-04, 1.1063e-03, 1.4267e-03,  ..., 1.8079e-01,\n",
      "          1.7737e-01, 1.3574e-01],\n",
      "         [2.8267e-03, 3.8052e-03, 4.7150e-03,  ..., 1.4868e-01,\n",
      "          1.3904e-01, 9.1553e-02],\n",
      "         [2.7466e-04, 4.1533e-04, 5.6076e-04,  ..., 1.6309e-01,\n",
      "          1.6028e-01, 1.1963e-01],\n",
      "         ...,\n",
      "         [7.4959e-04, 9.4032e-04, 1.0958e-03,  ..., 6.1829e-02,\n",
      "          6.0638e-02, 3.3386e-02],\n",
      "         [3.6049e-03, 5.0888e-03, 6.8741e-03,  ..., 1.8018e-01,\n",
      "          1.7493e-01, 1.1188e-01],\n",
      "         [7.7152e-04, 9.9373e-04, 1.2016e-03,  ..., 7.3242e-02,\n",
      "          6.9519e-02, 3.2318e-02]],\n",
      "\n",
      "        [[7.4482e-04, 1.0471e-03, 1.3208e-03,  ..., 1.9446e-01,\n",
      "          1.9177e-01, 1.8799e-01],\n",
      "         [9.9277e-04, 1.3924e-03, 1.7290e-03,  ..., 1.9019e-01,\n",
      "          1.7529e-01, 1.5527e-01],\n",
      "         [3.8671e-04, 5.7697e-04, 7.4768e-04,  ..., 1.9202e-01,\n",
      "          1.8909e-01, 1.8530e-01],\n",
      "         ...,\n",
      "         [4.9448e-04, 6.4135e-04, 7.7295e-04,  ..., 2.0459e-01,\n",
      "          2.0166e-01, 1.9873e-01],\n",
      "         [2.2087e-03, 3.0994e-03, 4.0741e-03,  ..., 3.9233e-01,\n",
      "          3.7720e-01, 3.6108e-01],\n",
      "         [3.8481e-04, 5.0592e-04, 6.0701e-04,  ..., 1.5100e-01,\n",
      "          1.4526e-01, 1.3452e-01]],\n",
      "\n",
      "        [[5.8746e-04, 8.7500e-04, 1.1539e-03,  ..., 2.6611e-01,\n",
      "          1.5823e-02, 1.3550e-02],\n",
      "         [5.4264e-04, 7.7152e-04, 9.8515e-04,  ..., 2.7686e-01,\n",
      "          1.4722e-01, 1.1658e-01],\n",
      "         [2.6059e-04, 3.8433e-04, 5.3692e-04,  ..., 2.4280e-01,\n",
      "          2.0920e-02, 1.9165e-02],\n",
      "         ...,\n",
      "         [3.0708e-04, 3.9768e-04, 4.7493e-04,  ..., 1.2280e-01,\n",
      "          1.5701e-02, 1.2863e-02],\n",
      "         [8.4209e-04, 1.1835e-03, 1.5383e-03,  ..., 1.3684e-01,\n",
      "          6.8848e-02, 5.0842e-02],\n",
      "         [2.0003e-04, 2.7561e-04, 3.3665e-04,  ..., 1.4539e-01,\n",
      "          8.4351e-02, 6.9946e-02]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[1.0765e-04, 1.4710e-04, 1.8311e-04,  ..., 6.1621e-01,\n",
      "          6.1475e-01, 5.4199e-01],\n",
      "         [6.9809e-04, 8.6975e-04, 1.0023e-03,  ..., 6.6650e-01,\n",
      "          6.6553e-01, 5.9033e-01],\n",
      "         [6.0380e-05, 8.0526e-05, 9.8586e-05,  ..., 6.4258e-01,\n",
      "          6.4160e-01, 5.7324e-01],\n",
      "         ...,\n",
      "         [1.3208e-03, 1.7214e-03, 2.0771e-03,  ..., 5.2148e-01,\n",
      "          5.1709e-01, 4.6338e-01],\n",
      "         [1.2770e-03, 1.7481e-03, 2.1877e-03,  ..., 5.1855e-01,\n",
      "          5.1416e-01, 4.6143e-01],\n",
      "         [8.8978e-04, 1.2808e-03, 1.5469e-03,  ..., 5.8984e-01,\n",
      "          5.8545e-01, 5.2832e-01]],\n",
      "\n",
      "        [[1.0526e-04, 1.4448e-04, 1.9073e-04,  ..., 1.0239e-02,\n",
      "          9.4376e-03, 6.9084e-03],\n",
      "         [6.8331e-04, 8.5974e-04, 1.0099e-03,  ..., 1.1703e-02,\n",
      "          1.1032e-02, 7.5378e-03],\n",
      "         [5.7220e-05, 7.8499e-05, 1.0163e-04,  ..., 8.9645e-03,\n",
      "          7.7782e-03, 4.8409e-03],\n",
      "         ...,\n",
      "         [1.8597e-03, 2.4529e-03, 2.9678e-03,  ..., 9.1782e-03,\n",
      "          7.4120e-03, 4.4403e-03],\n",
      "         [1.7586e-03, 2.3956e-03, 3.0231e-03,  ..., 6.2370e-03,\n",
      "          4.8637e-03, 3.0766e-03],\n",
      "         [1.1578e-03, 1.6088e-03, 1.9665e-03,  ..., 1.4275e-02,\n",
      "          1.2367e-02, 7.5340e-03]],\n",
      "\n",
      "        [[1.0538e-04, 1.4651e-04, 1.8668e-04,  ..., 3.9551e-02,\n",
      "          3.7659e-02, 3.5492e-02],\n",
      "         [5.9319e-04, 7.6056e-04, 8.7976e-04,  ..., 2.5406e-02,\n",
      "          2.3941e-02, 2.2766e-02],\n",
      "         [5.4300e-05, 7.3314e-05, 9.1136e-05,  ..., 3.4576e-02,\n",
      "          3.2410e-02, 2.9922e-02],\n",
      "         ...,\n",
      "         [1.9369e-03, 2.5463e-03, 3.0384e-03,  ..., 4.8615e-02,\n",
      "          4.3518e-02, 3.9154e-02],\n",
      "         [2.0256e-03, 2.7275e-03, 3.4428e-03,  ..., 4.2297e-02,\n",
      "          3.9154e-02, 3.5400e-02],\n",
      "         [1.4105e-03, 1.9407e-03, 2.3155e-03,  ..., 5.3741e-02,\n",
      "          5.1147e-02, 4.8523e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.3876e-04, 2.0707e-04, 2.8968e-04,  ..., 5.0446e-02,\n",
      "          5.0079e-02, 2.2095e-02],\n",
      "         [1.0900e-03, 1.4439e-03, 1.7376e-03,  ..., 4.2877e-02,\n",
      "          4.2480e-02, 1.4915e-02],\n",
      "         [1.1039e-04, 1.5688e-04, 2.0671e-04,  ..., 3.4576e-02,\n",
      "          3.4302e-02, 1.9455e-02],\n",
      "         ...,\n",
      "         [3.9673e-03, 5.3787e-03, 6.6566e-03,  ..., 1.9913e-02,\n",
      "          1.9012e-02, 1.1246e-02],\n",
      "         [3.7155e-03, 5.1231e-03, 6.6109e-03,  ..., 1.4679e-02,\n",
      "          1.3916e-02, 8.3466e-03],\n",
      "         [2.8706e-03, 3.9902e-03, 4.9858e-03,  ..., 2.5787e-02,\n",
      "          2.4643e-02, 1.8478e-02]],\n",
      "\n",
      "        [[1.6105e-04, 2.3055e-04, 2.9302e-04,  ..., 1.1206e-01,\n",
      "          1.1151e-01, 1.1066e-01],\n",
      "         [1.2293e-03, 1.5450e-03, 1.7357e-03,  ..., 9.0332e-02,\n",
      "          8.9722e-02, 8.9233e-02],\n",
      "         [8.9228e-05, 1.2422e-04, 1.5402e-04,  ..., 7.5928e-02,\n",
      "          7.4036e-02, 7.1350e-02],\n",
      "         ...,\n",
      "         [2.5215e-03, 3.3150e-03, 3.9673e-03,  ..., 1.6833e-01,\n",
      "          1.6736e-01, 1.6431e-01],\n",
      "         [2.4281e-03, 3.3588e-03, 4.3030e-03,  ..., 4.7394e-02,\n",
      "          4.6967e-02, 4.4434e-02],\n",
      "         [1.7223e-03, 2.3479e-03, 2.8095e-03,  ..., 6.4148e-02,\n",
      "          6.3538e-02, 5.9509e-02]],\n",
      "\n",
      "        [[1.0514e-04, 1.4508e-04, 1.8406e-04,  ..., 1.0498e-01,\n",
      "          9.1095e-03, 8.0032e-03],\n",
      "         [9.0551e-04, 1.1301e-03, 1.3170e-03,  ..., 1.1115e-01,\n",
      "          7.7057e-03, 7.0572e-03],\n",
      "         [5.3585e-05, 7.5042e-05, 9.5129e-05,  ..., 1.0022e-01,\n",
      "          1.6281e-02, 1.4633e-02],\n",
      "         ...,\n",
      "         [1.5478e-03, 2.0504e-03, 2.5425e-03,  ..., 7.8552e-02,\n",
      "          1.3168e-02, 1.1948e-02],\n",
      "         [1.7176e-03, 2.3499e-03, 3.0270e-03,  ..., 7.4341e-02,\n",
      "          1.5327e-02, 1.4122e-02],\n",
      "         [9.7561e-04, 1.4439e-03, 1.8129e-03,  ..., 9.0698e-02,\n",
      "          1.3283e-02, 1.1810e-02]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[0.0099, 0.0131, 0.0154,  ..., 0.5820, 0.5688, 0.5063],\n",
      "         [0.0080, 0.0107, 0.0126,  ..., 0.6562, 0.6304, 0.5098],\n",
      "         [0.0046, 0.0060, 0.0070,  ..., 0.5493, 0.5356, 0.4631],\n",
      "         ...,\n",
      "         [0.0014, 0.0017, 0.0022,  ..., 0.5918, 0.5898, 0.5615],\n",
      "         [0.0015, 0.0021, 0.0028,  ..., 0.6924, 0.6899, 0.6372],\n",
      "         [0.0016, 0.0023, 0.0036,  ..., 0.6162, 0.6143, 0.5752]],\n",
      "\n",
      "        [[0.0124, 0.0162, 0.0195,  ..., 0.0452, 0.0404, 0.0222],\n",
      "         [0.0094, 0.0124, 0.0152,  ..., 0.0966, 0.0854, 0.0602],\n",
      "         [0.0060, 0.0077, 0.0091,  ..., 0.0149, 0.0127, 0.0086],\n",
      "         ...,\n",
      "         [0.0016, 0.0019, 0.0022,  ..., 0.0880, 0.0834, 0.0759],\n",
      "         [0.0013, 0.0018, 0.0022,  ..., 0.0391, 0.0355, 0.0269],\n",
      "         [0.0021, 0.0030, 0.0052,  ..., 0.0759, 0.0731, 0.0620]],\n",
      "\n",
      "        [[0.0091, 0.0121, 0.0145,  ..., 0.1580, 0.1422, 0.1036],\n",
      "         [0.0089, 0.0116, 0.0139,  ..., 0.1907, 0.1625, 0.1437],\n",
      "         [0.0059, 0.0076, 0.0090,  ..., 0.0524, 0.0507, 0.0438],\n",
      "         ...,\n",
      "         [0.0017, 0.0021, 0.0024,  ..., 0.1517, 0.1510, 0.1486],\n",
      "         [0.0015, 0.0021, 0.0027,  ..., 0.0654, 0.0638, 0.0596],\n",
      "         [0.0020, 0.0028, 0.0048,  ..., 0.0429, 0.0413, 0.0382]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0642, 0.0863, 0.1114,  ..., 0.0245, 0.0202, 0.0149],\n",
      "         [0.0366, 0.0490, 0.0637,  ..., 0.1189, 0.1075, 0.0596],\n",
      "         [0.0219, 0.0295, 0.0373,  ..., 0.0225, 0.0212, 0.0147],\n",
      "         ...,\n",
      "         [0.0067, 0.0079, 0.0098,  ..., 0.1182, 0.1131, 0.1074],\n",
      "         [0.0079, 0.0101, 0.0123,  ..., 0.0387, 0.0337, 0.0294],\n",
      "         [0.0173, 0.0237, 0.0385,  ..., 0.0506, 0.0460, 0.0400]],\n",
      "\n",
      "        [[0.0196, 0.0257, 0.0308,  ..., 0.0401, 0.0352, 0.0313],\n",
      "         [0.0157, 0.0207, 0.0251,  ..., 0.2354, 0.2197, 0.2065],\n",
      "         [0.0089, 0.0117, 0.0139,  ..., 0.0523, 0.0500, 0.0446],\n",
      "         ...,\n",
      "         [0.0027, 0.0032, 0.0038,  ..., 0.1063, 0.0986, 0.0907],\n",
      "         [0.0025, 0.0034, 0.0042,  ..., 0.0460, 0.0388, 0.0352],\n",
      "         [0.0037, 0.0053, 0.0083,  ..., 0.0871, 0.0817, 0.0784]],\n",
      "\n",
      "        [[0.0081, 0.0108, 0.0133,  ..., 0.1207, 0.0651, 0.0478],\n",
      "         [0.0081, 0.0111, 0.0139,  ..., 0.1327, 0.0656, 0.0557],\n",
      "         [0.0049, 0.0064, 0.0078,  ..., 0.0652, 0.0218, 0.0177],\n",
      "         ...,\n",
      "         [0.0016, 0.0020, 0.0024,  ..., 0.1411, 0.1121, 0.1106],\n",
      "         [0.0017, 0.0024, 0.0030,  ..., 0.0764, 0.0242, 0.0215],\n",
      "         [0.0029, 0.0041, 0.0067,  ..., 0.0861, 0.0487, 0.0448]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward1>), tensor([[[1.8646e-02, 2.3422e-02, 2.7420e-02,  ..., 8.2861e-01,\n",
      "          7.9590e-01, 6.6211e-01],\n",
      "         [2.1347e-02, 2.8671e-02, 3.6194e-02,  ..., 6.8311e-01,\n",
      "          6.7969e-01, 6.4160e-01],\n",
      "         [1.7914e-02, 2.0844e-02, 2.2614e-02,  ..., 6.3086e-01,\n",
      "          6.1719e-01, 5.0879e-01],\n",
      "         ...,\n",
      "         [1.6823e-03, 2.1744e-03, 2.6817e-03,  ..., 7.3389e-01,\n",
      "          7.2705e-01, 6.5088e-01],\n",
      "         [5.0211e-04, 6.3658e-04, 7.7772e-04,  ..., 7.5635e-01,\n",
      "          7.4463e-01, 6.5967e-01],\n",
      "         [3.4881e-04, 4.5443e-04, 5.6219e-04,  ..., 6.0400e-01,\n",
      "          5.9814e-01, 5.4199e-01]],\n",
      "\n",
      "        [[1.9440e-02, 2.5513e-02, 3.1647e-02,  ..., 2.3666e-02,\n",
      "          1.9974e-02, 1.2077e-02],\n",
      "         [2.2125e-02, 3.1311e-02, 4.2389e-02,  ..., 2.8046e-02,\n",
      "          2.4353e-02, 7.4081e-03],\n",
      "         [1.5564e-02, 1.8799e-02, 2.1729e-02,  ..., 6.0669e-02,\n",
      "          4.5990e-02, 1.8799e-02],\n",
      "         ...,\n",
      "         [1.6651e-03, 2.2392e-03, 2.7199e-03,  ..., 3.1494e-02,\n",
      "          2.4399e-02, 1.4664e-02],\n",
      "         [6.3038e-04, 8.2159e-04, 9.8705e-04,  ..., 3.4729e-02,\n",
      "          3.2867e-02, 2.1164e-02],\n",
      "         [3.8791e-04, 5.2166e-04, 6.2895e-04,  ..., 1.5686e-02,\n",
      "          1.4671e-02, 1.1971e-02]],\n",
      "\n",
      "        [[1.5778e-02, 2.0645e-02, 2.5116e-02,  ..., 6.7505e-02,\n",
      "          5.0201e-02, 4.1412e-02],\n",
      "         [2.1149e-02, 2.9831e-02, 3.9124e-02,  ..., 3.9734e-02,\n",
      "          3.8574e-02, 3.5461e-02],\n",
      "         [1.8509e-02, 2.2125e-02, 2.4551e-02,  ..., 4.2206e-02,\n",
      "          3.7689e-02, 3.4241e-02],\n",
      "         ...,\n",
      "         [1.8396e-03, 2.3460e-03, 2.8610e-03,  ..., 7.6477e-02,\n",
      "          7.4768e-02, 7.0312e-02],\n",
      "         [5.6171e-04, 7.0572e-04, 8.5068e-04,  ..., 7.1411e-02,\n",
      "          6.9397e-02, 6.7017e-02],\n",
      "         [3.8910e-04, 5.2118e-04, 6.4087e-04,  ..., 3.0136e-02,\n",
      "          2.8564e-02, 2.6489e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[3.4332e-02, 4.6417e-02, 5.8289e-02,  ..., 5.8380e-02,\n",
      "          4.1840e-02, 2.8656e-02],\n",
      "         [3.2593e-02, 4.5685e-02, 6.0303e-02,  ..., 3.4149e-02,\n",
      "          2.5818e-02, 1.8036e-02],\n",
      "         [3.4882e-02, 4.3640e-02, 5.0995e-02,  ..., 1.1450e-01,\n",
      "          1.0425e-01, 5.6427e-02],\n",
      "         ...,\n",
      "         [3.0537e-03, 3.9902e-03, 4.8714e-03,  ..., 3.1860e-02,\n",
      "          2.7313e-02, 2.2247e-02],\n",
      "         [1.8349e-03, 2.3212e-03, 2.7637e-03,  ..., 4.6265e-02,\n",
      "          3.9459e-02, 2.4887e-02],\n",
      "         [1.2197e-03, 1.6060e-03, 1.9445e-03,  ..., 2.1210e-02,\n",
      "          1.6022e-02, 1.3466e-02]],\n",
      "\n",
      "        [[2.5040e-02, 3.3722e-02, 3.9398e-02,  ..., 1.3135e-01,\n",
      "          1.2323e-01, 1.2012e-01],\n",
      "         [2.5742e-02, 3.8422e-02, 5.0751e-02,  ..., 6.6162e-02,\n",
      "          6.2683e-02, 6.0974e-02],\n",
      "         [2.4216e-02, 3.1281e-02, 3.5278e-02,  ..., 2.0032e-01,\n",
      "          1.9543e-01, 1.9324e-01],\n",
      "         ...,\n",
      "         [2.0828e-03, 2.8896e-03, 3.5114e-03,  ..., 1.0419e-01,\n",
      "          9.7839e-02, 9.6191e-02],\n",
      "         [9.3412e-04, 1.2445e-03, 1.4725e-03,  ..., 1.7578e-01,\n",
      "          1.6858e-01, 1.6699e-01],\n",
      "         [6.9857e-04, 9.7752e-04, 1.1711e-03,  ..., 1.4771e-01,\n",
      "          1.4368e-01, 1.4160e-01]],\n",
      "\n",
      "        [[1.5884e-02, 2.0737e-02, 2.5696e-02,  ..., 7.7637e-02,\n",
      "          4.3945e-02, 3.3813e-02],\n",
      "         [1.9608e-02, 2.7527e-02, 3.7170e-02,  ..., 2.7710e-02,\n",
      "          1.0384e-02, 9.0103e-03],\n",
      "         [1.7410e-02, 2.0752e-02, 2.3376e-02,  ..., 8.2947e-02,\n",
      "          5.6213e-02, 5.0262e-02],\n",
      "         ...,\n",
      "         [1.7881e-03, 2.1954e-03, 2.7637e-03,  ..., 6.4819e-02,\n",
      "          3.3966e-02, 2.7161e-02],\n",
      "         [5.4550e-04, 6.7377e-04, 8.4639e-04,  ..., 6.7139e-02,\n",
      "          2.4628e-02, 1.8753e-02],\n",
      "         [3.6335e-04, 4.5991e-04, 5.8222e-04,  ..., 4.9377e-02,\n",
      "          1.5038e-02, 1.1642e-02]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[6.9141e-04, 8.5115e-04, 1.0061e-03,  ..., 5.4346e-01,\n",
      "          4.9756e-01, 4.4653e-01],\n",
      "         [9.0332e-03, 1.1566e-02, 1.3481e-02,  ..., 7.0557e-01,\n",
      "          7.0068e-01, 6.0449e-01],\n",
      "         [8.0872e-04, 9.8038e-04, 1.1339e-03,  ..., 6.9385e-01,\n",
      "          6.9141e-01, 5.9033e-01],\n",
      "         ...,\n",
      "         [8.0061e-04, 1.0538e-03, 1.2884e-03,  ..., 8.4375e-01,\n",
      "          7.7393e-01, 6.7236e-01],\n",
      "         [3.5357e-04, 4.5514e-04, 5.6028e-04,  ..., 8.3203e-01,\n",
      "          8.1738e-01, 7.1143e-01],\n",
      "         [1.9102e-03, 2.4509e-03, 3.4637e-03,  ..., 7.9150e-01,\n",
      "          7.9004e-01, 7.4072e-01]],\n",
      "\n",
      "        [[7.1001e-04, 9.7370e-04, 1.2455e-03,  ..., 3.1311e-02,\n",
      "          2.4704e-02, 2.1255e-02],\n",
      "         [5.1155e-03, 7.0572e-03, 8.9493e-03,  ..., 1.7456e-02,\n",
      "          1.5091e-02, 1.1154e-02],\n",
      "         [5.0211e-04, 6.6614e-04, 8.3017e-04,  ..., 9.5062e-03,\n",
      "          8.1177e-03, 5.5428e-03],\n",
      "         ...,\n",
      "         [1.5068e-03, 2.0409e-03, 2.4185e-03,  ..., 1.6281e-02,\n",
      "          1.4717e-02, 1.1131e-02],\n",
      "         [8.3923e-04, 1.1139e-03, 1.3313e-03,  ..., 2.8687e-02,\n",
      "          2.8290e-02, 1.5747e-02],\n",
      "         [2.6398e-03, 4.1504e-03, 7.3433e-03,  ..., 6.0883e-03,\n",
      "          5.6152e-03, 3.6354e-03]],\n",
      "\n",
      "        [[7.2765e-04, 9.9182e-04, 1.2369e-03,  ..., 1.1920e-01,\n",
      "          7.8369e-02, 5.0964e-02],\n",
      "         [6.4964e-03, 8.7585e-03, 1.0628e-02,  ..., 8.1055e-02,\n",
      "          7.5439e-02, 6.8054e-02],\n",
      "         [6.7711e-04, 8.9598e-04, 1.0996e-03,  ..., 6.7200e-02,\n",
      "          6.5430e-02, 6.1951e-02],\n",
      "         ...,\n",
      "         [1.1473e-03, 1.4267e-03, 1.6670e-03,  ..., 3.4088e-02,\n",
      "          2.9755e-02, 2.2507e-02],\n",
      "         [6.1369e-04, 7.6437e-04, 8.8215e-04,  ..., 3.2227e-02,\n",
      "          2.7847e-02, 2.1744e-02],\n",
      "         [2.5520e-03, 3.3436e-03, 4.4823e-03,  ..., 6.6040e-02,\n",
      "          6.3782e-02, 5.9723e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.4624e-03, 3.2501e-03, 4.0131e-03,  ..., 1.0857e-02,\n",
      "          8.0338e-03, 5.7640e-03],\n",
      "         [7.3929e-03, 9.7885e-03, 1.2238e-02,  ..., 3.8086e-02,\n",
      "          2.8198e-02, 1.7593e-02],\n",
      "         [7.5912e-04, 9.7561e-04, 1.1969e-03,  ..., 3.0167e-02,\n",
      "          2.4399e-02, 1.5091e-02],\n",
      "         ...,\n",
      "         [3.9062e-03, 5.2567e-03, 6.4087e-03,  ..., 5.5267e-02,\n",
      "          5.3558e-02, 1.6388e-02],\n",
      "         [2.8477e-03, 3.7479e-03, 4.5509e-03,  ..., 2.7206e-02,\n",
      "          1.7685e-02, 1.2627e-02],\n",
      "         [1.0719e-02, 1.8707e-02, 4.0924e-02,  ..., 2.8961e-02,\n",
      "          2.6764e-02, 2.4033e-02]],\n",
      "\n",
      "        [[8.5211e-04, 1.1587e-03, 1.4639e-03,  ..., 3.7567e-02,\n",
      "          3.4973e-02, 3.1433e-02],\n",
      "         [7.4158e-03, 9.8877e-03, 1.2222e-02,  ..., 2.2485e-01,\n",
      "          2.1960e-01, 2.1667e-01],\n",
      "         [8.0538e-04, 1.0738e-03, 1.3285e-03,  ..., 2.8809e-01,\n",
      "          2.8442e-01, 2.8076e-01],\n",
      "         ...,\n",
      "         [2.2278e-03, 2.8725e-03, 3.3989e-03,  ..., 9.4543e-02,\n",
      "          8.6487e-02, 8.4717e-02],\n",
      "         [1.3523e-03, 1.6890e-03, 1.9817e-03,  ..., 7.8613e-02,\n",
      "          6.9702e-02, 6.7505e-02],\n",
      "         [4.5891e-03, 6.9008e-03, 1.1566e-02,  ..., 4.4830e-02,\n",
      "          4.2023e-02, 4.1107e-02]],\n",
      "\n",
      "        [[5.6124e-04, 7.2479e-04, 8.7833e-04,  ..., 9.0393e-02,\n",
      "          6.3660e-02, 5.4382e-02],\n",
      "         [5.0316e-03, 6.8703e-03, 8.5068e-03,  ..., 9.1309e-02,\n",
      "          2.9114e-02, 2.6031e-02],\n",
      "         [5.9414e-04, 7.5817e-04, 8.8263e-04,  ..., 8.2397e-02,\n",
      "          2.2888e-02, 1.9836e-02],\n",
      "         ...,\n",
      "         [1.1187e-03, 1.5221e-03, 1.9274e-03,  ..., 6.6589e-02,\n",
      "          3.2379e-02, 2.9343e-02],\n",
      "         [5.5408e-04, 7.1859e-04, 8.7929e-04,  ..., 4.9774e-02,\n",
      "          2.4811e-02, 2.2903e-02],\n",
      "         [1.6422e-03, 2.4643e-03, 4.3449e-03,  ..., 3.3081e-02,\n",
      "          1.3466e-02, 1.2062e-02]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[1.5274e-02, 1.9730e-02, 2.3499e-02,  ..., 5.0635e-01,\n",
      "          4.9585e-01, 4.2505e-01],\n",
      "         [6.9160e-03, 8.8882e-03, 1.0628e-02,  ..., 6.0010e-01,\n",
      "          5.9033e-01, 4.8657e-01],\n",
      "         [5.0240e-03, 6.4163e-03, 7.7820e-03,  ..., 5.2783e-01,\n",
      "          5.2148e-01, 4.5386e-01],\n",
      "         ...,\n",
      "         [3.2210e-04, 3.9816e-04, 4.6706e-04,  ..., 7.0410e-01,\n",
      "          6.8555e-01, 6.2549e-01],\n",
      "         [6.2561e-04, 8.3399e-04, 1.1005e-03,  ..., 4.6265e-01,\n",
      "          4.3091e-01, 3.7720e-01],\n",
      "         [3.0279e-04, 3.7932e-04, 4.5085e-04,  ..., 6.2646e-01,\n",
      "          6.1621e-01, 5.3857e-01]],\n",
      "\n",
      "        [[3.7785e-03, 4.9858e-03, 6.2523e-03,  ..., 2.0706e-02,\n",
      "          1.8326e-02, 1.4893e-02],\n",
      "         [4.4136e-03, 5.8136e-03, 7.1144e-03,  ..., 3.2684e-02,\n",
      "          3.0151e-02, 2.6718e-02],\n",
      "         [2.4719e-03, 3.2139e-03, 3.9482e-03,  ..., 1.9958e-02,\n",
      "          1.7761e-02, 1.4908e-02],\n",
      "         ...,\n",
      "         [7.8344e-04, 1.0300e-03, 1.2369e-03,  ..., 1.6983e-02,\n",
      "          1.3206e-02, 8.9264e-03],\n",
      "         [9.7466e-04, 1.3504e-03, 1.7748e-03,  ..., 1.2189e-01,\n",
      "          7.4158e-02, 6.9580e-02],\n",
      "         [5.4455e-04, 7.0095e-04, 8.3733e-04,  ..., 2.6276e-02,\n",
      "          2.0508e-02, 1.6724e-02]],\n",
      "\n",
      "        [[6.9237e-03, 9.7122e-03, 1.2314e-02,  ..., 6.1371e-02,\n",
      "          4.9957e-02, 3.7170e-02],\n",
      "         [4.1695e-03, 5.6648e-03, 7.0267e-03,  ..., 6.3477e-02,\n",
      "          5.6885e-02, 4.7455e-02],\n",
      "         [3.1071e-03, 4.2801e-03, 5.4550e-03,  ..., 6.9580e-02,\n",
      "          6.2500e-02, 5.0873e-02],\n",
      "         ...,\n",
      "         [5.5504e-04, 7.4768e-04, 9.2840e-04,  ..., 3.5736e-02,\n",
      "          3.3142e-02, 3.1586e-02],\n",
      "         [7.9298e-04, 1.1425e-03, 1.6718e-03,  ..., 4.6997e-02,\n",
      "          4.2328e-02, 2.8595e-02],\n",
      "         [3.8838e-04, 5.2786e-04, 6.5422e-04,  ..., 4.5898e-02,\n",
      "          4.4556e-02, 4.2328e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.0056e-02, 1.3176e-02, 1.5717e-02,  ..., 4.5891e-03,\n",
      "          3.6201e-03, 2.8267e-03],\n",
      "         [4.6501e-03, 6.0692e-03, 7.3814e-03,  ..., 1.0460e-02,\n",
      "          8.5373e-03, 5.7182e-03],\n",
      "         [4.4670e-03, 5.7259e-03, 6.8245e-03,  ..., 9.9869e-03,\n",
      "          8.2932e-03, 5.0888e-03],\n",
      "         ...,\n",
      "         [8.5449e-03, 1.0521e-02, 1.2550e-02,  ..., 7.8201e-03,\n",
      "          6.7177e-03, 5.4054e-03],\n",
      "         [1.4503e-02, 1.8845e-02, 2.3361e-02,  ..., 1.6907e-02,\n",
      "          8.6060e-03, 6.1646e-03],\n",
      "         [6.6872e-03, 8.4686e-03, 1.0063e-02,  ..., 1.3977e-02,\n",
      "          9.5596e-03, 5.2986e-03]],\n",
      "\n",
      "        [[5.9204e-03, 8.4152e-03, 1.0185e-02,  ..., 8.0566e-03,\n",
      "          6.3400e-03, 4.7188e-03],\n",
      "         [3.4904e-03, 4.7569e-03, 6.0616e-03,  ..., 4.3213e-02,\n",
      "          3.9520e-02, 3.8147e-02],\n",
      "         [2.1591e-03, 3.0384e-03, 3.8414e-03,  ..., 2.4826e-02,\n",
      "          2.1286e-02, 1.9638e-02],\n",
      "         ...,\n",
      "         [1.2264e-03, 1.4715e-03, 1.6356e-03,  ..., 1.9791e-02,\n",
      "          1.8097e-02, 1.7487e-02],\n",
      "         [1.0347e-03, 1.4086e-03, 1.8835e-03,  ..., 4.8279e-02,\n",
      "          4.6021e-02, 4.1840e-02],\n",
      "         [7.7486e-04, 9.9182e-04, 1.1387e-03,  ..., 3.7842e-02,\n",
      "          3.3478e-02, 2.9404e-02]],\n",
      "\n",
      "        [[9.4910e-03, 1.3237e-02, 1.8463e-02,  ..., 5.1208e-02,\n",
      "          2.7969e-02, 1.9150e-02],\n",
      "         [5.2071e-03, 7.3471e-03, 1.0262e-02,  ..., 5.7373e-02,\n",
      "          2.6199e-02, 2.0584e-02],\n",
      "         [2.2526e-03, 3.0212e-03, 4.1084e-03,  ..., 6.4941e-02,\n",
      "          3.3386e-02, 2.0798e-02],\n",
      "         ...,\n",
      "         [6.9857e-04, 8.5974e-04, 1.0147e-03,  ..., 2.9510e-02,\n",
      "          5.7144e-03, 4.7417e-03],\n",
      "         [1.0061e-03, 1.3351e-03, 1.8539e-03,  ..., 6.6467e-02,\n",
      "          4.1412e-02, 2.6245e-02],\n",
      "         [5.5933e-04, 6.9904e-04, 8.4019e-04,  ..., 4.0802e-02,\n",
      "          1.0788e-02, 8.3923e-03]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[5.7602e-03, 6.0120e-03, 7.5684e-03,  ..., 3.9062e-01,\n",
      "          3.9038e-01, 3.6133e-01],\n",
      "         [5.0926e-04, 6.1464e-04, 7.4244e-04,  ..., 5.6299e-01,\n",
      "          5.6201e-01, 5.1904e-01],\n",
      "         [1.2565e-04, 1.6296e-04, 1.9264e-04,  ..., 6.3477e-01,\n",
      "          6.2207e-01, 5.3418e-01],\n",
      "         ...,\n",
      "         [1.0786e-03, 1.5602e-03, 2.0790e-03,  ..., 5.6494e-01,\n",
      "          5.4443e-01, 4.5972e-01],\n",
      "         [3.4213e-05, 5.0843e-05, 6.7532e-05,  ..., 5.8447e-01,\n",
      "          5.7080e-01, 5.1904e-01],\n",
      "         [6.3181e-05, 8.4043e-05, 1.0711e-04,  ..., 4.5679e-01,\n",
      "          4.3677e-01, 3.3521e-01]],\n",
      "\n",
      "        [[5.8594e-03, 6.1378e-03, 7.9880e-03,  ..., 2.3987e-02,\n",
      "          2.2980e-02, 2.1973e-02],\n",
      "         [4.7088e-04, 5.6219e-04, 6.9904e-04,  ..., 1.1497e-02,\n",
      "          1.0605e-02, 7.5607e-03],\n",
      "         [1.4341e-04, 1.9264e-04, 2.2614e-04,  ..., 4.0649e-02,\n",
      "          3.1372e-02, 1.6113e-02],\n",
      "         ...,\n",
      "         [2.3575e-03, 3.3016e-03, 4.0779e-03,  ..., 3.4393e-02,\n",
      "          3.1738e-02, 2.0493e-02],\n",
      "         [1.1504e-04, 1.6904e-04, 2.1791e-04,  ..., 7.8857e-02,\n",
      "          7.5928e-02, 5.9296e-02],\n",
      "         [2.2638e-04, 3.0994e-04, 3.7217e-04,  ..., 5.5389e-02,\n",
      "          4.9316e-02, 3.6774e-02]],\n",
      "\n",
      "        [[4.8180e-03, 4.9934e-03, 6.4659e-03,  ..., 9.4727e-02,\n",
      "          9.3140e-02, 9.0576e-02],\n",
      "         [4.5276e-04, 5.3024e-04, 6.3419e-04,  ..., 9.8389e-02,\n",
      "          9.6741e-02, 9.3689e-02],\n",
      "         [9.1851e-05, 1.2159e-04, 1.4400e-04,  ..., 1.8567e-01,\n",
      "          1.7310e-01, 1.5222e-01],\n",
      "         ...,\n",
      "         [1.7118e-03, 2.3041e-03, 2.9316e-03,  ..., 1.1865e-01,\n",
      "          1.0828e-01, 9.9609e-02],\n",
      "         [5.7578e-05, 8.2970e-05, 1.0961e-04,  ..., 1.3831e-01,\n",
      "          1.2903e-01, 1.2024e-01],\n",
      "         [1.1623e-04, 1.5044e-04, 1.8072e-04,  ..., 2.3596e-01,\n",
      "          2.2754e-01, 2.1436e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.5945e-02, 1.6602e-02, 2.2049e-02,  ..., 1.6980e-01,\n",
      "          1.6943e-01, 1.6663e-01],\n",
      "         [2.5196e-03, 2.8400e-03, 3.2845e-03,  ..., 2.3834e-02,\n",
      "          2.2583e-02, 1.9699e-02],\n",
      "         [4.4942e-04, 6.2084e-04, 7.7009e-04,  ..., 5.2826e-02,\n",
      "          4.4434e-02, 2.6276e-02],\n",
      "         ...,\n",
      "         [6.5308e-03, 8.9569e-03, 1.1421e-02,  ..., 2.9846e-02,\n",
      "          2.7328e-02, 1.8066e-02],\n",
      "         [4.2057e-04, 5.8365e-04, 7.5102e-04,  ..., 9.9670e-02,\n",
      "          9.3018e-02, 7.0129e-02],\n",
      "         [6.5804e-04, 8.6784e-04, 1.0509e-03,  ..., 1.6858e-01,\n",
      "          1.5332e-01, 8.8135e-02]],\n",
      "\n",
      "        [[5.2452e-03, 5.4245e-03, 6.8512e-03,  ..., 1.8481e-01,\n",
      "          1.8201e-01, 1.8152e-01],\n",
      "         [4.9210e-04, 5.6362e-04, 6.7091e-04,  ..., 3.1494e-02,\n",
      "          2.7954e-02, 2.5894e-02],\n",
      "         [8.3864e-05, 1.1826e-04, 1.4925e-04,  ..., 8.8013e-02,\n",
      "          7.9102e-02, 7.4707e-02],\n",
      "         ...,\n",
      "         [3.1700e-03, 4.3907e-03, 5.6534e-03,  ..., 1.9446e-01,\n",
      "          1.8542e-01, 1.8115e-01],\n",
      "         [1.4603e-04, 2.1148e-04, 2.8825e-04,  ..., 3.3496e-01,\n",
      "          3.1567e-01, 3.0737e-01],\n",
      "         [2.3270e-04, 3.1853e-04, 3.9124e-04,  ..., 3.1860e-01,\n",
      "          3.0737e-01, 3.0493e-01]],\n",
      "\n",
      "        [[2.8286e-03, 3.0441e-03, 4.2801e-03,  ..., 6.5857e-02,\n",
      "          3.7323e-02, 3.6713e-02],\n",
      "         [4.0627e-04, 5.3120e-04, 6.5470e-04,  ..., 9.4543e-02,\n",
      "          4.8218e-02, 4.7180e-02],\n",
      "         [1.0377e-04, 1.5211e-04, 1.8346e-04,  ..., 1.5405e-01,\n",
      "          8.5754e-02, 7.9590e-02],\n",
      "         ...,\n",
      "         [1.4210e-03, 1.9951e-03, 2.6569e-03,  ..., 8.9233e-02,\n",
      "          2.7298e-02, 2.3499e-02],\n",
      "         [6.3837e-05, 9.6083e-05, 1.3101e-04,  ..., 2.1204e-01,\n",
      "          1.6406e-01, 1.5857e-01],\n",
      "         [1.5187e-04, 2.0003e-04, 2.5463e-04,  ..., 1.3928e-01,\n",
      "          7.8430e-02, 7.1655e-02]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[3.6793e-03, 4.7760e-03, 5.8861e-03,  ..., 4.9927e-01,\n",
      "          4.2163e-01, 3.1323e-01],\n",
      "         [1.6357e-02, 2.3300e-02, 2.9694e-02,  ..., 1.9800e-01,\n",
      "          1.7651e-01, 1.1475e-01],\n",
      "         [1.8864e-03, 2.6035e-03, 3.2730e-03,  ..., 2.9590e-01,\n",
      "          2.7881e-01, 2.3267e-01],\n",
      "         ...,\n",
      "         [4.5061e-04, 5.9080e-04, 7.1335e-04,  ..., 5.2588e-01,\n",
      "          4.9121e-01, 3.8452e-01],\n",
      "         [4.4274e-04, 6.7043e-04, 1.0624e-03,  ..., 2.6196e-01,\n",
      "          2.6123e-01, 2.3584e-01],\n",
      "         [3.6645e-04, 4.8494e-04, 5.8985e-04,  ..., 4.5117e-01,\n",
      "          4.2505e-01, 3.0493e-01]],\n",
      "\n",
      "        [[9.2010e-03, 1.0910e-02, 1.2871e-02,  ..., 5.1544e-02,\n",
      "          3.9520e-02, 2.9358e-02],\n",
      "         [4.2114e-02, 5.5237e-02, 7.1777e-02,  ..., 5.0385e-02,\n",
      "          4.7028e-02, 3.6377e-02],\n",
      "         [3.9444e-03, 5.2185e-03, 6.4583e-03,  ..., 5.4504e-02,\n",
      "          5.1086e-02, 3.7109e-02],\n",
      "         ...,\n",
      "         [4.0174e-04, 6.1226e-04, 6.9761e-04,  ..., 1.0608e-01,\n",
      "          8.3740e-02, 5.2734e-02],\n",
      "         [2.2259e-03, 3.3264e-03, 3.6678e-03,  ..., 3.8357e-03,\n",
      "          3.3321e-03, 2.0504e-03],\n",
      "         [4.4417e-04, 6.6328e-04, 7.5340e-04,  ..., 3.6682e-02,\n",
      "          3.2440e-02, 2.3514e-02]],\n",
      "\n",
      "        [[4.8027e-03, 6.4011e-03, 7.6561e-03,  ..., 4.4360e-01,\n",
      "          4.1577e-01, 3.6108e-01],\n",
      "         [2.0309e-02, 2.7771e-02, 3.5828e-02,  ..., 1.1597e-01,\n",
      "          9.9609e-02, 8.9600e-02],\n",
      "         [2.3956e-03, 3.0899e-03, 3.8834e-03,  ..., 2.5391e-01,\n",
      "          2.2681e-01, 1.8713e-01],\n",
      "         ...,\n",
      "         [4.5967e-04, 6.0511e-04, 7.5340e-04,  ..., 4.7754e-01,\n",
      "          4.3701e-01, 3.6401e-01],\n",
      "         [2.4033e-03, 3.1815e-03, 4.0703e-03,  ..., 6.1249e-02,\n",
      "          5.9448e-02, 5.2399e-02],\n",
      "         [5.3883e-04, 6.4278e-04, 7.4387e-04,  ..., 2.5488e-01,\n",
      "          2.1985e-01, 1.8250e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[8.3466e-03, 1.0445e-02, 1.2619e-02,  ..., 4.6753e-02,\n",
      "          4.3457e-02, 1.2917e-02],\n",
      "         [8.4900e-02, 1.1011e-01, 1.4307e-01,  ..., 8.7204e-03,\n",
      "          7.5874e-03, 2.6093e-03],\n",
      "         [1.6556e-02, 2.1561e-02, 2.7939e-02,  ..., 1.8738e-02,\n",
      "          1.5511e-02, 6.9008e-03],\n",
      "         ...,\n",
      "         [9.7370e-04, 1.2388e-03, 1.5240e-03,  ..., 1.0809e-01,\n",
      "          9.0149e-02, 4.8187e-02],\n",
      "         [1.1215e-02, 1.4069e-02, 1.8036e-02,  ..., 1.4019e-03,\n",
      "          1.3762e-03, 3.0303e-04],\n",
      "         [3.8166e-03, 4.8714e-03, 6.1989e-03,  ..., 2.1255e-02,\n",
      "          1.9730e-02, 6.6986e-03]],\n",
      "\n",
      "        [[7.2861e-03, 9.3536e-03, 1.1154e-02,  ..., 4.3970e-01,\n",
      "          4.3286e-01, 4.2065e-01],\n",
      "         [4.9225e-02, 7.0251e-02, 8.8623e-02,  ..., 3.7109e-01,\n",
      "          3.6060e-01, 3.5107e-01],\n",
      "         [5.3558e-03, 7.2823e-03, 8.9645e-03,  ..., 1.4075e-01,\n",
      "          1.2720e-01, 1.0785e-01],\n",
      "         ...,\n",
      "         [5.2404e-04, 6.8092e-04, 8.1968e-04,  ..., 4.5972e-01,\n",
      "          4.4434e-01, 4.2188e-01],\n",
      "         [4.5090e-03, 5.6458e-03, 6.7444e-03,  ..., 2.6978e-02,\n",
      "          2.6825e-02, 2.5970e-02],\n",
      "         [6.6996e-04, 8.5163e-04, 1.0118e-03,  ..., 1.1426e-01,\n",
      "          1.0889e-01, 1.0309e-01]],\n",
      "\n",
      "        [[4.3259e-03, 5.3253e-03, 6.2561e-03,  ..., 2.7930e-01,\n",
      "          1.7529e-01, 1.5295e-01],\n",
      "         [1.6266e-02, 2.1790e-02, 2.7618e-02,  ..., 8.9722e-02,\n",
      "          5.7495e-02, 3.6865e-02],\n",
      "         [1.8539e-03, 2.3632e-03, 2.9049e-03,  ..., 1.7932e-01,\n",
      "          1.3049e-01, 8.3130e-02],\n",
      "         ...,\n",
      "         [4.7088e-04, 6.0749e-04, 7.3338e-04,  ..., 4.1089e-01,\n",
      "          3.0493e-01, 2.8125e-01],\n",
      "         [2.6512e-03, 3.9215e-03, 5.3978e-03,  ..., 5.2368e-02,\n",
      "          1.8326e-02, 1.4778e-02],\n",
      "         [5.6219e-04, 7.6628e-04, 9.5367e-04,  ..., 2.0129e-01,\n",
      "          1.3403e-01, 1.0925e-01]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[5.1558e-05, 7.1228e-05, 8.8871e-05,  ..., 4.3066e-01,\n",
      "          3.9404e-01, 3.1787e-01],\n",
      "         [1.1206e-05, 1.4246e-05, 1.6391e-05,  ..., 4.7290e-01,\n",
      "          4.3408e-01, 3.8965e-01],\n",
      "         [4.0710e-05, 5.3942e-05, 6.2823e-05,  ..., 4.1797e-01,\n",
      "          4.0625e-01, 3.4326e-01],\n",
      "         ...,\n",
      "         [1.0818e-02, 1.3908e-02, 1.7776e-02,  ..., 3.6035e-01,\n",
      "          2.5513e-01, 1.6479e-01],\n",
      "         [2.0206e-04, 2.5344e-04, 3.0994e-04,  ..., 1.0919e-01,\n",
      "          9.1309e-02, 7.3975e-02],\n",
      "         [1.8482e-03, 2.3327e-03, 2.9449e-03,  ..., 3.9160e-01,\n",
      "          3.2324e-01, 2.5684e-01]],\n",
      "\n",
      "        [[9.6858e-05, 1.2946e-04, 1.7190e-04,  ..., 1.2079e-01,\n",
      "          1.1615e-01, 2.9236e-02],\n",
      "         [3.4332e-05, 4.2737e-05, 5.2035e-05,  ..., 6.5674e-02,\n",
      "          5.6335e-02, 2.4948e-02],\n",
      "         [8.5235e-05, 1.0461e-04, 1.2636e-04,  ..., 7.0312e-02,\n",
      "          6.0883e-02, 2.0569e-02],\n",
      "         ...,\n",
      "         [1.6449e-02, 2.2095e-02, 2.6871e-02,  ..., 1.6028e-01,\n",
      "          1.5247e-01, 1.0846e-01],\n",
      "         [3.3522e-04, 4.1986e-04, 5.0592e-04,  ..., 1.2708e-01,\n",
      "          1.2280e-01, 8.5571e-02],\n",
      "         [3.6945e-03, 4.9667e-03, 5.8327e-03,  ..., 1.4514e-01,\n",
      "          1.3708e-01, 9.2834e-02]],\n",
      "\n",
      "        [[6.1929e-05, 9.0480e-05, 1.1665e-04,  ..., 2.4170e-01,\n",
      "          2.2974e-01, 2.1741e-01],\n",
      "         [1.6391e-05, 2.0623e-05, 2.4319e-05,  ..., 3.0103e-01,\n",
      "          2.8052e-01, 2.4329e-01],\n",
      "         [4.6670e-05, 6.0022e-05, 7.0155e-05,  ..., 3.1152e-01,\n",
      "          3.0688e-01, 2.9492e-01],\n",
      "         ...,\n",
      "         [1.2337e-02, 1.6800e-02, 2.1866e-02,  ..., 3.3569e-01,\n",
      "          2.3694e-01, 1.6919e-01],\n",
      "         [1.6165e-04, 2.0826e-04, 2.4557e-04,  ..., 1.4624e-01,\n",
      "          8.9233e-02, 6.2805e-02],\n",
      "         [2.1210e-03, 2.7237e-03, 3.3455e-03,  ..., 7.7197e-01,\n",
      "          5.9229e-01, 3.5889e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[5.3692e-04, 8.3256e-04, 1.0958e-03,  ..., 6.9519e-02,\n",
      "          6.0730e-02, 3.7750e-02],\n",
      "         [1.0079e-04, 1.3340e-04, 1.6165e-04,  ..., 8.4412e-02,\n",
      "          7.2449e-02, 5.9998e-02],\n",
      "         [3.4952e-04, 4.6468e-04, 5.4598e-04,  ..., 4.6112e-02,\n",
      "          4.4250e-02, 3.0792e-02],\n",
      "         ...,\n",
      "         [1.1032e-02, 1.4542e-02, 1.7914e-02,  ..., 2.9663e-02,\n",
      "          2.1317e-02, 1.5945e-02],\n",
      "         [5.9414e-04, 7.6866e-04, 9.1124e-04,  ..., 2.0390e-03,\n",
      "          1.5926e-03, 1.0376e-03],\n",
      "         [3.3531e-03, 4.3678e-03, 5.4779e-03,  ..., 1.6556e-02,\n",
      "          1.4702e-02, 1.1421e-02]],\n",
      "\n",
      "        [[9.0599e-05, 1.1140e-04, 1.3328e-04,  ..., 2.3706e-01,\n",
      "          2.2974e-01, 2.2485e-01],\n",
      "         [2.9624e-05, 3.4511e-05, 3.8803e-05,  ..., 2.0984e-01,\n",
      "          1.9263e-01, 1.7249e-01],\n",
      "         [1.3459e-04, 1.4901e-04, 1.5867e-04,  ..., 1.9189e-01,\n",
      "          1.8823e-01, 1.8542e-01],\n",
      "         ...,\n",
      "         [2.0615e-02, 3.0167e-02, 4.1595e-02,  ..., 1.1365e-01,\n",
      "          1.0040e-01, 6.2042e-02],\n",
      "         [1.2851e-04, 1.8859e-04, 2.7013e-04,  ..., 1.9730e-02,\n",
      "          1.8753e-02, 1.2711e-02],\n",
      "         [2.8896e-03, 4.0169e-03, 5.2872e-03,  ..., 6.9153e-02,\n",
      "          6.5186e-02, 4.5441e-02]],\n",
      "\n",
      "        [[9.4891e-05, 1.3006e-04, 1.6034e-04,  ..., 1.2244e-01,\n",
      "          6.9336e-02, 6.6101e-02],\n",
      "         [2.7061e-05, 3.2544e-05, 3.6180e-05,  ..., 2.3682e-01,\n",
      "          2.0312e-01, 1.9922e-01],\n",
      "         [8.0824e-05, 1.0437e-04, 1.1951e-04,  ..., 1.0687e-01,\n",
      "          5.2979e-02, 5.0293e-02],\n",
      "         ...,\n",
      "         [7.7248e-03, 9.6054e-03, 1.2405e-02,  ..., 2.1472e-01,\n",
      "          1.9214e-01, 1.3916e-01],\n",
      "         [1.1456e-04, 1.5557e-04, 2.0170e-04,  ..., 1.3965e-01,\n",
      "          1.2927e-01, 3.9825e-02],\n",
      "         [1.3933e-03, 1.7223e-03, 2.2259e-03,  ..., 3.6646e-01,\n",
      "          3.2935e-01, 1.8726e-01]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[0.0037, 0.0044, 0.0050,  ..., 0.2197, 0.2148, 0.2068],\n",
      "         [0.0008, 0.0011, 0.0013,  ..., 0.3257, 0.3145, 0.3015],\n",
      "         [0.0013, 0.0017, 0.0021,  ..., 0.3010, 0.2524, 0.2346],\n",
      "         ...,\n",
      "         [0.0009, 0.0011, 0.0013,  ..., 0.1483, 0.1241, 0.0987],\n",
      "         [0.0020, 0.0025, 0.0030,  ..., 0.2981, 0.2815, 0.2352],\n",
      "         [0.0018, 0.0023, 0.0026,  ..., 0.3494, 0.2686, 0.2146]],\n",
      "\n",
      "        [[0.0153, 0.0174, 0.0189,  ..., 0.0817, 0.0724, 0.0605],\n",
      "         [0.0036, 0.0045, 0.0054,  ..., 0.1237, 0.0763, 0.0535],\n",
      "         [0.0057, 0.0073, 0.0082,  ..., 0.0748, 0.0517, 0.0354],\n",
      "         ...,\n",
      "         [0.0019, 0.0024, 0.0028,  ..., 0.1731, 0.1635, 0.1075],\n",
      "         [0.0035, 0.0042, 0.0049,  ..., 0.0792, 0.0623, 0.0192],\n",
      "         [0.0032, 0.0038, 0.0043,  ..., 0.2379, 0.2124, 0.0817]],\n",
      "\n",
      "        [[0.0076, 0.0091, 0.0103,  ..., 0.1537, 0.1484, 0.1392],\n",
      "         [0.0017, 0.0022, 0.0028,  ..., 0.2617, 0.2417, 0.2169],\n",
      "         [0.0027, 0.0037, 0.0044,  ..., 0.1681, 0.1552, 0.1356],\n",
      "         ...,\n",
      "         [0.0008, 0.0010, 0.0013,  ..., 0.1521, 0.1194, 0.0675],\n",
      "         [0.0018, 0.0028, 0.0035,  ..., 0.2271, 0.2211, 0.2119],\n",
      "         [0.0019, 0.0026, 0.0029,  ..., 0.1646, 0.0958, 0.0754]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0244, 0.0290, 0.0320,  ..., 0.0456, 0.0432, 0.0363],\n",
      "         [0.0060, 0.0075, 0.0088,  ..., 0.0717, 0.0701, 0.0634],\n",
      "         [0.0096, 0.0122, 0.0137,  ..., 0.0670, 0.0612, 0.0540],\n",
      "         ...,\n",
      "         [0.0017, 0.0022, 0.0026,  ..., 0.0051, 0.0035, 0.0028],\n",
      "         [0.0086, 0.0112, 0.0137,  ..., 0.1718, 0.1307, 0.0515],\n",
      "         [0.0051, 0.0063, 0.0075,  ..., 0.0270, 0.0202, 0.0166]],\n",
      "\n",
      "        [[0.0158, 0.0175, 0.0182,  ..., 0.2161, 0.2074, 0.1918],\n",
      "         [0.0035, 0.0042, 0.0048,  ..., 0.2952, 0.2795, 0.2242],\n",
      "         [0.0053, 0.0069, 0.0075,  ..., 0.1687, 0.1615, 0.1478],\n",
      "         ...,\n",
      "         [0.0007, 0.0010, 0.0015,  ..., 0.0753, 0.0734, 0.0656],\n",
      "         [0.0062, 0.0074, 0.0082,  ..., 0.3088, 0.3030, 0.2966],\n",
      "         [0.0032, 0.0039, 0.0043,  ..., 0.1396, 0.1277, 0.1074]],\n",
      "\n",
      "        [[0.0063, 0.0077, 0.0088,  ..., 0.1395, 0.0780, 0.0749],\n",
      "         [0.0016, 0.0021, 0.0026,  ..., 0.3984, 0.3555, 0.3481],\n",
      "         [0.0030, 0.0040, 0.0047,  ..., 0.2100, 0.1772, 0.1729],\n",
      "         ...,\n",
      "         [0.0006, 0.0009, 0.0012,  ..., 0.1235, 0.1086, 0.0678],\n",
      "         [0.0023, 0.0030, 0.0036,  ..., 0.2544, 0.2491, 0.2443],\n",
      "         [0.0015, 0.0020, 0.0023,  ..., 0.2380, 0.2328, 0.2194]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward1>), tensor([[[1.0860e-04, 1.2779e-04, 1.3793e-04,  ..., 1.8518e-01,\n",
      "          1.3086e-01, 5.7617e-02],\n",
      "         [7.8964e-03, 1.0315e-02, 1.2329e-02,  ..., 1.1330e-02,\n",
      "          6.4011e-03, 4.1962e-03],\n",
      "         [3.7217e-04, 4.3821e-04, 4.9114e-04,  ..., 5.5542e-02,\n",
      "          4.3884e-02, 3.6377e-02],\n",
      "         ...,\n",
      "         [4.9973e-04, 8.2445e-04, 1.1101e-03,  ..., 6.5674e-02,\n",
      "          5.1941e-02, 3.5187e-02],\n",
      "         [8.8024e-04, 1.0242e-03, 1.1511e-03,  ..., 8.3313e-02,\n",
      "          7.3853e-02, 5.2063e-02],\n",
      "         [2.1095e-03, 2.8610e-03, 3.6163e-03,  ..., 3.1372e-02,\n",
      "          2.4994e-02, 1.9257e-02]],\n",
      "\n",
      "        [[9.9421e-05, 1.2267e-04, 1.3840e-04,  ..., 1.4816e-02,\n",
      "          1.3130e-02, 1.1192e-02],\n",
      "         [1.4542e-02, 1.8738e-02, 2.1927e-02,  ..., 4.1809e-02,\n",
      "          3.6560e-02, 3.1525e-02],\n",
      "         [1.8501e-04, 2.2566e-04, 2.7514e-04,  ..., 1.2032e-02,\n",
      "          9.2087e-03, 7.1259e-03],\n",
      "         ...,\n",
      "         [3.8552e-04, 5.2214e-04, 6.7616e-04,  ..., 1.1658e-02,\n",
      "          1.0223e-02, 7.9346e-03],\n",
      "         [6.8665e-04, 7.7200e-04, 8.8501e-04,  ..., 7.7942e-02,\n",
      "          5.2704e-02, 2.6428e-02],\n",
      "         [3.5095e-03, 4.3678e-03, 5.3749e-03,  ..., 1.4923e-02,\n",
      "          1.4069e-02, 1.2123e-02]],\n",
      "\n",
      "        [[4.9949e-05, 6.2585e-05, 7.4446e-05,  ..., 3.1281e-02,\n",
      "          2.7573e-02, 2.3544e-02],\n",
      "         [1.0086e-02, 1.4076e-02, 1.8967e-02,  ..., 7.2212e-03,\n",
      "          5.5962e-03, 4.5013e-03],\n",
      "         [3.2425e-04, 3.8004e-04, 4.5061e-04,  ..., 1.5839e-02,\n",
      "          1.4595e-02, 1.2886e-02],\n",
      "         ...,\n",
      "         [2.3067e-04, 3.3951e-04, 4.6849e-04,  ..., 9.7656e-03,\n",
      "          7.4577e-03, 6.5918e-03],\n",
      "         [4.5562e-04, 5.5075e-04, 6.6376e-04,  ..., 3.6194e-02,\n",
      "          2.9266e-02, 1.9028e-02],\n",
      "         [2.6627e-03, 3.4428e-03, 4.3716e-03,  ..., 1.6678e-02,\n",
      "          1.2978e-02, 9.3689e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[5.5027e-04, 6.7520e-04, 7.5769e-04,  ..., 4.3530e-01,\n",
      "          4.2920e-01, 7.4158e-02],\n",
      "         [6.7200e-02, 8.6182e-02, 1.0199e-01,  ..., 2.0256e-03,\n",
      "          1.5068e-03, 8.7309e-04],\n",
      "         [2.2945e-03, 2.6798e-03, 3.0594e-03,  ..., 1.4931e-02,\n",
      "          1.3664e-02, 1.0277e-02],\n",
      "         ...,\n",
      "         [1.1894e-02, 1.9073e-02, 2.5696e-02,  ..., 7.7057e-03,\n",
      "          6.8588e-03, 4.8904e-03],\n",
      "         [2.4170e-02, 3.1036e-02, 3.8147e-02,  ..., 1.2642e-02,\n",
      "          1.1383e-02, 9.4681e-03],\n",
      "         [5.4077e-02, 7.6050e-02, 9.9304e-02,  ..., 1.7822e-02,\n",
      "          1.4969e-02, 1.0544e-02]],\n",
      "\n",
      "        [[6.7234e-05, 8.9049e-05, 1.2255e-04,  ..., 2.7075e-01,\n",
      "          2.6978e-01, 2.6807e-01],\n",
      "         [2.0020e-02, 2.5650e-02, 3.4241e-02,  ..., 2.6581e-02,\n",
      "          2.5116e-02, 1.9745e-02],\n",
      "         [2.7895e-04, 3.5691e-04, 4.7612e-04,  ..., 7.3425e-02,\n",
      "          7.2021e-02, 6.9214e-02],\n",
      "         ...,\n",
      "         [7.7677e-04, 1.1196e-03, 1.4811e-03,  ..., 2.0996e-02,\n",
      "          2.0050e-02, 1.9043e-02],\n",
      "         [1.6165e-03, 1.9217e-03, 2.2736e-03,  ..., 6.5186e-02,\n",
      "          6.2561e-02, 5.8899e-02],\n",
      "         [6.4392e-03, 8.4152e-03, 1.1459e-02,  ..., 6.0883e-02,\n",
      "          6.0181e-02, 5.8624e-02]],\n",
      "\n",
      "        [[3.4511e-05, 4.2737e-05, 5.2869e-05,  ..., 2.0340e-02,\n",
      "          1.5274e-02, 1.4732e-02],\n",
      "         [6.3057e-03, 8.7204e-03, 1.2184e-02,  ..., 6.2218e-03,\n",
      "          5.5428e-03, 3.3436e-03],\n",
      "         [1.4305e-04, 1.8990e-04, 2.4676e-04,  ..., 9.9106e-03,\n",
      "          9.1858e-03, 8.5831e-03],\n",
      "         ...,\n",
      "         [2.2566e-04, 3.6716e-04, 5.3692e-04,  ..., 7.2098e-03,\n",
      "          5.9967e-03, 5.5580e-03],\n",
      "         [4.8447e-04, 5.9223e-04, 7.2002e-04,  ..., 3.5583e-02,\n",
      "          3.2074e-02, 2.8702e-02],\n",
      "         [2.2793e-03, 2.9488e-03, 3.8109e-03,  ..., 1.4000e-02,\n",
      "          1.1169e-02, 8.5144e-03]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[0.0026, 0.0037, 0.0050,  ..., 0.1210, 0.1022, 0.0827],\n",
      "         [0.0022, 0.0032, 0.0052,  ..., 0.0293, 0.0285, 0.0276],\n",
      "         [0.0013, 0.0019, 0.0029,  ..., 0.0409, 0.0372, 0.0280],\n",
      "         ...,\n",
      "         [0.0018, 0.0023, 0.0026,  ..., 0.0310, 0.0251, 0.0174],\n",
      "         [0.0021, 0.0025, 0.0027,  ..., 0.0733, 0.0590, 0.0471],\n",
      "         [0.0129, 0.0209, 0.0318,  ..., 0.0991, 0.0962, 0.0943]],\n",
      "\n",
      "        [[0.0063, 0.0073, 0.0080,  ..., 0.0524, 0.0488, 0.0352],\n",
      "         [0.0061, 0.0070, 0.0080,  ..., 0.0165, 0.0162, 0.0159],\n",
      "         [0.0035, 0.0042, 0.0048,  ..., 0.0920, 0.0878, 0.0810],\n",
      "         ...,\n",
      "         [0.0042, 0.0049, 0.0054,  ..., 0.0155, 0.0149, 0.0113],\n",
      "         [0.0020, 0.0023, 0.0025,  ..., 0.0206, 0.0174, 0.0149],\n",
      "         [0.0126, 0.0189, 0.0268,  ..., 0.1260, 0.1257, 0.1255]],\n",
      "\n",
      "        [[0.0039, 0.0054, 0.0073,  ..., 0.0657, 0.0591, 0.0469],\n",
      "         [0.0030, 0.0046, 0.0080,  ..., 0.0479, 0.0475, 0.0468],\n",
      "         [0.0019, 0.0029, 0.0045,  ..., 0.0291, 0.0284, 0.0275],\n",
      "         ...,\n",
      "         [0.0024, 0.0030, 0.0036,  ..., 0.0192, 0.0150, 0.0126],\n",
      "         [0.0015, 0.0017, 0.0018,  ..., 0.0356, 0.0317, 0.0257],\n",
      "         [0.0108, 0.0149, 0.0238,  ..., 0.0930, 0.0921, 0.0909]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0379, 0.0504, 0.0581,  ..., 0.0719, 0.0615, 0.0451],\n",
      "         [0.0283, 0.0356, 0.0472,  ..., 0.0419, 0.0416, 0.0410],\n",
      "         [0.0161, 0.0216, 0.0287,  ..., 0.1923, 0.1735, 0.1605],\n",
      "         ...,\n",
      "         [0.0235, 0.0289, 0.0334,  ..., 0.0114, 0.0087, 0.0063],\n",
      "         [0.0073, 0.0083, 0.0090,  ..., 0.0680, 0.0594, 0.0504],\n",
      "         [0.0529, 0.0720, 0.1147,  ..., 0.1884, 0.1880, 0.1875]],\n",
      "\n",
      "        [[0.0184, 0.0217, 0.0238,  ..., 0.0760, 0.0742, 0.0703],\n",
      "         [0.0190, 0.0213, 0.0241,  ..., 0.1009, 0.1004, 0.0992],\n",
      "         [0.0109, 0.0129, 0.0152,  ..., 0.0534, 0.0518, 0.0506],\n",
      "         ...,\n",
      "         [0.0112, 0.0135, 0.0155,  ..., 0.0117, 0.0105, 0.0079],\n",
      "         [0.0035, 0.0041, 0.0045,  ..., 0.0385, 0.0364, 0.0314],\n",
      "         [0.0194, 0.0295, 0.0416,  ..., 0.1763, 0.1759, 0.1752]],\n",
      "\n",
      "        [[0.0039, 0.0048, 0.0059,  ..., 0.0501, 0.0478, 0.0464],\n",
      "         [0.0043, 0.0052, 0.0069,  ..., 0.0163, 0.0162, 0.0157],\n",
      "         [0.0023, 0.0027, 0.0035,  ..., 0.0157, 0.0154, 0.0149],\n",
      "         ...,\n",
      "         [0.0013, 0.0017, 0.0021,  ..., 0.0236, 0.0214, 0.0154],\n",
      "         [0.0010, 0.0012, 0.0013,  ..., 0.0249, 0.0237, 0.0210],\n",
      "         [0.0099, 0.0143, 0.0183,  ..., 0.0679, 0.0674, 0.0668]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward1>), tensor([[[4.0233e-05, 5.7817e-05, 6.4552e-05,  ..., 4.1695e-03,\n",
      "          3.9597e-03, 3.5400e-03],\n",
      "         [1.1444e-05, 1.6570e-05, 1.9789e-05,  ..., 1.2708e-04,\n",
      "          1.1325e-04, 1.0902e-04],\n",
      "         [1.2517e-05, 2.5272e-05, 6.1750e-05,  ..., 4.1723e-07,\n",
      "          2.9802e-07, 2.9802e-07],\n",
      "         ...,\n",
      "         [7.7486e-07, 1.6689e-06, 2.1458e-06,  ..., 2.6643e-05,\n",
      "          1.2517e-05, 1.2398e-05],\n",
      "         [2.0576e-04, 2.4199e-04, 2.5105e-04,  ..., 3.1342e-02,\n",
      "          2.8870e-02, 8.6594e-03],\n",
      "         [7.0512e-05, 9.0957e-05, 9.6917e-05,  ..., 1.8578e-03,\n",
      "          1.7424e-03, 1.7090e-03]],\n",
      "\n",
      "        [[3.2783e-06, 7.6294e-06, 9.7752e-06,  ..., 1.5011e-03,\n",
      "          1.3857e-03, 7.3004e-04],\n",
      "         [6.5565e-07, 7.1526e-07, 7.7486e-07,  ..., 1.0192e-04,\n",
      "          9.3579e-05, 5.3108e-05],\n",
      "         [5.0664e-06, 5.1856e-06, 1.2279e-05,  ..., 5.3287e-05,\n",
      "          5.2869e-05, 1.3232e-05],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0192e-05,\n",
      "          8.8215e-06, 3.3975e-06],\n",
      "         [5.9605e-08, 5.9605e-08, 3.6955e-06,  ..., 3.4561e-03,\n",
      "          3.1834e-03, 1.5087e-03],\n",
      "         [4.1723e-07, 1.0729e-06, 2.3842e-06,  ..., 3.3927e-04,\n",
      "          2.8563e-04, 1.2171e-04]],\n",
      "\n",
      "        [[1.1861e-05, 1.9372e-05, 2.1875e-05,  ..., 2.6436e-03,\n",
      "          1.7576e-03, 6.7902e-04],\n",
      "         [1.3709e-06, 2.0266e-06, 2.0862e-06,  ..., 7.5221e-05,\n",
      "          4.4703e-05, 1.9491e-05],\n",
      "         [2.4199e-05, 3.3498e-05, 3.4630e-05,  ..., 1.8418e-05,\n",
      "          1.8001e-05, 1.1623e-05],\n",
      "         ...,\n",
      "         [1.7881e-07, 2.3842e-07, 2.3842e-07,  ..., 4.4584e-05,\n",
      "          3.8981e-05, 2.9266e-05],\n",
      "         [1.6093e-06, 2.5034e-06, 3.3975e-06,  ..., 7.4844e-03,\n",
      "          6.1378e-03, 2.9163e-03],\n",
      "         [2.5034e-06, 3.5763e-06, 4.4703e-06,  ..., 1.4095e-03,\n",
      "          8.1635e-04, 1.4734e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[3.8767e-04, 4.8923e-04, 5.7125e-04,  ..., 1.3208e-03,\n",
      "          1.2894e-03, 1.2617e-03],\n",
      "         [2.3842e-06, 3.5167e-06, 4.1127e-06,  ..., 5.3644e-07,\n",
      "          5.3644e-07, 5.3644e-07],\n",
      "         [8.9228e-05, 1.3053e-04, 1.6475e-04,  ..., 2.1458e-06,\n",
      "          2.3842e-07, 5.9605e-08],\n",
      "         ...,\n",
      "         [2.9802e-07, 5.9605e-07, 6.5565e-07,  ..., 4.2319e-06,\n",
      "          4.0531e-06, 1.4901e-06],\n",
      "         [6.0081e-05, 9.2983e-05, 1.0699e-04,  ..., 6.0797e-04,\n",
      "          4.3631e-04, 2.7847e-04],\n",
      "         [1.5736e-05, 3.4690e-05, 3.8087e-05,  ..., 1.1802e-05,\n",
      "          1.0610e-05, 1.0312e-05]],\n",
      "\n",
      "        [[3.5703e-05, 4.5419e-05, 4.8518e-05,  ..., 5.3596e-04,\n",
      "          4.5824e-04, 4.3225e-04],\n",
      "         [3.2783e-06, 4.5300e-06, 4.6492e-06,  ..., 5.1856e-06,\n",
      "          5.0664e-06, 4.7088e-06],\n",
      "         [3.9577e-05, 5.5492e-05, 5.9962e-05,  ..., 1.0431e-05,\n",
      "          1.0431e-05, 4.4703e-06],\n",
      "         ...,\n",
      "         [1.1921e-07, 8.9407e-07, 8.9407e-07,  ..., 7.2718e-06,\n",
      "          6.7353e-06, 6.4969e-06],\n",
      "         [1.6582e-04, 1.7798e-04, 1.8787e-04,  ..., 7.3509e-03,\n",
      "          7.0534e-03, 6.8893e-03],\n",
      "         [3.4869e-05, 1.0037e-04, 1.0425e-04,  ..., 1.2589e-04,\n",
      "          1.1939e-04, 1.1796e-04]],\n",
      "\n",
      "        [[1.4842e-05, 1.5318e-05, 2.6345e-05,  ..., 6.3171e-03,\n",
      "          6.2408e-03, 4.6234e-03],\n",
      "         [4.2319e-06, 4.5300e-06, 7.4506e-06,  ..., 3.3164e-04,\n",
      "          3.3116e-04, 1.8704e-04],\n",
      "         [6.0141e-05, 6.0260e-05, 6.3300e-05,  ..., 2.9206e-06,\n",
      "          2.9206e-06, 2.9206e-06],\n",
      "         ...,\n",
      "         [2.9802e-07, 3.5763e-07, 5.3644e-07,  ..., 3.4213e-05,\n",
      "          3.4034e-05, 2.8253e-05],\n",
      "         [2.9325e-05, 2.9325e-05, 3.3677e-05,  ..., 1.2360e-03,\n",
      "          1.1063e-03, 8.6260e-04],\n",
      "         [8.1062e-06, 8.8215e-06, 1.4663e-05,  ..., 4.3030e-03,\n",
      "          4.2801e-03, 2.1114e-03]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[2.6226e-06, 3.5167e-06, 4.5300e-06,  ..., 1.8018e-01,\n",
      "          1.2622e-01, 9.4849e-02],\n",
      "         [2.7418e-06, 4.1723e-06, 5.7220e-06,  ..., 3.9337e-02,\n",
      "          3.1433e-02, 2.2125e-02],\n",
      "         [1.9073e-06, 2.5630e-06, 3.2187e-06,  ..., 2.6294e-01,\n",
      "          1.8384e-01, 1.0919e-01],\n",
      "         ...,\n",
      "         [8.9741e-04, 1.2360e-03, 1.6289e-03,  ..., 1.9409e-01,\n",
      "          1.1566e-01, 5.4901e-02],\n",
      "         [6.2799e-04, 8.4352e-04, 1.0214e-03,  ..., 1.2140e-01,\n",
      "          1.0077e-01, 7.9163e-02],\n",
      "         [1.4076e-03, 1.8167e-03, 2.1992e-03,  ..., 2.5562e-01,\n",
      "          2.1643e-01, 1.6333e-01]],\n",
      "\n",
      "        [[2.1458e-06, 2.6822e-06, 3.3975e-06,  ..., 5.3528e-02,\n",
      "          4.8157e-02, 4.1656e-02],\n",
      "         [4.1127e-06, 5.1260e-06, 6.1989e-06,  ..., 2.2491e-02,\n",
      "          1.9333e-02, 1.2253e-02],\n",
      "         [1.6093e-06, 1.9670e-06, 2.3842e-06,  ..., 9.4421e-02,\n",
      "          8.3008e-02, 7.0129e-02],\n",
      "         ...,\n",
      "         [4.8018e-04, 6.1226e-04, 7.4387e-04,  ..., 5.5237e-02,\n",
      "          2.7344e-02, 2.0279e-02],\n",
      "         [3.3712e-04, 4.3035e-04, 5.2261e-04,  ..., 6.7078e-02,\n",
      "          5.4688e-02, 3.5461e-02],\n",
      "         [5.9128e-04, 7.3957e-04, 9.3985e-04,  ..., 8.4656e-02,\n",
      "          7.2449e-02, 5.7556e-02]],\n",
      "\n",
      "        [[4.9472e-06, 5.6624e-06, 6.3777e-06,  ..., 3.5309e-02,\n",
      "          3.0411e-02, 2.7054e-02],\n",
      "         [4.7088e-06, 6.1989e-06, 7.0930e-06,  ..., 3.3478e-02,\n",
      "          2.7191e-02, 2.2278e-02],\n",
      "         [2.7418e-06, 3.3975e-06, 3.9339e-06,  ..., 1.9788e-01,\n",
      "          1.7212e-01, 1.2219e-01],\n",
      "         ...,\n",
      "         [2.6417e-04, 3.4571e-04, 4.4203e-04,  ..., 9.8572e-02,\n",
      "          6.8359e-02, 4.4769e-02],\n",
      "         [3.3283e-04, 4.2009e-04, 5.0163e-04,  ..., 8.9722e-02,\n",
      "          7.1655e-02, 5.1147e-02],\n",
      "         [9.6798e-04, 1.2074e-03, 1.4620e-03,  ..., 2.1936e-01,\n",
      "          1.6589e-01, 1.1249e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[4.3452e-05, 5.3883e-05, 6.4254e-05,  ..., 1.9073e-02,\n",
      "          1.5549e-02, 1.1452e-02],\n",
      "         [1.8060e-05, 2.3425e-05, 2.8193e-05,  ..., 5.7459e-04,\n",
      "          4.3154e-04, 3.4690e-04],\n",
      "         [2.7120e-05, 3.3975e-05, 4.0352e-05,  ..., 2.5977e-01,\n",
      "          2.3291e-01, 1.9128e-01],\n",
      "         ...,\n",
      "         [1.0557e-03, 1.3027e-03, 1.5383e-03,  ..., 5.9175e-04,\n",
      "          3.7384e-04, 2.0885e-04],\n",
      "         [5.1785e-04, 6.8808e-04, 8.5306e-04,  ..., 1.0805e-03,\n",
      "          8.6117e-04, 6.4278e-04],\n",
      "         [4.1199e-03, 5.4207e-03, 6.8893e-03,  ..., 6.1798e-03,\n",
      "          5.3139e-03, 4.6310e-03]],\n",
      "\n",
      "        [[6.7949e-06, 9.2387e-06, 1.2040e-05,  ..., 3.7415e-02,\n",
      "          3.6469e-02, 3.4882e-02],\n",
      "         [4.1127e-06, 5.7817e-06, 7.1526e-06,  ..., 3.1372e-02,\n",
      "          3.0853e-02, 2.8946e-02],\n",
      "         [4.4107e-06, 6.1989e-06, 7.7486e-06,  ..., 3.8940e-01,\n",
      "          3.7720e-01, 3.4619e-01],\n",
      "         ...,\n",
      "         [2.8038e-04, 4.8399e-04, 6.5947e-04,  ..., 1.5343e-02,\n",
      "          1.4771e-02, 6.9733e-03],\n",
      "         [2.2912e-04, 4.6802e-04, 7.0190e-04,  ..., 4.6326e-02,\n",
      "          4.5135e-02, 3.9124e-02],\n",
      "         [1.1711e-03, 1.7347e-03, 2.3155e-03,  ..., 3.9764e-02,\n",
      "          3.6774e-02, 2.3804e-02]],\n",
      "\n",
      "        [[2.5630e-06, 3.3975e-06, 3.9339e-06,  ..., 4.0283e-02,\n",
      "          3.8544e-02, 3.7292e-02],\n",
      "         [2.3842e-06, 3.3975e-06, 4.2319e-06,  ..., 3.1860e-02,\n",
      "          2.8976e-02, 2.3727e-02],\n",
      "         [2.9802e-06, 3.9339e-06, 4.7088e-06,  ..., 1.5369e-01,\n",
      "          1.3696e-01, 1.1560e-01],\n",
      "         ...,\n",
      "         [3.3784e-04, 4.2033e-04, 4.9782e-04,  ..., 7.0801e-02,\n",
      "          6.7810e-02, 4.0558e-02],\n",
      "         [2.0564e-04, 2.7370e-04, 3.3164e-04,  ..., 4.7882e-02,\n",
      "          4.3732e-02, 3.1189e-02],\n",
      "         [6.6948e-04, 9.3412e-04, 1.2016e-03,  ..., 9.8267e-02,\n",
      "          9.4604e-02, 8.3069e-02]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[5.4970e-03, 6.6986e-03, 8.0872e-03,  ..., 3.1055e-01,\n",
      "          2.4463e-01, 1.3464e-01],\n",
      "         [6.0158e-03, 7.7629e-03, 9.3842e-03,  ..., 2.5439e-01,\n",
      "          1.6309e-01, 5.9906e-02],\n",
      "         [1.0529e-03, 1.3514e-03, 1.6441e-03,  ..., 4.4067e-02,\n",
      "          1.7059e-02, 9.8343e-03],\n",
      "         ...,\n",
      "         [1.1873e-03, 1.4267e-03, 1.6546e-03,  ..., 2.4634e-01,\n",
      "          2.2839e-01, 2.1045e-01],\n",
      "         [1.6952e-04, 2.1565e-04, 2.5749e-04,  ..., 2.5049e-01,\n",
      "          2.0996e-01, 1.7090e-01],\n",
      "         [6.2287e-05, 7.6532e-05, 9.5010e-05,  ..., 9.2041e-02,\n",
      "          9.0210e-02, 8.6609e-02]],\n",
      "\n",
      "        [[5.6648e-03, 8.0490e-03, 1.0277e-02,  ..., 4.2389e-02,\n",
      "          3.6285e-02, 3.1021e-02],\n",
      "         [1.1017e-02, 1.5533e-02, 2.0218e-02,  ..., 1.2589e-02,\n",
      "          1.0307e-02, 7.0457e-03],\n",
      "         [1.5438e-04, 1.9062e-04, 2.3067e-04,  ..., 3.5095e-03,\n",
      "          2.7504e-03, 1.8454e-03],\n",
      "         ...,\n",
      "         [2.8944e-04, 3.6287e-04, 4.2534e-04,  ..., 6.2317e-02,\n",
      "          5.1697e-02, 3.5858e-02],\n",
      "         [2.1148e-04, 2.7418e-04, 3.2425e-04,  ..., 8.1543e-02,\n",
      "          6.8481e-02, 4.8950e-02],\n",
      "         [1.9217e-04, 2.3556e-04, 2.7347e-04,  ..., 9.8572e-02,\n",
      "          9.7778e-02, 9.7046e-02]],\n",
      "\n",
      "        [[5.1956e-03, 6.3133e-03, 7.7515e-03,  ..., 1.7468e-01,\n",
      "          1.4722e-01, 1.2329e-01],\n",
      "         [3.6774e-03, 4.6425e-03, 5.8365e-03,  ..., 8.7402e-02,\n",
      "          3.9246e-02, 2.4643e-02],\n",
      "         [2.0349e-04, 2.8634e-04, 3.5930e-04,  ..., 3.1204e-02,\n",
      "          2.1805e-02, 1.4992e-02],\n",
      "         ...,\n",
      "         [5.4789e-04, 6.5804e-04, 7.7677e-04,  ..., 1.7944e-01,\n",
      "          1.6394e-01, 1.1755e-01],\n",
      "         [1.9276e-04, 2.3293e-04, 2.7561e-04,  ..., 1.4893e-01,\n",
      "          1.1554e-01, 7.2571e-02],\n",
      "         [1.1289e-04, 1.2887e-04, 1.5152e-04,  ..., 1.0193e-01,\n",
      "          1.0120e-01, 9.9854e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.6123e-02, 3.3203e-02, 3.9398e-02,  ..., 4.9341e-01,\n",
      "          4.6826e-01, 3.0103e-01],\n",
      "         [1.4465e-02, 1.8875e-02, 2.3087e-02,  ..., 2.4445e-02,\n",
      "          2.0370e-02, 1.6190e-02],\n",
      "         [2.3365e-04, 2.9230e-04, 3.5071e-04,  ..., 2.6035e-03,\n",
      "          1.8950e-03, 1.1568e-03],\n",
      "         ...,\n",
      "         [1.2405e-02, 1.4565e-02, 1.6937e-02,  ..., 7.0984e-02,\n",
      "          5.3070e-02, 4.4525e-02],\n",
      "         [7.9498e-03, 9.9258e-03, 1.2375e-02,  ..., 3.9246e-02,\n",
      "          3.3875e-02, 2.9099e-02],\n",
      "         [3.2997e-03, 4.0169e-03, 5.1079e-03,  ..., 1.7041e-01,\n",
      "          1.6931e-01, 1.6870e-01]],\n",
      "\n",
      "        [[4.5052e-03, 5.4741e-03, 6.5842e-03,  ..., 1.7456e-01,\n",
      "          1.7114e-01, 1.6357e-01],\n",
      "         [1.0567e-02, 1.2894e-02, 1.5671e-02,  ..., 4.9561e-02,\n",
      "          4.4250e-02, 2.6886e-02],\n",
      "         [2.0123e-04, 3.3927e-04, 4.8566e-04,  ..., 9.1400e-03,\n",
      "          8.8654e-03, 8.1711e-03],\n",
      "         ...,\n",
      "         [6.3753e-04, 9.8896e-04, 1.3046e-03,  ..., 1.0822e-01,\n",
      "          1.0315e-01, 8.6670e-02],\n",
      "         [2.8062e-04, 4.7374e-04, 6.6280e-04,  ..., 6.7139e-02,\n",
      "          6.2073e-02, 4.0466e-02],\n",
      "         [1.0854e-04, 1.4198e-04, 1.7488e-04,  ..., 1.4587e-01,\n",
      "          1.4551e-01, 1.4502e-01]],\n",
      "\n",
      "        [[3.0422e-03, 4.0817e-03, 4.6577e-03,  ..., 1.9800e-01,\n",
      "          1.8701e-01, 1.8262e-01],\n",
      "         [3.9444e-03, 5.4626e-03, 6.2218e-03,  ..., 3.7354e-02,\n",
      "          3.6011e-02, 3.4332e-02],\n",
      "         [1.8311e-04, 2.4724e-04, 2.8563e-04,  ..., 1.1528e-02,\n",
      "          9.7809e-03, 6.5613e-03],\n",
      "         ...,\n",
      "         [1.2093e-03, 1.5411e-03, 1.8568e-03,  ..., 4.8291e-01,\n",
      "          4.8145e-01, 4.7632e-01],\n",
      "         [1.9503e-04, 2.5058e-04, 3.1090e-04,  ..., 1.0199e-01,\n",
      "          9.7717e-02, 8.7158e-02],\n",
      "         [7.7963e-05, 9.1314e-05, 1.0759e-04,  ..., 2.6108e-02,\n",
      "          2.5864e-02, 2.5162e-02]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[6.3467e-04, 7.8583e-04, 9.0218e-04,  ..., 3.1204e-02,\n",
      "          2.5589e-02, 1.8906e-02],\n",
      "         [1.2541e-03, 1.7567e-03, 2.0981e-03,  ..., 1.9058e-02,\n",
      "          1.3138e-02, 7.9575e-03],\n",
      "         [2.4929e-03, 3.5572e-03, 4.2763e-03,  ..., 1.6687e-01,\n",
      "          1.3318e-01, 9.7229e-02],\n",
      "         ...,\n",
      "         [2.3365e-04, 2.8729e-04, 3.3736e-04,  ..., 1.6418e-01,\n",
      "          1.3110e-01, 7.8491e-02],\n",
      "         [6.7663e-04, 8.7166e-04, 1.0233e-03,  ..., 1.3501e-01,\n",
      "          1.1987e-01, 9.5398e-02],\n",
      "         [4.5204e-03, 5.4855e-03, 6.7139e-03,  ..., 8.8623e-02,\n",
      "          8.2886e-02, 6.9946e-02]],\n",
      "\n",
      "        [[6.9380e-05, 8.5533e-05, 9.7692e-05,  ..., 6.2332e-03,\n",
      "          4.0665e-03, 2.4834e-03],\n",
      "         [4.6432e-05, 5.6982e-05, 6.8247e-05,  ..., 1.0223e-03,\n",
      "          9.9468e-04, 5.7840e-04],\n",
      "         [4.6039e-04, 6.2704e-04, 7.9918e-04,  ..., 7.4120e-03,\n",
      "          6.5346e-03, 3.8795e-03],\n",
      "         ...,\n",
      "         [2.2471e-04, 3.0279e-04, 3.9601e-04,  ..., 2.1896e-02,\n",
      "          1.8539e-02, 1.6205e-02],\n",
      "         [9.4700e-04, 1.3638e-03, 1.8702e-03,  ..., 7.1335e-03,\n",
      "          5.6725e-03, 4.6692e-03],\n",
      "         [3.7670e-03, 4.6806e-03, 5.9280e-03,  ..., 4.4708e-02,\n",
      "          3.9490e-02, 3.5828e-02]],\n",
      "\n",
      "        [[1.4806e-04, 1.6356e-04, 1.8084e-04,  ..., 1.5686e-02,\n",
      "          1.2596e-02, 8.0032e-03],\n",
      "         [1.6439e-04, 2.1887e-04, 2.5940e-04,  ..., 1.7838e-02,\n",
      "          1.3611e-02, 9.5673e-03],\n",
      "         [7.4244e-04, 9.0075e-04, 1.0242e-03,  ..., 1.9727e-01,\n",
      "          1.4453e-01, 9.0820e-02],\n",
      "         ...,\n",
      "         [3.6740e-04, 4.9496e-04, 6.4516e-04,  ..., 1.1383e-01,\n",
      "          9.7961e-02, 7.3975e-02],\n",
      "         [1.9150e-03, 3.0022e-03, 4.0207e-03,  ..., 6.0150e-02,\n",
      "          2.6001e-02, 1.5640e-02],\n",
      "         [3.0365e-03, 3.7231e-03, 4.8485e-03,  ..., 3.9520e-02,\n",
      "          3.7384e-02, 3.3356e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.3943e-03, 1.6956e-03, 1.9951e-03,  ..., 1.2767e-04,\n",
      "          1.1683e-04, 1.0228e-04],\n",
      "         [2.2507e-03, 2.5558e-03, 2.8610e-03,  ..., 1.5020e-05,\n",
      "          9.8348e-06, 7.3910e-06],\n",
      "         [2.3479e-03, 2.7580e-03, 3.1662e-03,  ..., 7.2670e-04,\n",
      "          4.9067e-04, 3.0065e-04],\n",
      "         ...,\n",
      "         [3.3092e-03, 3.9978e-03, 4.7493e-03,  ..., 5.3894e-02,\n",
      "          4.6967e-02, 3.8818e-02],\n",
      "         [6.6605e-03, 8.5831e-03, 1.0521e-02,  ..., 5.7640e-03,\n",
      "          4.2076e-03, 3.1567e-03],\n",
      "         [2.1210e-02, 2.5543e-02, 3.0472e-02,  ..., 1.6602e-01,\n",
      "          1.5979e-01, 1.3831e-01]],\n",
      "\n",
      "        [[1.2946e-04, 2.2769e-04, 2.6822e-04,  ..., 6.9475e-04,\n",
      "          6.3324e-04, 4.1413e-04],\n",
      "         [1.0147e-03, 1.2960e-03, 1.5097e-03,  ..., 7.1049e-04,\n",
      "          7.0047e-04, 6.7854e-04],\n",
      "         [5.6458e-04, 9.0075e-04, 1.1692e-03,  ..., 1.8120e-03,\n",
      "          1.4877e-03, 7.3147e-04],\n",
      "         ...,\n",
      "         [3.7909e-04, 7.0858e-04, 9.4271e-04,  ..., 4.0161e-02,\n",
      "          3.7079e-02, 2.7435e-02],\n",
      "         [1.0805e-03, 2.1057e-03, 2.9335e-03,  ..., 2.7802e-02,\n",
      "          2.6031e-02, 1.4648e-02],\n",
      "         [4.4594e-03, 8.5144e-03, 1.1719e-02,  ..., 1.3232e-01,\n",
      "          1.3159e-01, 1.2720e-01]],\n",
      "\n",
      "        [[1.3876e-04, 1.6999e-04, 2.4331e-04,  ..., 5.5733e-03,\n",
      "          5.1994e-03, 4.8218e-03],\n",
      "         [9.1493e-05, 1.0180e-04, 1.1617e-04,  ..., 1.2070e-02,\n",
      "          1.1086e-02, 9.9869e-03],\n",
      "         [4.4346e-04, 5.4407e-04, 6.2466e-04,  ..., 1.2610e-01,\n",
      "          1.1346e-01, 8.7036e-02],\n",
      "         ...,\n",
      "         [9.5129e-05, 1.2875e-04, 1.8811e-04,  ..., 4.6448e-02,\n",
      "          4.2328e-02, 4.0253e-02],\n",
      "         [2.7418e-04, 4.1413e-04, 6.3944e-04,  ..., 1.8936e-02,\n",
      "          1.7807e-02, 1.3359e-02],\n",
      "         [2.3842e-03, 3.0766e-03, 4.4060e-03,  ..., 4.6204e-02,\n",
      "          4.4434e-02, 4.3152e-02]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[1.3351e-05, 1.8477e-05, 2.2173e-05,  ..., 1.1639e-01,\n",
      "          8.8013e-02, 4.7485e-02],\n",
      "         [4.1127e-06, 5.4836e-06, 6.4969e-06,  ..., 1.6956e-01,\n",
      "          1.2671e-01, 7.7942e-02],\n",
      "         [3.2365e-05, 4.4465e-05, 5.4657e-05,  ..., 8.1543e-02,\n",
      "          6.0547e-02, 3.6469e-02],\n",
      "         ...,\n",
      "         [1.6093e-04, 1.9431e-04, 5.5838e-04,  ..., 9.9030e-03,\n",
      "          6.9847e-03, 1.0204e-03],\n",
      "         [9.9540e-06, 1.5497e-05, 3.5942e-05,  ..., 5.1618e-05,\n",
      "          4.3690e-05, 3.5763e-05],\n",
      "         [8.3447e-07, 9.5367e-07, 2.3246e-06,  ..., 2.7332e-03,\n",
      "          1.7328e-03, 1.1244e-03]],\n",
      "\n",
      "        [[2.6822e-05, 3.4988e-05, 4.2439e-05,  ..., 2.9144e-02,\n",
      "          2.0950e-02, 1.7822e-02],\n",
      "         [1.9372e-05, 2.6345e-05, 3.1769e-05,  ..., 2.4994e-02,\n",
      "          2.1164e-02, 1.8814e-02],\n",
      "         [1.7655e-04, 2.3353e-04, 2.7275e-04,  ..., 9.3872e-02,\n",
      "          8.7341e-02, 7.9590e-02],\n",
      "         ...,\n",
      "         [7.6413e-05, 3.1185e-04, 3.6097e-04,  ..., 1.6804e-03,\n",
      "          1.4944e-03, 4.8232e-04],\n",
      "         [6.7532e-05, 2.9969e-04, 3.0661e-04,  ..., 3.0935e-05,\n",
      "          2.8372e-05, 1.8358e-05],\n",
      "         [6.7353e-06, 2.4855e-05, 2.6226e-05,  ..., 8.9359e-04,\n",
      "          7.0238e-04, 5.4455e-04]],\n",
      "\n",
      "        [[1.3590e-05, 1.5974e-05, 1.8597e-05,  ..., 5.6091e-02,\n",
      "          4.5227e-02, 3.5248e-02],\n",
      "         [5.3644e-06, 6.3181e-06, 7.3314e-06,  ..., 1.9958e-01,\n",
      "          1.7725e-01, 1.5222e-01],\n",
      "         [6.5804e-05, 8.3029e-05, 9.8765e-05,  ..., 5.7404e-02,\n",
      "          4.9011e-02, 3.6743e-02],\n",
      "         ...,\n",
      "         [1.7285e-05, 1.7345e-05, 2.0444e-05,  ..., 2.8744e-03,\n",
      "          2.5024e-03, 1.1272e-03],\n",
      "         [4.5300e-06, 4.5300e-06, 5.7220e-06,  ..., 2.6524e-05,\n",
      "          2.1517e-05, 9.7156e-06],\n",
      "         [2.3842e-07, 2.3842e-07, 3.5763e-07,  ..., 3.1586e-03,\n",
      "          2.4109e-03, 2.9564e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.8764e-04, 2.4104e-04, 2.9421e-04,  ..., 3.0319e-02,\n",
      "          2.3285e-02, 1.3588e-02],\n",
      "         [1.4186e-04, 1.8251e-04, 2.1791e-04,  ..., 1.0846e-01,\n",
      "          9.2834e-02, 7.3914e-02],\n",
      "         [1.3494e-03, 1.6851e-03, 1.9932e-03,  ..., 1.2927e-01,\n",
      "          1.1462e-01, 8.9600e-02],\n",
      "         ...,\n",
      "         [4.6670e-05, 6.8605e-05, 8.6904e-05,  ..., 1.3876e-04,\n",
      "          1.3149e-04, 2.3723e-05],\n",
      "         [8.2064e-04, 1.4381e-03, 1.6994e-03,  ..., 2.9802e-07,\n",
      "          2.3842e-07, 5.9605e-08],\n",
      "         [4.1246e-05, 5.9724e-05, 9.0599e-05,  ..., 4.9829e-05,\n",
      "          4.7803e-05, 1.2577e-05]],\n",
      "\n",
      "        [[1.5795e-05, 2.7478e-05, 3.6776e-05,  ..., 1.1206e-01,\n",
      "          1.0938e-01, 1.0077e-01],\n",
      "         [1.0908e-05, 1.7047e-05, 2.1100e-05,  ..., 6.5979e-02,\n",
      "          6.5369e-02, 6.2561e-02],\n",
      "         [1.4853e-04, 2.3675e-04, 2.9802e-04,  ..., 6.7444e-02,\n",
      "          6.4636e-02, 5.2246e-02],\n",
      "         ...,\n",
      "         [5.4777e-05, 6.8009e-05, 7.0989e-05,  ..., 9.7733e-03,\n",
      "          9.7656e-03, 9.7580e-03],\n",
      "         [7.5161e-05, 8.6784e-05, 9.9003e-05,  ..., 7.7486e-07,\n",
      "          7.7486e-07, 2.9802e-07],\n",
      "         [3.7551e-06, 4.2319e-06, 4.8876e-06,  ..., 2.8896e-04,\n",
      "          2.8753e-04, 2.7680e-04]],\n",
      "\n",
      "        [[1.5497e-06, 2.3246e-06, 3.3975e-06,  ..., 2.0493e-02,\n",
      "          1.6068e-02, 1.2741e-02],\n",
      "         [5.9605e-07, 8.3447e-07, 1.1921e-06,  ..., 1.7346e-01,\n",
      "          1.6504e-01, 1.6150e-01],\n",
      "         [1.7583e-05, 2.5749e-05, 3.4273e-05,  ..., 3.3386e-02,\n",
      "          3.1525e-02, 2.9587e-02],\n",
      "         ...,\n",
      "         [1.6034e-05, 4.2617e-05, 7.3850e-05,  ..., 1.2579e-03,\n",
      "          1.1969e-03, 1.0967e-03],\n",
      "         [2.5630e-06, 8.4639e-06, 1.4365e-05,  ..., 6.1572e-05,\n",
      "          5.9485e-05, 5.1677e-05],\n",
      "         [8.9407e-07, 1.5497e-06, 2.9802e-06,  ..., 4.4556e-03,\n",
      "          4.1351e-03, 3.1891e-03]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[1.9550e-04, 2.5582e-04, 3.1304e-04,  ..., 7.9712e-02,\n",
      "          3.7048e-02, 1.9836e-02],\n",
      "         [4.0591e-05, 5.5134e-05, 6.7294e-05,  ..., 1.2244e-01,\n",
      "          1.0828e-01, 9.3628e-02],\n",
      "         [2.9259e-03, 3.8338e-03, 4.6883e-03,  ..., 1.0626e-01,\n",
      "          8.8684e-02, 7.1533e-02],\n",
      "         ...,\n",
      "         [1.5378e-05, 1.8954e-05, 2.1935e-05,  ..., 7.4341e-02,\n",
      "          6.2561e-02, 5.9235e-02],\n",
      "         [1.1921e-07, 1.1921e-07, 1.1921e-07,  ..., 3.0957e-01,\n",
      "          2.5269e-01, 8.6243e-02],\n",
      "         [3.3259e-05, 3.7670e-05, 4.1902e-05,  ..., 6.0883e-02,\n",
      "          5.9021e-02, 5.5939e-02]],\n",
      "\n",
      "        [[2.2459e-04, 2.7227e-04, 3.1447e-04,  ..., 1.4771e-01,\n",
      "          1.1450e-01, 1.0052e-01],\n",
      "         [1.0854e-04, 1.2982e-04, 1.4567e-04,  ..., 3.0380e-02,\n",
      "          2.4765e-02, 1.9989e-02],\n",
      "         [8.8196e-03, 1.1452e-02, 1.3290e-02,  ..., 5.7526e-02,\n",
      "          4.6936e-02, 3.6713e-02],\n",
      "         ...,\n",
      "         [3.9935e-05, 5.0604e-05, 5.9187e-05,  ..., 7.9727e-03,\n",
      "          7.0267e-03, 6.2294e-03],\n",
      "         [2.9802e-07, 3.5763e-07, 4.1723e-07,  ..., 2.0752e-01,\n",
      "          2.0251e-01, 1.9849e-01],\n",
      "         [1.1718e-04, 1.3411e-04, 1.4579e-04,  ..., 2.7664e-02,\n",
      "          2.7252e-02, 2.7084e-02]],\n",
      "\n",
      "        [[6.2764e-05, 8.7738e-05, 1.0490e-04,  ..., 9.8145e-02,\n",
      "          7.4402e-02, 4.2725e-02],\n",
      "         [5.6505e-05, 8.6963e-05, 1.1265e-04,  ..., 3.8721e-01,\n",
      "          2.9858e-01, 1.7419e-01],\n",
      "         [3.6526e-03, 5.1575e-03, 6.6185e-03,  ..., 9.8206e-02,\n",
      "          8.1604e-02, 4.9194e-02],\n",
      "         ...,\n",
      "         [9.8348e-06, 1.3053e-05, 1.6332e-05,  ..., 2.1057e-02,\n",
      "          1.9348e-02, 1.5083e-02],\n",
      "         [3.5763e-07, 4.7684e-07, 5.3644e-07,  ..., 3.1299e-01,\n",
      "          2.6831e-01, 2.1863e-01],\n",
      "         [7.6473e-05, 8.4102e-05, 8.9705e-05,  ..., 3.2898e-02,\n",
      "          3.2562e-02, 3.1616e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.8944e-04, 3.7527e-04, 4.6277e-04,  ..., 8.9417e-03,\n",
      "          6.0043e-03, 3.5172e-03],\n",
      "         [1.8907e-04, 2.3961e-04, 2.9421e-04,  ..., 1.1940e-02,\n",
      "          7.2365e-03, 5.1346e-03],\n",
      "         [4.1748e-02, 5.4016e-02, 6.7993e-02,  ..., 1.2764e-02,\n",
      "          9.7046e-03, 5.5275e-03],\n",
      "         ...,\n",
      "         [7.8857e-05, 9.8288e-05, 1.2183e-04,  ..., 5.2376e-03,\n",
      "          4.0283e-03, 2.9640e-03],\n",
      "         [2.5034e-06, 3.2187e-06, 3.7551e-06,  ..., 8.6304e-02,\n",
      "          7.4890e-02, 5.3467e-02],\n",
      "         [8.4209e-04, 8.7118e-04, 9.3222e-04,  ..., 5.1460e-03,\n",
      "          4.9667e-03, 4.9286e-03]],\n",
      "\n",
      "        [[6.9439e-05, 9.9540e-05, 1.1659e-04,  ..., 1.5427e-02,\n",
      "          1.5068e-02, 1.1124e-02],\n",
      "         [4.9949e-05, 7.3016e-05, 8.8036e-05,  ..., 3.9581e-02,\n",
      "          3.7659e-02, 2.8778e-02],\n",
      "         [5.9662e-03, 7.8430e-03, 9.1248e-03,  ..., 1.6663e-01,\n",
      "          1.6309e-01, 1.4563e-01],\n",
      "         ...,\n",
      "         [5.1379e-05, 6.5923e-05, 8.0466e-05,  ..., 1.0376e-02,\n",
      "          1.0071e-02, 9.3231e-03],\n",
      "         [2.3842e-07, 3.5763e-07, 3.5763e-07,  ..., 1.3220e-01,\n",
      "          1.3147e-01, 1.2866e-01],\n",
      "         [1.0502e-04, 1.3840e-04, 1.4997e-04,  ..., 5.2452e-03,\n",
      "          5.2185e-03, 4.4327e-03]],\n",
      "\n",
      "        [[3.7968e-05, 5.1141e-05, 6.4969e-05,  ..., 1.7139e-01,\n",
      "          1.6711e-01, 2.9205e-02],\n",
      "         [3.0339e-05, 4.4525e-05, 5.8770e-05,  ..., 1.2878e-01,\n",
      "          1.2646e-01, 6.6284e-02],\n",
      "         [2.5177e-03, 3.3970e-03, 4.4937e-03,  ..., 5.0690e-02,\n",
      "          4.3671e-02, 2.6184e-02],\n",
      "         ...,\n",
      "         [8.0466e-06, 1.2517e-05, 1.9073e-05,  ..., 9.0027e-03,\n",
      "          8.5297e-03, 6.0501e-03],\n",
      "         [1.1921e-07, 1.7881e-07, 2.3842e-07,  ..., 2.9688e-01,\n",
      "          2.9419e-01, 2.8979e-01],\n",
      "         [7.2479e-05, 8.5950e-05, 9.3818e-05,  ..., 4.6021e-02,\n",
      "          4.5898e-02, 4.5471e-02]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[1.2004e-04, 1.6141e-04, 1.9956e-04,  ..., 6.1737e-02,\n",
      "          5.8350e-02, 3.8879e-02],\n",
      "         [2.7704e-04, 4.3106e-04, 6.5470e-04,  ..., 5.8167e-02,\n",
      "          5.3009e-02, 3.4546e-02],\n",
      "         [1.2035e-03, 1.5240e-03, 1.8530e-03,  ..., 1.6602e-01,\n",
      "          1.6260e-01, 1.5112e-01],\n",
      "         ...,\n",
      "         [1.7405e-05, 2.3544e-05, 3.3915e-05,  ..., 7.3730e-02,\n",
      "          6.7810e-02, 5.4871e-02],\n",
      "         [2.3007e-05, 2.9981e-05, 4.5598e-05,  ..., 4.1595e-02,\n",
      "          3.5645e-02, 2.3682e-02],\n",
      "         [2.3723e-05, 3.3379e-05, 4.7266e-05,  ..., 6.9946e-02,\n",
      "          5.9845e-02, 5.1819e-02]],\n",
      "\n",
      "        [[7.0691e-05, 9.8288e-05, 1.3781e-04,  ..., 7.6294e-02,\n",
      "          7.3608e-02, 7.1411e-02],\n",
      "         [1.9014e-04, 2.7823e-04, 4.4322e-04,  ..., 9.5978e-03,\n",
      "          7.7438e-03, 6.6986e-03],\n",
      "         [6.5327e-04, 8.0824e-04, 9.7227e-04,  ..., 1.8542e-01,\n",
      "          1.8457e-01, 1.8396e-01],\n",
      "         ...,\n",
      "         [1.0729e-05, 1.3351e-05, 1.6332e-05,  ..., 8.0643e-03,\n",
      "          5.7411e-03, 4.6539e-03],\n",
      "         [3.0875e-05, 4.2260e-05, 5.7757e-05,  ..., 1.7042e-03,\n",
      "          1.2150e-03, 1.0424e-03],\n",
      "         [1.7881e-05, 2.4021e-05, 3.0577e-05,  ..., 3.6850e-03,\n",
      "          3.0956e-03, 2.5520e-03]],\n",
      "\n",
      "        [[7.4923e-05, 1.0169e-04, 1.3435e-04,  ..., 5.7281e-02,\n",
      "          5.3314e-02, 4.6661e-02],\n",
      "         [1.9765e-04, 2.7800e-04, 4.1461e-04,  ..., 6.2744e-02,\n",
      "          6.0944e-02, 5.8838e-02],\n",
      "         [1.2560e-03, 1.5612e-03, 1.8387e-03,  ..., 3.4790e-01,\n",
      "          3.4595e-01, 3.3545e-01],\n",
      "         ...,\n",
      "         [1.3649e-05, 1.7643e-05, 2.4974e-05,  ..., 4.8096e-02,\n",
      "          3.4576e-02, 2.0691e-02],\n",
      "         [1.2577e-05, 1.8895e-05, 2.4617e-05,  ..., 2.3117e-02,\n",
      "          1.7090e-02, 1.1101e-02],\n",
      "         [2.7359e-05, 3.5405e-05, 4.6492e-05,  ..., 1.9318e-02,\n",
      "          1.5602e-02, 1.1627e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[6.0368e-04, 7.9250e-04, 9.6560e-04,  ..., 2.2430e-02,\n",
      "          2.1194e-02, 1.7578e-02],\n",
      "         [2.0542e-03, 3.0308e-03, 5.3406e-03,  ..., 8.3313e-02,\n",
      "          7.2998e-02, 5.1300e-02],\n",
      "         [1.0773e-02, 1.3199e-02, 1.6129e-02,  ..., 2.4878e-01,\n",
      "          2.3853e-01, 2.2375e-01],\n",
      "         ...,\n",
      "         [3.8791e-04, 5.0783e-04, 6.4707e-04,  ..., 1.2207e-02,\n",
      "          9.1705e-03, 6.9618e-03],\n",
      "         [4.3178e-04, 5.8126e-04, 7.8821e-04,  ..., 3.0289e-03,\n",
      "          2.4395e-03, 1.6155e-03],\n",
      "         [3.9077e-04, 5.0354e-04, 6.3372e-04,  ..., 4.4006e-02,\n",
      "          3.2043e-02, 2.5116e-02]],\n",
      "\n",
      "        [[1.1432e-04, 1.5855e-04, 2.0134e-04,  ..., 4.2664e-02,\n",
      "          4.1992e-02, 4.0405e-02],\n",
      "         [7.6342e-04, 1.2331e-03, 2.0027e-03,  ..., 2.8793e-02,\n",
      "          2.8275e-02, 2.7832e-02],\n",
      "         [2.0733e-03, 2.5826e-03, 2.9278e-03,  ..., 9.1675e-02,\n",
      "          9.1309e-02, 8.9844e-02],\n",
      "         ...,\n",
      "         [5.8889e-05, 8.8632e-05, 1.0812e-04,  ..., 3.1830e-02,\n",
      "          3.1311e-02, 2.9358e-02],\n",
      "         [1.6165e-04, 2.4033e-04, 2.9612e-04,  ..., 9.8267e-03,\n",
      "          9.6436e-03, 9.1400e-03],\n",
      "         [5.9426e-05, 1.1033e-04, 1.4019e-04,  ..., 7.0190e-02,\n",
      "          6.6895e-02, 6.4941e-02]],\n",
      "\n",
      "        [[2.1935e-05, 3.5644e-05, 5.3585e-05,  ..., 4.5685e-02,\n",
      "          3.6560e-02, 3.2928e-02],\n",
      "         [1.0216e-04, 2.1017e-04, 3.7909e-04,  ..., 4.6661e-02,\n",
      "          4.5776e-02, 4.5135e-02],\n",
      "         [3.3927e-04, 4.9067e-04, 6.3086e-04,  ..., 4.9194e-02,\n",
      "          4.9011e-02, 4.8737e-02],\n",
      "         ...,\n",
      "         [1.0908e-05, 1.6868e-05, 2.3901e-05,  ..., 2.7695e-02,\n",
      "          2.5208e-02, 2.1790e-02],\n",
      "         [1.4186e-05, 1.8179e-05, 2.5153e-05,  ..., 7.3967e-03,\n",
      "          6.7787e-03, 5.6038e-03],\n",
      "         [1.4484e-05, 2.1279e-05, 3.3796e-05,  ..., 1.0513e-02,\n",
      "          9.8801e-03, 8.9340e-03]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[6.6614e-04, 8.2541e-04, 9.6369e-04,  ..., 2.0947e-01,\n",
      "          1.8884e-01, 1.6296e-01],\n",
      "         [3.4785e-04, 4.0984e-04, 4.5848e-04,  ..., 8.5266e-02,\n",
      "          8.0078e-02, 6.7383e-02],\n",
      "         [3.4928e-05, 4.2379e-05, 5.0128e-05,  ..., 8.3130e-02,\n",
      "          7.5745e-02, 6.1523e-02],\n",
      "         ...,\n",
      "         [1.5092e-04, 2.0576e-04, 3.2449e-04,  ..., 1.3721e-01,\n",
      "          1.2769e-01, 1.0394e-01],\n",
      "         [1.5855e-04, 2.0564e-04, 2.4188e-04,  ..., 3.8574e-01,\n",
      "          3.6914e-01, 3.3838e-01],\n",
      "         [3.4642e-04, 4.5609e-04, 5.5552e-04,  ..., 3.0078e-01,\n",
      "          2.7954e-01, 2.6172e-01]],\n",
      "\n",
      "        [[1.5335e-03, 1.8339e-03, 2.0599e-03,  ..., 2.9175e-02,\n",
      "          2.7481e-02, 2.3788e-02],\n",
      "         [3.8934e-04, 4.9257e-04, 5.6458e-04,  ..., 2.5772e-02,\n",
      "          2.1378e-02, 1.9165e-02],\n",
      "         [2.4199e-05, 2.9445e-05, 3.3736e-05,  ..., 2.5269e-02,\n",
      "          1.5526e-02, 1.0887e-02],\n",
      "         ...,\n",
      "         [1.4412e-04, 2.0552e-04, 2.7823e-04,  ..., 1.8872e-01,\n",
      "          1.8579e-01, 1.8335e-01],\n",
      "         [4.4823e-05, 5.8174e-05, 7.0453e-05,  ..., 4.1351e-02,\n",
      "          2.8137e-02, 2.0111e-02],\n",
      "         [1.1694e-04, 1.5020e-04, 1.8406e-04,  ..., 2.5894e-02,\n",
      "          1.6739e-02, 1.1871e-02]],\n",
      "\n",
      "        [[4.9448e-04, 5.9795e-04, 6.8474e-04,  ..., 8.9478e-02,\n",
      "          7.7820e-02, 6.4026e-02],\n",
      "         [3.1519e-04, 3.7622e-04, 4.3058e-04,  ..., 1.5027e-01,\n",
      "          1.4417e-01, 1.3782e-01],\n",
      "         [3.5524e-05, 4.4227e-05, 5.3644e-05,  ..., 3.0176e-01,\n",
      "          2.4341e-01, 1.7432e-01],\n",
      "         ...,\n",
      "         [8.3745e-05, 1.0359e-04, 1.3983e-04,  ..., 2.1484e-01,\n",
      "          1.9666e-01, 1.6943e-01],\n",
      "         [7.0691e-05, 8.6904e-05, 1.0687e-04,  ..., 1.7798e-01,\n",
      "          1.3525e-01, 8.8074e-02],\n",
      "         [1.7786e-04, 2.2745e-04, 2.8968e-04,  ..., 1.2756e-01,\n",
      "          9.8450e-02, 6.7871e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[5.6992e-03, 7.4615e-03, 9.0408e-03,  ..., 2.8961e-02,\n",
      "          2.3376e-02, 2.0569e-02],\n",
      "         [3.8872e-03, 4.9362e-03, 5.7335e-03,  ..., 2.5772e-02,\n",
      "          1.8890e-02, 1.3649e-02],\n",
      "         [2.6822e-04, 3.5048e-04, 4.3583e-04,  ..., 1.2321e-02,\n",
      "          7.7400e-03, 6.8970e-03],\n",
      "         ...,\n",
      "         [2.9874e-04, 3.9124e-04, 5.2309e-04,  ..., 2.3413e-01,\n",
      "          2.3047e-01, 2.2607e-01],\n",
      "         [5.4169e-04, 6.8808e-04, 8.1396e-04,  ..., 1.4099e-02,\n",
      "          8.5602e-03, 7.3624e-03],\n",
      "         [8.6975e-04, 1.1148e-03, 1.3351e-03,  ..., 2.6291e-02,\n",
      "          2.1912e-02, 2.0752e-02]],\n",
      "\n",
      "        [[2.9793e-03, 3.9368e-03, 5.0850e-03,  ..., 4.1229e-02,\n",
      "          3.6530e-02, 2.8290e-02],\n",
      "         [1.2293e-03, 1.6308e-03, 1.9836e-03,  ..., 4.4434e-02,\n",
      "          4.2267e-02, 3.7048e-02],\n",
      "         [4.9949e-05, 6.7890e-05, 9.1016e-05,  ..., 5.0751e-02,\n",
      "          4.7729e-02, 3.8971e-02],\n",
      "         ...,\n",
      "         [1.7941e-04, 2.6894e-04, 4.0960e-04,  ..., 1.2451e-01,\n",
      "          1.2329e-01, 1.1603e-01],\n",
      "         [6.0141e-05, 9.2506e-05, 1.1480e-04,  ..., 7.4829e-02,\n",
      "          7.2754e-02, 6.1462e-02],\n",
      "         [1.6093e-04, 2.5201e-04, 3.4809e-04,  ..., 3.9124e-02,\n",
      "          3.6957e-02, 2.7649e-02]],\n",
      "\n",
      "        [[5.1594e-04, 6.5136e-04, 8.2064e-04,  ..., 5.7678e-02,\n",
      "          4.8798e-02, 4.1321e-02],\n",
      "         [1.7762e-04, 2.3746e-04, 3.0279e-04,  ..., 6.8970e-02,\n",
      "          6.7139e-02, 5.1208e-02],\n",
      "         [1.1683e-05, 1.7226e-05, 2.8610e-05,  ..., 8.3923e-02,\n",
      "          7.9834e-02, 6.4819e-02],\n",
      "         ...,\n",
      "         [8.0764e-05, 1.2159e-04, 2.2864e-04,  ..., 1.6113e-01,\n",
      "          1.5955e-01, 1.4453e-01],\n",
      "         [7.9453e-05, 1.0598e-04, 1.4126e-04,  ..., 1.2286e-01,\n",
      "          1.1957e-01, 9.9121e-02],\n",
      "         [9.5844e-05, 1.3685e-04, 1.9503e-04,  ..., 8.1482e-02,\n",
      "          7.7271e-02, 4.0588e-02]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[8.3506e-05, 1.0246e-04, 1.3793e-04,  ..., 7.4615e-03,\n",
      "          7.1030e-03, 4.6158e-03],\n",
      "         [1.6809e-04, 1.8549e-04, 2.0695e-04,  ..., 6.4087e-02,\n",
      "          5.5573e-02, 1.9287e-02],\n",
      "         [7.4291e-04, 1.0796e-03, 1.2779e-03,  ..., 1.7120e-02,\n",
      "          1.0155e-02, 8.3771e-03],\n",
      "         ...,\n",
      "         [3.9935e-06, 5.5432e-06, 6.7353e-06,  ..., 7.7332e-02,\n",
      "          7.4890e-02, 7.0618e-02],\n",
      "         [6.2585e-06, 8.7619e-06, 1.1444e-05,  ..., 7.2449e-02,\n",
      "          7.1777e-02, 7.0190e-02],\n",
      "         [6.1393e-06, 8.7023e-06, 1.0371e-05,  ..., 3.4302e-02,\n",
      "          3.3203e-02, 3.0167e-02]],\n",
      "\n",
      "        [[4.2200e-04, 5.6458e-04, 7.2861e-04,  ..., 3.6373e-03,\n",
      "          3.4237e-03, 3.1223e-03],\n",
      "         [1.5843e-04, 1.7846e-04, 1.8835e-04,  ..., 3.9368e-02,\n",
      "          3.9124e-02, 3.8177e-02],\n",
      "         [1.1845e-03, 1.4429e-03, 1.7090e-03,  ..., 1.3103e-03,\n",
      "          5.2166e-04, 4.9496e-04],\n",
      "         ...,\n",
      "         [2.7359e-05, 3.2067e-05, 3.5346e-05,  ..., 1.4145e-02,\n",
      "          1.1894e-02, 9.2239e-03],\n",
      "         [2.0707e-04, 2.4605e-04, 2.7776e-04,  ..., 6.6910e-03,\n",
      "          5.5695e-03, 4.3716e-03],\n",
      "         [6.3241e-05, 7.5102e-05, 8.6904e-05,  ..., 1.6754e-02,\n",
      "          1.3573e-02, 8.9493e-03]],\n",
      "\n",
      "        [[3.3641e-04, 4.6444e-04, 5.5408e-04,  ..., 1.6769e-02,\n",
      "          1.6129e-02, 1.5732e-02],\n",
      "         [3.0935e-05, 3.9160e-05, 4.4167e-05,  ..., 6.4819e-02,\n",
      "          6.0760e-02, 5.4504e-02],\n",
      "         [2.8572e-03, 3.6469e-03, 4.1428e-03,  ..., 1.4374e-02,\n",
      "          8.0795e-03, 5.8289e-03],\n",
      "         ...,\n",
      "         [1.3351e-05, 1.4782e-05, 1.6272e-05,  ..., 1.0718e-01,\n",
      "          9.2651e-02, 8.1909e-02],\n",
      "         [3.4392e-05, 3.7849e-05, 4.1544e-05,  ..., 8.6670e-02,\n",
      "          7.7148e-02, 7.1167e-02],\n",
      "         [2.6047e-05, 2.9922e-05, 3.4034e-05,  ..., 4.1229e-02,\n",
      "          3.3417e-02, 2.9617e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[4.3178e-04, 5.4693e-04, 6.8235e-04,  ..., 1.3275e-03,\n",
      "          1.2875e-03, 7.6675e-04],\n",
      "         [6.3181e-05, 7.5459e-05, 9.1374e-05,  ..., 1.3550e-01,\n",
      "          1.3049e-01, 9.4299e-02],\n",
      "         [2.2335e-03, 2.8419e-03, 3.4885e-03,  ..., 1.1635e-03,\n",
      "          9.2983e-04, 8.3542e-04],\n",
      "         ...,\n",
      "         [1.3316e-04, 1.7536e-04, 2.2137e-04,  ..., 4.4060e-03,\n",
      "          3.5210e-03, 3.0327e-03],\n",
      "         [4.0703e-03, 5.2719e-03, 6.6147e-03,  ..., 1.6842e-03,\n",
      "          1.3180e-03, 8.9741e-04],\n",
      "         [1.1854e-03, 1.6003e-03, 2.0199e-03,  ..., 8.7833e-04,\n",
      "          7.4577e-04, 5.9938e-04]],\n",
      "\n",
      "        [[2.5320e-04, 4.3821e-04, 6.1560e-04,  ..., 2.4643e-02,\n",
      "          2.4643e-02, 2.4612e-02],\n",
      "         [7.0930e-05, 1.0234e-04, 1.3161e-04,  ..., 9.6436e-02,\n",
      "          9.5886e-02, 9.5093e-02],\n",
      "         [6.5517e-04, 1.1206e-03, 1.5326e-03,  ..., 2.8191e-03,\n",
      "          2.7599e-03, 2.2049e-03],\n",
      "         ...,\n",
      "         [1.6785e-04, 2.5797e-04, 3.4308e-04,  ..., 2.2476e-02,\n",
      "          2.1408e-02, 1.6922e-02],\n",
      "         [4.8137e-04, 7.7295e-04, 1.0948e-03,  ..., 3.7937e-03,\n",
      "          3.6526e-03, 2.7065e-03],\n",
      "         [1.6737e-04, 2.9421e-04, 4.4799e-04,  ..., 1.6586e-02,\n",
      "          1.6373e-02, 1.4061e-02]],\n",
      "\n",
      "        [[1.3185e-04, 1.7595e-04, 2.5439e-04,  ..., 6.2675e-03,\n",
      "          4.9973e-03, 6.4659e-04],\n",
      "         [9.3400e-05, 1.0866e-04, 1.3137e-04,  ..., 4.1199e-02,\n",
      "          3.8055e-02, 3.6804e-02],\n",
      "         [4.3249e-04, 7.9250e-04, 1.2169e-03,  ..., 4.6654e-03,\n",
      "          4.3907e-03, 2.9221e-03],\n",
      "         ...,\n",
      "         [2.3663e-05, 3.4630e-05, 5.5075e-05,  ..., 3.7061e-01,\n",
      "          3.6548e-01, 3.1079e-01],\n",
      "         [3.5882e-05, 4.9651e-05, 7.5698e-05,  ..., 1.1353e-01,\n",
      "          1.0944e-01, 9.9792e-03],\n",
      "         [7.9751e-05, 1.1355e-04, 1.7202e-04,  ..., 5.1331e-02,\n",
      "          4.5837e-02, 5.1880e-03]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[4.7326e-05, 6.1572e-05, 7.4327e-05,  ..., 2.7344e-02,\n",
      "          2.6642e-02, 2.5925e-02],\n",
      "         [9.3818e-05, 1.1510e-04, 1.3673e-04,  ..., 7.9895e-02,\n",
      "          7.8918e-02, 7.7148e-02],\n",
      "         [6.9797e-05, 7.9870e-05, 8.9765e-05,  ..., 7.2632e-02,\n",
      "          6.5674e-02, 6.1340e-02],\n",
      "         ...,\n",
      "         [5.5122e-04, 7.2145e-04, 9.4318e-04,  ..., 9.6359e-03,\n",
      "          4.2114e-03, 2.9373e-03],\n",
      "         [4.4405e-05, 6.6638e-05, 8.6427e-05,  ..., 1.9852e-02,\n",
      "          1.3924e-02, 6.0844e-03],\n",
      "         [3.9101e-03, 5.2719e-03, 6.4888e-03,  ..., 2.3773e-02,\n",
      "          1.2138e-02, 1.0628e-02]],\n",
      "\n",
      "        [[1.2684e-04, 1.5724e-04, 1.8179e-04,  ..., 3.9787e-03,\n",
      "          1.6518e-03, 1.1492e-03],\n",
      "         [2.5582e-04, 3.1948e-04, 3.5930e-04,  ..., 2.7561e-03,\n",
      "          2.4624e-03, 1.8854e-03],\n",
      "         [5.0688e-04, 6.2037e-04, 6.9809e-04,  ..., 2.7142e-03,\n",
      "          2.4529e-03, 1.7252e-03],\n",
      "         ...,\n",
      "         [1.7605e-03, 2.2526e-03, 2.5444e-03,  ..., 1.2350e-03,\n",
      "          1.1969e-03, 1.0700e-03],\n",
      "         [8.6069e-05, 1.1498e-04, 1.3959e-04,  ..., 2.4776e-03,\n",
      "          2.3823e-03, 1.5144e-03],\n",
      "         [3.5172e-03, 4.9362e-03, 6.0349e-03,  ..., 3.2253e-03,\n",
      "          2.9755e-03, 2.4071e-03]],\n",
      "\n",
      "        [[4.6551e-05, 5.4181e-05, 6.2287e-05,  ..., 2.3651e-02,\n",
      "          1.9043e-02, 3.4351e-03],\n",
      "         [1.5068e-04, 1.7750e-04, 2.0397e-04,  ..., 5.8167e-02,\n",
      "          3.9307e-02, 2.5818e-02],\n",
      "         [1.6296e-04, 1.8334e-04, 2.0790e-04,  ..., 1.5625e-02,\n",
      "          1.1765e-02, 9.9411e-03],\n",
      "         ...,\n",
      "         [7.4244e-04, 1.1330e-03, 1.4181e-03,  ..., 4.0779e-03,\n",
      "          3.2616e-03, 2.3041e-03],\n",
      "         [6.9618e-05, 9.3818e-05, 1.1557e-04,  ..., 1.5289e-02,\n",
      "          1.0788e-02, 9.5062e-03],\n",
      "         [4.8294e-03, 6.8321e-03, 8.4229e-03,  ..., 1.3977e-02,\n",
      "          1.2169e-02, 8.3313e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[3.5262e-04, 4.8470e-04, 5.7650e-04,  ..., 4.1389e-03,\n",
      "          3.6049e-03, 3.2730e-03],\n",
      "         [4.2892e-04, 6.0415e-04, 7.1716e-04,  ..., 2.5082e-03,\n",
      "          2.1172e-03, 1.2608e-03],\n",
      "         [2.6665e-03, 3.4981e-03, 4.2534e-03,  ..., 9.9258e-03,\n",
      "          8.0566e-03, 7.0114e-03],\n",
      "         ...,\n",
      "         [1.0195e-03, 1.2712e-03, 1.6470e-03,  ..., 2.0657e-03,\n",
      "          2.0237e-03, 5.3072e-04],\n",
      "         [1.5843e-04, 1.9503e-04, 2.6488e-04,  ..., 4.1046e-03,\n",
      "          4.0665e-03, 1.3542e-03],\n",
      "         [7.6389e-04, 8.9693e-04, 1.1044e-03,  ..., 4.1428e-03,\n",
      "          3.9330e-03, 1.0128e-03]],\n",
      "\n",
      "        [[2.9445e-05, 5.1439e-05, 8.0585e-05,  ..., 1.3247e-03,\n",
      "          1.0729e-03, 6.0511e-04],\n",
      "         [7.2479e-05, 1.4281e-04, 2.2089e-04,  ..., 4.4212e-03,\n",
      "          4.2877e-03, 3.9291e-03],\n",
      "         [3.1281e-04, 4.3201e-04, 5.9605e-04,  ..., 8.0414e-03,\n",
      "          6.9389e-03, 6.2180e-03],\n",
      "         ...,\n",
      "         [8.0919e-04, 1.2531e-03, 1.6842e-03,  ..., 1.7471e-02,\n",
      "          1.7426e-02, 1.7242e-02],\n",
      "         [1.3983e-04, 1.8251e-04, 2.3592e-04,  ..., 1.3680e-02,\n",
      "          1.3672e-02, 1.3611e-02],\n",
      "         [5.7106e-03, 8.8882e-03, 1.1955e-02,  ..., 1.8250e-02,\n",
      "          1.8219e-02, 1.7944e-02]],\n",
      "\n",
      "        [[2.3365e-05, 3.6597e-05, 6.3241e-05,  ..., 1.1650e-02,\n",
      "          1.0780e-02, 9.6741e-03],\n",
      "         [3.8922e-05, 5.6207e-05, 9.3758e-05,  ..., 2.1240e-02,\n",
      "          1.9897e-02, 3.4580e-03],\n",
      "         [1.6534e-04, 2.3437e-04, 3.3116e-04,  ..., 8.5983e-03,\n",
      "          7.4196e-03, 5.9013e-03],\n",
      "         ...,\n",
      "         [3.9864e-04, 5.1737e-04, 7.1001e-04,  ..., 1.4353e-03,\n",
      "          1.2045e-03, 9.4080e-04],\n",
      "         [4.8816e-05, 6.8605e-05, 9.5248e-05,  ..., 4.2229e-03,\n",
      "          1.7967e-03, 1.3237e-03],\n",
      "         [1.4658e-03, 2.0885e-03, 3.4313e-03,  ..., 1.4839e-02,\n",
      "          1.4114e-02, 1.2215e-02]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[1.8716e-04, 2.2638e-04, 2.6679e-04,  ..., 1.2781e-01,\n",
      "          9.3750e-02, 4.2358e-02],\n",
      "         [5.8115e-05, 6.9141e-05, 8.2314e-05,  ..., 7.5623e-02,\n",
      "          5.1514e-02, 2.4521e-02],\n",
      "         [6.4516e-04, 7.4863e-04, 8.4209e-04,  ..., 8.5907e-03,\n",
      "          7.9498e-03, 5.4207e-03],\n",
      "         ...,\n",
      "         [3.7050e-04, 4.6849e-04, 5.3263e-04,  ..., 2.1469e-02,\n",
      "          2.1255e-02, 2.1149e-02],\n",
      "         [4.8423e-04, 5.8508e-04, 6.8283e-04,  ..., 1.3252e-02,\n",
      "          1.3123e-02, 1.2878e-02],\n",
      "         [3.7599e-04, 4.6539e-04, 5.5981e-04,  ..., 5.9875e-02,\n",
      "          5.2002e-02, 2.1408e-02]],\n",
      "\n",
      "        [[3.9434e-04, 5.1022e-04, 5.8699e-04,  ..., 8.2626e-03,\n",
      "          7.0648e-03, 6.7253e-03],\n",
      "         [1.1796e-04, 1.3602e-04, 1.4973e-04,  ..., 1.1162e-02,\n",
      "          1.0750e-02, 1.0513e-02],\n",
      "         [1.1396e-03, 1.3990e-03, 1.5678e-03,  ..., 1.3649e-02,\n",
      "          1.3123e-02, 1.2741e-02],\n",
      "         ...,\n",
      "         [1.7726e-04, 2.0933e-04, 2.4629e-04,  ..., 4.2343e-03,\n",
      "          3.3264e-03, 1.9245e-03],\n",
      "         [2.4939e-04, 3.0088e-04, 3.5214e-04,  ..., 1.4858e-03,\n",
      "          1.2474e-03, 1.1482e-03],\n",
      "         [7.5197e-04, 9.1219e-04, 1.1511e-03,  ..., 4.6616e-03,\n",
      "          4.3259e-03, 4.1428e-03]],\n",
      "\n",
      "        [[9.0241e-05, 1.1456e-04, 1.3423e-04,  ..., 5.0598e-02,\n",
      "          4.0649e-02, 2.9770e-02],\n",
      "         [2.3007e-05, 2.7776e-05, 3.1531e-05,  ..., 1.1543e-02,\n",
      "          1.0300e-02, 8.7662e-03],\n",
      "         [4.3964e-04, 5.4073e-04, 6.2799e-04,  ..., 4.4022e-03,\n",
      "          4.1351e-03, 3.4027e-03],\n",
      "         ...,\n",
      "         [1.0860e-04, 1.3578e-04, 1.5795e-04,  ..., 1.1482e-02,\n",
      "          1.0681e-02, 8.0566e-03],\n",
      "         [1.6391e-04, 2.0170e-04, 2.3329e-04,  ..., 1.4160e-01,\n",
      "          1.3977e-01, 6.9458e-02],\n",
      "         [3.0231e-04, 3.9601e-04, 4.5466e-04,  ..., 3.2440e-02,\n",
      "          2.9770e-02, 2.7344e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[6.6147e-03, 8.3466e-03, 9.9945e-03,  ..., 2.3145e-01,\n",
      "          2.2571e-01, 2.4963e-02],\n",
      "         [2.8286e-03, 3.5095e-03, 4.1542e-03,  ..., 6.8359e-02,\n",
      "          3.7262e-02, 1.4854e-02],\n",
      "         [4.2419e-03, 5.1880e-03, 6.0806e-03,  ..., 2.8101e-01,\n",
      "          2.7759e-01, 1.5007e-02],\n",
      "         ...,\n",
      "         [1.2124e-04, 1.6439e-04, 2.0945e-04,  ..., 2.3041e-03,\n",
      "          2.0370e-03, 1.9226e-03],\n",
      "         [6.5756e-04, 8.5735e-04, 1.0328e-03,  ..., 3.8471e-03,\n",
      "          3.2158e-03, 2.8973e-03],\n",
      "         [3.9902e-03, 5.3978e-03, 7.0496e-03,  ..., 1.4984e-02,\n",
      "          1.2993e-02, 1.0605e-02]],\n",
      "\n",
      "        [[1.6212e-03, 2.0390e-03, 2.4109e-03,  ..., 1.1304e-01,\n",
      "          1.1182e-01, 1.0510e-01],\n",
      "         [3.3379e-04, 4.1175e-04, 4.9114e-04,  ..., 1.8958e-01,\n",
      "          1.8872e-01, 1.8506e-01],\n",
      "         [6.0806e-03, 7.6294e-03, 9.2468e-03,  ..., 1.7847e-01,\n",
      "          1.7810e-01, 1.7676e-01],\n",
      "         ...,\n",
      "         [9.0480e-05, 1.6153e-04, 2.1851e-04,  ..., 1.1055e-02,\n",
      "          9.8648e-03, 1.3466e-03],\n",
      "         [1.9455e-04, 3.7360e-04, 5.2357e-04,  ..., 2.1954e-03,\n",
      "          2.0466e-03, 1.1034e-03],\n",
      "         [9.3269e-04, 1.5726e-03, 2.3308e-03,  ..., 2.8183e-02,\n",
      "          2.7664e-02, 2.5513e-02]],\n",
      "\n",
      "        [[9.6858e-05, 1.2326e-04, 1.4591e-04,  ..., 5.5115e-02,\n",
      "          5.4138e-02, 5.0201e-02],\n",
      "         [4.8101e-05, 6.2883e-05, 7.7128e-05,  ..., 2.1652e-02,\n",
      "          1.7151e-02, 1.5419e-02],\n",
      "         [5.2452e-04, 6.4707e-04, 7.4625e-04,  ..., 1.4086e-03,\n",
      "          1.0157e-03, 6.5899e-04],\n",
      "         ...,\n",
      "         [6.4075e-05, 7.5638e-05, 1.0300e-04,  ..., 7.4524e-02,\n",
      "          7.4402e-02, 3.0308e-03],\n",
      "         [7.0155e-05, 9.2924e-05, 1.3351e-04,  ..., 7.3975e-02,\n",
      "          7.3853e-02, 7.1960e-02],\n",
      "         [2.1350e-04, 3.0804e-04, 4.5800e-04,  ..., 1.1932e-01,\n",
      "          1.1542e-01, 1.1414e-01]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[5.5730e-05, 6.7353e-05, 7.6652e-05,  ..., 1.0291e-01,\n",
      "          1.0217e-01, 1.0144e-01],\n",
      "         [4.0650e-05, 5.1498e-05, 6.0678e-05,  ..., 5.6519e-02,\n",
      "          4.6844e-02, 3.3844e-02],\n",
      "         [1.1194e-04, 1.4091e-04, 1.7047e-04,  ..., 6.5796e-02,\n",
      "          5.4810e-02, 3.2227e-02],\n",
      "         ...,\n",
      "         [1.5640e-04, 2.0409e-04, 2.4629e-04,  ..., 2.1667e-02,\n",
      "          2.0233e-02, 1.9241e-02],\n",
      "         [3.8862e-04, 4.9305e-04, 6.5994e-04,  ..., 1.3159e-01,\n",
      "          1.1957e-01, 1.0962e-01],\n",
      "         [3.0637e-05, 4.1366e-05, 4.9353e-05,  ..., 7.1655e-02,\n",
      "          6.8054e-02, 6.7200e-02]],\n",
      "\n",
      "        [[1.5497e-05, 1.8954e-05, 2.3544e-05,  ..., 4.8485e-03,\n",
      "          3.8204e-03, 3.5954e-03],\n",
      "         [2.2650e-05, 2.7716e-05, 3.2604e-05,  ..., 2.3136e-03,\n",
      "          2.1267e-03, 1.9798e-03],\n",
      "         [1.6141e-04, 1.9789e-04, 2.4271e-04,  ..., 4.9210e-03,\n",
      "          4.5815e-03, 4.4518e-03],\n",
      "         ...,\n",
      "         [6.2883e-05, 7.7128e-05, 9.3043e-05,  ..., 1.9363e-02,\n",
      "          8.2855e-03, 6.8550e-03],\n",
      "         [3.2711e-04, 3.9816e-04, 4.7994e-04,  ..., 7.6332e-03,\n",
      "          6.7329e-03, 5.7983e-03],\n",
      "         [1.4424e-05, 1.7226e-05, 1.9729e-05,  ..., 1.1473e-03,\n",
      "          7.9584e-04, 6.6662e-04]],\n",
      "\n",
      "        [[1.6212e-05, 2.1577e-05, 2.5570e-05,  ..., 9.3628e-02,\n",
      "          8.2397e-02, 4.5135e-02],\n",
      "         [1.2398e-05, 1.7047e-05, 2.0266e-05,  ..., 3.4058e-02,\n",
      "          3.0350e-02, 2.6108e-02],\n",
      "         [4.7326e-05, 6.4313e-05, 7.7248e-05,  ..., 1.7975e-02,\n",
      "          1.5427e-02, 9.2468e-03],\n",
      "         ...,\n",
      "         [1.2767e-04, 1.6332e-04, 1.8573e-04,  ..., 3.4760e-02,\n",
      "          2.8748e-02, 1.3847e-02],\n",
      "         [2.0051e-04, 2.6178e-04, 2.9588e-04,  ..., 3.8422e-02,\n",
      "          3.0075e-02, 2.1698e-02],\n",
      "         [4.7147e-05, 5.5492e-05, 6.1095e-05,  ..., 3.7933e-02,\n",
      "          3.0731e-02, 1.1192e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.1577e-05, 2.8789e-05, 3.5584e-05,  ..., 8.2445e-04,\n",
      "          6.3753e-04, 5.3883e-04],\n",
      "         [8.9645e-05, 1.1563e-04, 1.3983e-04,  ..., 4.2267e-03,\n",
      "          3.7384e-03, 2.7618e-03],\n",
      "         [8.8692e-04, 1.1463e-03, 1.4591e-03,  ..., 1.4366e-02,\n",
      "          9.8038e-03, 7.3204e-03],\n",
      "         ...,\n",
      "         [4.6492e-05, 6.3360e-05, 7.5519e-05,  ..., 4.4899e-03,\n",
      "          3.8280e-03, 3.3836e-03],\n",
      "         [8.4782e-04, 1.1377e-03, 1.4572e-03,  ..., 2.3926e-02,\n",
      "          1.5533e-02, 1.3756e-02],\n",
      "         [4.9710e-05, 6.6698e-05, 7.7605e-05,  ..., 1.2817e-03,\n",
      "          8.0681e-04, 7.4863e-04]],\n",
      "\n",
      "        [[8.7023e-06, 2.0266e-05, 3.3319e-05,  ..., 2.0962e-03,\n",
      "          2.0828e-03, 1.6928e-03],\n",
      "         [2.6524e-05, 4.6432e-05, 6.7115e-05,  ..., 8.0200e-02,\n",
      "          7.9407e-02, 6.5369e-02],\n",
      "         [2.7800e-04, 4.6682e-04, 6.8045e-04,  ..., 1.6248e-01,\n",
      "          1.6211e-01, 1.5173e-01],\n",
      "         ...,\n",
      "         [6.8963e-05, 1.1712e-04, 1.6212e-04,  ..., 4.0207e-03,\n",
      "          3.9215e-03, 3.5725e-03],\n",
      "         [3.3164e-04, 4.2892e-04, 5.4026e-04,  ..., 1.8509e-02,\n",
      "          1.6632e-02, 1.2589e-02],\n",
      "         [1.2696e-05, 2.4140e-05, 3.5942e-05,  ..., 2.0866e-03,\n",
      "          1.8721e-03, 1.5030e-03]],\n",
      "\n",
      "        [[1.1563e-05, 1.5020e-05, 2.0564e-05,  ..., 5.2185e-02,\n",
      "          5.0110e-02, 2.7374e-02],\n",
      "         [2.4199e-05, 3.2842e-05, 4.5180e-05,  ..., 1.7258e-02,\n",
      "          1.4915e-02, 1.0292e-02],\n",
      "         [1.0902e-04, 1.3733e-04, 1.8752e-04,  ..., 3.1525e-02,\n",
      "          2.9190e-02, 1.8356e-02],\n",
      "         ...,\n",
      "         [4.1664e-05, 5.1498e-05, 7.1585e-05,  ..., 5.0659e-02,\n",
      "          4.9835e-02, 4.0283e-02],\n",
      "         [2.7680e-04, 4.0627e-04, 5.9700e-04,  ..., 2.4292e-02,\n",
      "          1.8356e-02, 1.2810e-02],\n",
      "         [2.5213e-05, 3.2246e-05, 4.2677e-05,  ..., 7.6721e-02,\n",
      "          7.6172e-02, 7.4829e-02]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[6.6280e-05, 8.7500e-05, 1.0264e-04,  ..., 8.1360e-02,\n",
      "          7.6660e-02, 7.1533e-02],\n",
      "         [2.4617e-05, 3.2187e-05, 4.0412e-05,  ..., 1.5320e-01,\n",
      "          1.5247e-01, 1.5125e-01],\n",
      "         [1.3375e-04, 1.6594e-04, 1.9455e-04,  ..., 8.2886e-02,\n",
      "          7.8613e-02, 6.2988e-02],\n",
      "         ...,\n",
      "         [4.6372e-05, 5.6088e-05, 6.5565e-05,  ..., 1.7395e-02,\n",
      "          1.5854e-02, 1.5015e-02],\n",
      "         [1.8322e-04, 2.2328e-04, 2.6894e-04,  ..., 1.5637e-01,\n",
      "          1.5369e-01, 1.4990e-01],\n",
      "         [9.1195e-06, 1.2159e-05, 1.6391e-05,  ..., 8.4900e-02,\n",
      "          8.1360e-02, 5.9814e-02]],\n",
      "\n",
      "        [[2.4974e-05, 3.0041e-05, 3.6836e-05,  ..., 1.3748e-02,\n",
      "          1.2772e-02, 1.2054e-02],\n",
      "         [2.4199e-05, 2.7478e-05, 3.1769e-05,  ..., 4.8256e-03,\n",
      "          4.2839e-03, 3.8090e-03],\n",
      "         [8.3447e-05, 1.0359e-04, 1.3626e-04,  ..., 2.8870e-02,\n",
      "          2.6398e-02, 2.5757e-02],\n",
      "         ...,\n",
      "         [1.3232e-05, 1.7345e-05, 2.1636e-05,  ..., 2.6569e-03,\n",
      "          1.6565e-03, 1.3924e-03],\n",
      "         [2.7180e-05, 3.3319e-05, 4.1306e-05,  ..., 5.9319e-03,\n",
      "          5.2223e-03, 4.6082e-03],\n",
      "         [2.2054e-06, 3.8147e-06, 6.3777e-06,  ..., 2.2125e-02,\n",
      "          1.8188e-02, 1.7334e-02]],\n",
      "\n",
      "        [[1.8418e-05, 2.4140e-05, 2.8789e-05,  ..., 3.0838e-02,\n",
      "          2.7557e-02, 2.0721e-02],\n",
      "         [1.0133e-05, 1.3530e-05, 1.6868e-05,  ..., 1.8762e-01,\n",
      "          1.7603e-01, 1.0437e-01],\n",
      "         [7.8678e-05, 9.8646e-05, 1.1450e-04,  ..., 1.6211e-01,\n",
      "          1.5393e-01, 1.0779e-01],\n",
      "         ...,\n",
      "         [1.5557e-05, 1.9550e-05, 2.3067e-05,  ..., 5.9448e-02,\n",
      "          2.7847e-02, 9.8724e-03],\n",
      "         [4.3035e-05, 5.7578e-05, 6.7592e-05,  ..., 3.1586e-02,\n",
      "          2.8702e-02, 2.0615e-02],\n",
      "         [3.6359e-06, 6.6161e-06, 8.8215e-06,  ..., 2.3816e-01,\n",
      "          2.3267e-01, 2.0215e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.0163e-04, 1.3697e-04, 1.6999e-04,  ..., 3.2558e-03,\n",
      "          2.8095e-03, 2.3785e-03],\n",
      "         [4.3154e-05, 5.7876e-05, 7.2479e-05,  ..., 5.4884e-04,\n",
      "          3.6049e-04, 3.1424e-04],\n",
      "         [4.3106e-04, 5.1403e-04, 5.9652e-04,  ..., 2.9892e-02,\n",
      "          2.3193e-02, 1.8005e-02],\n",
      "         ...,\n",
      "         [1.2434e-04, 1.6129e-04, 1.8728e-04,  ..., 1.9274e-03,\n",
      "          8.7547e-04, 7.4100e-04],\n",
      "         [6.5625e-05, 8.5115e-05, 1.0085e-04,  ..., 1.1692e-03,\n",
      "          9.7179e-04, 8.2874e-04],\n",
      "         [3.8505e-05, 5.1200e-05, 6.5565e-05,  ..., 4.4769e-02,\n",
      "          3.5919e-02, 2.4872e-02]],\n",
      "\n",
      "        [[3.1769e-05, 5.4896e-05, 8.8811e-05,  ..., 8.6670e-03,\n",
      "          8.5297e-03, 7.8812e-03],\n",
      "         [5.0664e-06, 1.4126e-05, 2.2054e-05,  ..., 1.9665e-03,\n",
      "          1.9369e-03, 1.8597e-03],\n",
      "         [9.6440e-05, 1.4508e-04, 2.0599e-04,  ..., 4.9713e-02,\n",
      "          4.9225e-02, 4.4312e-02],\n",
      "         ...,\n",
      "         [4.7684e-06, 8.2850e-06, 1.0729e-05,  ..., 8.0490e-04,\n",
      "          7.7200e-04, 6.6805e-04],\n",
      "         [3.9339e-06, 8.4639e-06, 1.2040e-05,  ..., 9.9564e-04,\n",
      "          8.9455e-04, 7.8201e-04],\n",
      "         [6.1393e-06, 1.3649e-05, 2.0921e-05,  ..., 1.3745e-01,\n",
      "          1.3599e-01, 1.1011e-01]],\n",
      "\n",
      "        [[4.4346e-05, 6.5684e-05, 8.5950e-05,  ..., 2.0065e-02,\n",
      "          1.8173e-02, 1.2344e-02],\n",
      "         [1.6749e-05, 2.4736e-05, 3.5822e-05,  ..., 5.8014e-02,\n",
      "          5.7129e-02, 4.8157e-02],\n",
      "         [1.0234e-04, 1.5211e-04, 2.0421e-04,  ..., 7.1472e-02,\n",
      "          6.2805e-02, 6.1035e-02],\n",
      "         ...,\n",
      "         [4.9472e-06, 7.1526e-06, 1.0371e-05,  ..., 1.7624e-02,\n",
      "          1.6708e-02, 1.3229e-02],\n",
      "         [2.3067e-05, 3.1531e-05, 4.5240e-05,  ..., 2.6337e-02,\n",
      "          2.4521e-02, 1.9852e-02],\n",
      "         [4.8876e-06, 9.4175e-06, 1.9193e-05,  ..., 4.8889e-02,\n",
      "          3.9062e-02, 3.6926e-02]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[7.4291e-04, 1.0147e-03, 1.3590e-03,  ..., 6.6345e-02,\n",
      "          5.1971e-02, 4.3182e-02],\n",
      "         [4.5085e-04, 5.8222e-04, 6.9332e-04,  ..., 2.3352e-01,\n",
      "          1.9385e-01, 1.1322e-01],\n",
      "         [9.7418e-04, 1.2693e-03, 1.5841e-03,  ..., 2.3148e-02,\n",
      "          1.8600e-02, 1.4633e-02],\n",
      "         ...,\n",
      "         [2.9640e-03, 3.7613e-03, 4.4289e-03,  ..., 3.3997e-02,\n",
      "          3.2471e-02, 2.2049e-02],\n",
      "         [1.1873e-03, 1.5078e-03, 1.7347e-03,  ..., 5.7037e-02,\n",
      "          5.1575e-02, 4.6570e-02],\n",
      "         [7.9441e-04, 1.0328e-03, 1.2445e-03,  ..., 3.5736e-02,\n",
      "          2.9846e-02, 2.1820e-02]],\n",
      "\n",
      "        [[2.7037e-04, 4.7302e-04, 7.3814e-04,  ..., 3.1311e-02,\n",
      "          2.7267e-02, 2.5726e-02],\n",
      "         [8.4043e-05, 1.1665e-04, 1.5640e-04,  ..., 1.0876e-01,\n",
      "          9.2224e-02, 8.7952e-02],\n",
      "         [5.5742e-04, 7.5340e-04, 9.6655e-04,  ..., 9.4666e-02,\n",
      "          9.2041e-02, 8.7402e-02],\n",
      "         ...,\n",
      "         [8.8644e-04, 1.0853e-03, 1.3781e-03,  ..., 3.6926e-02,\n",
      "          3.4332e-02, 3.3630e-02],\n",
      "         [1.3237e-03, 1.7567e-03, 2.2316e-03,  ..., 1.0078e-02,\n",
      "          9.1476e-03, 8.5754e-03],\n",
      "         [2.2113e-04, 2.8682e-04, 3.8457e-04,  ..., 1.7166e-02,\n",
      "          1.4618e-02, 1.3695e-02]],\n",
      "\n",
      "        [[3.8123e-04, 5.5456e-04, 7.5769e-04,  ..., 4.5441e-02,\n",
      "          3.7170e-02, 2.9617e-02],\n",
      "         [1.9407e-04, 2.5129e-04, 3.0375e-04,  ..., 1.7383e-01,\n",
      "          1.6260e-01, 1.3660e-01],\n",
      "         [2.8181e-04, 3.7503e-04, 4.6611e-04,  ..., 1.8326e-02,\n",
      "          1.5358e-02, 1.3283e-02],\n",
      "         ...,\n",
      "         [2.2068e-03, 2.8076e-03, 3.3512e-03,  ..., 1.2109e-01,\n",
      "          1.1945e-01, 1.1322e-01],\n",
      "         [1.9894e-03, 2.6398e-03, 3.2291e-03,  ..., 4.2297e-02,\n",
      "          4.1290e-02, 3.7231e-02],\n",
      "         [4.3774e-04, 6.0415e-04, 7.6723e-04,  ..., 2.7100e-02,\n",
      "          2.5101e-02, 2.0645e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.8057e-03, 4.0016e-03, 5.4817e-03,  ..., 7.7026e-02,\n",
      "          6.6895e-02, 4.7363e-02],\n",
      "         [7.0381e-04, 8.9550e-04, 1.0395e-03,  ..., 1.0663e-01,\n",
      "          1.0034e-01, 9.3750e-02],\n",
      "         [3.9444e-03, 5.2338e-03, 6.4316e-03,  ..., 7.3547e-02,\n",
      "          6.0760e-02, 5.4596e-02],\n",
      "         ...,\n",
      "         [6.7978e-03, 8.2779e-03, 9.8724e-03,  ..., 3.6316e-02,\n",
      "          3.0167e-02, 2.5146e-02],\n",
      "         [5.7068e-03, 8.0643e-03, 1.0384e-02,  ..., 6.2408e-02,\n",
      "          2.1133e-02, 1.3916e-02],\n",
      "         [5.2414e-03, 7.3738e-03, 9.3613e-03,  ..., 1.8616e-01,\n",
      "          1.1395e-01, 9.8083e-02]],\n",
      "\n",
      "        [[1.1330e-03, 1.8063e-03, 2.6016e-03,  ..., 3.7415e-02,\n",
      "          3.6926e-02, 3.3173e-02],\n",
      "         [2.9349e-04, 4.2033e-04, 5.4312e-04,  ..., 6.1371e-02,\n",
      "          6.1127e-02, 5.8899e-02],\n",
      "         [1.2388e-03, 1.7004e-03, 2.2430e-03,  ..., 3.1006e-02,\n",
      "          3.0487e-02, 2.6978e-02],\n",
      "         ...,\n",
      "         [1.2980e-03, 2.0447e-03, 2.7332e-03,  ..., 2.9602e-02,\n",
      "          2.9480e-02, 2.8961e-02],\n",
      "         [1.1139e-03, 1.9531e-03, 2.6798e-03,  ..., 8.2214e-02,\n",
      "          8.1970e-02, 7.6233e-02],\n",
      "         [4.5753e-04, 7.0667e-04, 9.3794e-04,  ..., 9.2957e-02,\n",
      "          9.0942e-02, 8.7219e-02]],\n",
      "\n",
      "        [[4.6968e-04, 7.5912e-04, 1.0471e-03,  ..., 4.6692e-02,\n",
      "          4.3915e-02, 3.9795e-02],\n",
      "         [1.0848e-04, 1.5295e-04, 1.8919e-04,  ..., 2.3169e-01,\n",
      "          2.2595e-01, 2.1362e-01],\n",
      "         [5.1975e-04, 7.1192e-04, 8.8358e-04,  ..., 1.6281e-02,\n",
      "          1.3405e-02, 1.0994e-02],\n",
      "         ...,\n",
      "         [7.5436e-04, 1.0605e-03, 1.3485e-03,  ..., 1.5511e-02,\n",
      "          1.1017e-02, 1.0864e-02],\n",
      "         [5.2977e-04, 7.7391e-04, 9.9564e-04,  ..., 1.3008e-02,\n",
      "          1.1459e-02, 1.1269e-02],\n",
      "         [2.6083e-04, 4.0555e-04, 5.4932e-04,  ..., 2.1759e-02,\n",
      "          2.0279e-02, 1.5053e-02]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[1.2696e-05, 1.8179e-05, 2.5928e-05,  ..., 3.9398e-02,\n",
      "          3.7354e-02, 2.2095e-02],\n",
      "         [1.5914e-05, 1.9729e-05, 2.2411e-05,  ..., 1.7725e-01,\n",
      "          1.7383e-01, 9.7656e-02],\n",
      "         [4.5717e-05, 5.7101e-05, 6.5923e-05,  ..., 4.9866e-02,\n",
      "          4.6173e-02, 3.0655e-02],\n",
      "         ...,\n",
      "         [9.8133e-04, 1.2941e-03, 1.5697e-03,  ..., 4.8004e-02,\n",
      "          4.2206e-02, 2.9755e-02],\n",
      "         [1.5676e-04, 2.2602e-04, 2.9063e-04,  ..., 1.1139e-01,\n",
      "          1.0706e-01, 8.6182e-02],\n",
      "         [9.7990e-05, 1.4842e-04, 2.0647e-04,  ..., 3.0334e-02,\n",
      "          2.7496e-02, 2.3041e-02]],\n",
      "\n",
      "        [[3.5167e-06, 4.7684e-06, 7.5698e-06,  ..., 3.4332e-02,\n",
      "          3.2990e-02, 3.1677e-02],\n",
      "         [6.6161e-06, 7.5102e-06, 8.1062e-06,  ..., 8.8379e-02,\n",
      "          8.6121e-02, 8.4717e-02],\n",
      "         [1.0312e-05, 1.3173e-05, 1.6868e-05,  ..., 2.5955e-02,\n",
      "          2.3575e-02, 1.7380e-02],\n",
      "         ...,\n",
      "         [2.8515e-04, 3.8481e-04, 4.9639e-04,  ..., 2.2629e-02,\n",
      "          1.9455e-02, 1.4450e-02],\n",
      "         [6.0260e-05, 8.3029e-05, 1.0103e-04,  ..., 6.7627e-02,\n",
      "          5.5389e-02, 5.1147e-02],\n",
      "         [3.9876e-05, 5.3585e-05, 7.2479e-05,  ..., 1.2159e-03,\n",
      "          1.0986e-03, 8.3637e-04]],\n",
      "\n",
      "        [[6.7949e-06, 9.1195e-06, 1.1742e-05,  ..., 7.9773e-02,\n",
      "          7.8796e-02, 6.1615e-02],\n",
      "         [1.4901e-05, 1.8537e-05, 2.1517e-05,  ..., 4.0112e-01,\n",
      "          3.9160e-01, 3.0249e-01],\n",
      "         [2.0802e-05, 2.7955e-05, 3.4332e-05,  ..., 1.5771e-01,\n",
      "          1.3965e-01, 6.6895e-02],\n",
      "         ...,\n",
      "         [6.7902e-04, 9.2411e-04, 1.1168e-03,  ..., 4.7180e-02,\n",
      "          4.2236e-02, 3.4241e-02],\n",
      "         [4.8757e-05, 6.7174e-05, 8.3685e-05,  ..., 1.1664e-01,\n",
      "          1.1432e-01, 1.0437e-01],\n",
      "         [5.2571e-05, 7.3373e-05, 9.4473e-05,  ..., 8.1177e-03,\n",
      "          5.9090e-03, 4.3106e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[4.8518e-05, 6.9320e-05, 9.1076e-05,  ..., 3.4119e-02,\n",
      "          3.0319e-02, 2.4765e-02],\n",
      "         [3.6478e-05, 4.6194e-05, 5.2750e-05,  ..., 3.8208e-02,\n",
      "          2.4948e-02, 1.1398e-02],\n",
      "         [1.3411e-04, 1.8036e-04, 2.2542e-04,  ..., 8.3923e-03,\n",
      "          6.7329e-03, 3.2253e-03],\n",
      "         ...,\n",
      "         [6.3753e-04, 8.5020e-04, 1.0958e-03,  ..., 2.1988e-02,\n",
      "          1.5144e-02, 1.2413e-02],\n",
      "         [9.1791e-05, 1.3447e-04, 1.7786e-04,  ..., 6.3965e-02,\n",
      "          3.6438e-02, 3.0640e-02],\n",
      "         [1.5378e-04, 2.2948e-04, 3.2234e-04,  ..., 3.1812e-01,\n",
      "          6.6872e-03, 3.6907e-03]],\n",
      "\n",
      "        [[2.1815e-05, 5.1737e-05, 9.6262e-05,  ..., 3.3234e-02,\n",
      "          3.3112e-02, 3.2593e-02],\n",
      "         [1.6332e-05, 2.7239e-05, 3.7253e-05,  ..., 8.2947e-02,\n",
      "          8.2703e-02, 8.1177e-02],\n",
      "         [2.8610e-05, 4.7088e-05, 7.0333e-05,  ..., 1.1066e-01,\n",
      "          1.1066e-01, 1.0608e-01],\n",
      "         ...,\n",
      "         [1.1196e-03, 1.4887e-03, 1.8711e-03,  ..., 3.6591e-02,\n",
      "          3.5675e-02, 3.1921e-02],\n",
      "         [9.7871e-05, 1.4365e-04, 1.9264e-04,  ..., 5.5511e-02,\n",
      "          5.5145e-02, 5.1575e-02],\n",
      "         [1.5116e-04, 2.3890e-04, 3.4928e-04,  ..., 1.7700e-02,\n",
      "          1.7548e-02, 1.7410e-02]],\n",
      "\n",
      "        [[6.7949e-06, 1.3947e-05, 2.6584e-05,  ..., 1.2299e-02,\n",
      "          1.0727e-02, 1.0300e-02],\n",
      "         [6.8545e-06, 1.0252e-05, 1.3649e-05,  ..., 1.3220e-01,\n",
      "          1.1774e-01, 1.1078e-01],\n",
      "         [9.5963e-06, 1.5259e-05, 2.2411e-05,  ..., 3.0457e-02,\n",
      "          2.4063e-02, 1.6632e-02],\n",
      "         ...,\n",
      "         [2.7251e-04, 3.8075e-04, 4.9496e-04,  ..., 2.7191e-02,\n",
      "          2.2461e-02, 1.9302e-02],\n",
      "         [4.1723e-05, 5.8830e-05, 7.5877e-05,  ..., 4.7211e-02,\n",
      "          4.4617e-02, 4.3671e-02],\n",
      "         [3.7551e-05, 5.6922e-05, 8.0645e-05,  ..., 6.4430e-03,\n",
      "          3.7556e-03, 3.1300e-03]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[[2.9349e-04, 3.8266e-04, 4.6420e-04,  ..., 2.2314e-01,\n",
      "          1.9128e-01, 1.3513e-01],\n",
      "         [2.7585e-04, 3.7217e-04, 4.4870e-04,  ..., 2.2314e-01,\n",
      "          2.0728e-01, 1.3269e-01],\n",
      "         [8.6308e-04, 1.1501e-03, 1.4467e-03,  ..., 1.2695e-01,\n",
      "          1.2201e-01, 9.9365e-02],\n",
      "         ...,\n",
      "         [2.1327e-04, 2.7347e-04, 3.3593e-04,  ..., 5.5328e-02,\n",
      "          5.2887e-02, 4.7607e-02],\n",
      "         [1.4913e-04, 1.7583e-04, 1.9908e-04,  ..., 2.4097e-01,\n",
      "          2.2998e-01, 1.3916e-01],\n",
      "         [5.8556e-04, 7.7343e-04, 9.6560e-04,  ..., 2.4002e-02,\n",
      "          1.9562e-02, 1.7151e-02]],\n",
      "\n",
      "        [[1.3626e-04, 1.6677e-04, 1.9968e-04,  ..., 1.5540e-01,\n",
      "          1.3171e-01, 1.2225e-01],\n",
      "         [8.4460e-05, 1.0544e-04, 1.2136e-04,  ..., 5.3467e-02,\n",
      "          4.6021e-02, 4.1229e-02],\n",
      "         [4.4823e-04, 5.5122e-04, 6.3467e-04,  ..., 5.5328e-02,\n",
      "          5.2094e-02, 4.3549e-02],\n",
      "         ...,\n",
      "         [2.7108e-04, 3.6168e-04, 4.2677e-04,  ..., 3.4515e-02,\n",
      "          2.7634e-02, 1.5182e-02],\n",
      "         [9.7632e-05, 1.2302e-04, 1.4400e-04,  ..., 8.3374e-02,\n",
      "          7.7515e-02, 7.0007e-02],\n",
      "         [4.6372e-04, 6.2323e-04, 7.5674e-04,  ..., 4.8218e-02,\n",
      "          4.0466e-02, 2.7252e-02]],\n",
      "\n",
      "        [[3.8528e-04, 5.1451e-04, 6.2799e-04,  ..., 2.5439e-01,\n",
      "          2.3047e-01, 1.7004e-01],\n",
      "         [2.0111e-04, 2.7966e-04, 3.4499e-04,  ..., 4.1504e-01,\n",
      "          4.0771e-01, 3.1348e-01],\n",
      "         [7.3671e-04, 9.6655e-04, 1.1501e-03,  ..., 1.8860e-01,\n",
      "          1.5747e-01, 1.1975e-01],\n",
      "         ...,\n",
      "         [8.8096e-05, 1.2219e-04, 1.4818e-04,  ..., 1.6589e-01,\n",
      "          1.4941e-01, 1.1487e-01],\n",
      "         [4.8935e-05, 6.4313e-05, 7.4208e-05,  ..., 5.3125e-01,\n",
      "          5.1221e-01, 3.9429e-01],\n",
      "         [3.5858e-04, 5.8985e-04, 7.8821e-04,  ..., 1.8469e-01,\n",
      "          1.1157e-01, 7.3486e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[3.9864e-04, 5.2404e-04, 6.5327e-04,  ..., 6.5430e-02,\n",
      "          3.1036e-02, 2.6367e-02],\n",
      "         [2.2614e-04, 2.9230e-04, 3.6335e-04,  ..., 4.8492e-02,\n",
      "          2.9785e-02, 1.8341e-02],\n",
      "         [1.1835e-03, 1.5564e-03, 1.9112e-03,  ..., 3.0991e-02,\n",
      "          2.0599e-02, 1.5419e-02],\n",
      "         ...,\n",
      "         [5.4550e-04, 6.8474e-04, 8.2779e-04,  ..., 3.5248e-03,\n",
      "          2.3880e-03, 1.7471e-03],\n",
      "         [7.2479e-05, 9.5248e-05, 1.1498e-04,  ..., 5.7007e-02,\n",
      "          4.1840e-02, 1.9394e-02],\n",
      "         [1.2655e-03, 1.6031e-03, 1.9093e-03,  ..., 3.0861e-03,\n",
      "          1.9855e-03, 1.6727e-03]],\n",
      "\n",
      "        [[1.5187e-04, 2.5606e-04, 3.9744e-04,  ..., 1.2805e-01,\n",
      "          1.2695e-01, 1.2067e-01],\n",
      "         [1.1408e-04, 1.9848e-04, 3.0041e-04,  ..., 2.5073e-01,\n",
      "          2.5049e-01, 2.4463e-01],\n",
      "         [6.5470e-04, 9.8038e-04, 1.3103e-03,  ..., 5.2277e-02,\n",
      "          5.2155e-02, 4.9042e-02],\n",
      "         ...,\n",
      "         [2.5201e-04, 4.2248e-04, 5.9700e-04,  ..., 9.1324e-03,\n",
      "          9.0790e-03, 8.2703e-03],\n",
      "         [2.4152e-04, 3.0828e-04, 3.5858e-04,  ..., 9.5032e-02,\n",
      "          9.4727e-02, 9.2285e-02],\n",
      "         [3.5214e-04, 6.2609e-04, 9.0885e-04,  ..., 8.0338e-03,\n",
      "          8.0109e-03, 6.3171e-03]],\n",
      "\n",
      "        [[8.0228e-05, 1.2457e-04, 1.7750e-04,  ..., 2.4487e-01,\n",
      "          1.9922e-01, 1.5540e-01],\n",
      "         [1.6272e-04, 2.3901e-04, 3.1662e-04,  ..., 1.4355e-01,\n",
      "          1.1890e-01, 1.1145e-01],\n",
      "         [4.9353e-04, 6.9427e-04, 8.9836e-04,  ..., 1.3635e-01,\n",
      "          1.0278e-01, 6.5002e-02],\n",
      "         ...,\n",
      "         [1.5652e-04, 2.3580e-04, 3.1948e-04,  ..., 5.8472e-02,\n",
      "          5.3986e-02, 3.2806e-02],\n",
      "         [1.7035e-04, 1.9813e-04, 2.2364e-04,  ..., 2.2791e-01,\n",
      "          1.9287e-01, 1.8298e-01],\n",
      "         [2.8086e-04, 4.3678e-04, 6.2132e-04,  ..., 2.3453e-02,\n",
      "          2.0737e-02, 1.1497e-02]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward1>))\n",
      "First dim length:  30\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "Sublen:  9\n",
      "original summed weights shape:  torch.Size([9, 2000])\n",
      "Flat weights shape:  torch.Size([48])\n",
      "Attention weight chunk:  torch.Size([1907])\n",
      "Flat weights shape:  torch.Size([1955])\n",
      "Attention weight chunk:  torch.Size([1907])\n",
      "Flat weights shape:  torch.Size([3862])\n",
      "Attention weight chunk:  torch.Size([1907])\n",
      "Flat weights shape:  torch.Size([5769])\n",
      "Attention weight chunk:  torch.Size([1907])\n",
      "Flat weights shape:  torch.Size([7676])\n",
      "Attention weight chunk:  torch.Size([1907])\n",
      "Flat weights shape:  torch.Size([9583])\n",
      "Attention weight chunk:  torch.Size([1907])\n",
      "Flat weights shape:  torch.Size([11490])\n",
      "Attention weight chunk:  torch.Size([1907])\n",
      "Flat weights shape:  torch.Size([13397])\n",
      "Attention weight chunk:  torch.Size([1907])\n",
      "Flat weights shape:  torch.Size([15304])\n",
      "Attention weight chunk:  torch.Size([1904])\n",
      "Flat weights shape:  torch.Size([17208])\n",
      "tensor([ 0.5410,  0.5322,  0.4902,  ..., 28.0469, 21.0000, 17.0312],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<CatBackward0>)\n",
      "tensor([   10,    11,    12,  ..., 17205, 17206, 17207])\n",
      "Decoded text:  LLMs) have made, with the Key- However, the growth of the KV To address this problem, this paper an innovative and fine- comparable performance in real- evaluation of the generated context sizes of we identify the patterns of these important attention features robustness of to explore the patterns of pattern in the attention allocated nature of the user's most of the LLMs we conduct the Needle.\n",
      "\n",
      "     works address the KV cache compression byicting the KV} maintains the first few tokens and the local effectively reduce the KV., faces the challenge ofV}} \n",
      "Another to compress the KV generation steps cumulative attention. While this effectively compresses the KVooks compression of the input sequence is for reducing memory and computational overhead.\n",
      "Building on similar concept, Adaptive Kression (FastGen compression policies. Initially, obtained from prompt encoding. Subsequently,aches during the generation phase these policies. Nonetheless, it faces the similar problem2 focuses on identifying and retaining pivotal tokens that exhibit a consistent attention weight pattern previous token windows generation steps. However, this method concentrates solely on the window of previous pivotal in generation and neglects the extensive input that contains essential information for generating accurate responses. This oversight could lead to an inability to extract detailed information from prompts.\n",
      "\n",
      "In summary, existing compression methods merely address the challenges encountered in real-world applications, such as document processing and multi-round chats, where prompts are exceptionally long yet require accurate information retrieval. In common use cases, the generated outputs, like summaries, code pieces, or retrieved data, are significantly shorter compared to the extensive input sequences from novels, entire code bases, or annual financial reports. Although these techniques may effectively reduce the KV cache size during the generation phase, they do not tackle the primary overhead and challenges arising from a lack of comprehension of complex input contexts, thus leaving the critical issues unresolved.\n",
      "\\section{Observations}\\label{sec: obs}\n",
      "\n",
      "In this section, we present our observations regarding the patterns in the Query-Key matrix during token generation. We discuss how these patterns can be potentially exploited for KV cache compression. Our findings are based on the analysis of various generation contexts and the behavior of attention mechanisms in LLMs and are concluded into three key observations as follows: \n",
      "\n",
      "\\begin{enumerate}\n",
      "\\item \\textbf{Pattern consistency across contexts:} Irrespective of the generation context length, we observed keys within the prompt consistently related to the structure and content of the prompt.ref{sec: Multi subset of of the generation outcome) represents the attention features between the current generated quantifies the alignment of the current attention ratio of the sum of metric for the efficacy of the attention mechanism in recognizing features within the context. We can combination of eq.p = 0. due to the softmax) for the observation experiments. The model we probe is2}.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\subsection{Observations in Multi-Turn Conversations}\n",
      "\\label}\n",
      "\n",
      "\\ering\n",
      "    % \\scale = 0./hit_ratescale = 0.scale = 0.\n",
      "    The layer-wise hit rate of important positions utilized along token generation with an length exceeding 3k.\n",
      "    }\n",
      "   }\n",
      "\n",
      "\\begin/question_front. hit rate of important positions utilized by prompts with questions at the beginning and the end.}\n",
      "   pos}}\n",
      "\n",
      "This study examines if the positions of features identified as crucial in the observation window maintain their significance in the subsequent token generation. The analysis utilizes samples from Ultrachat~\\cite{ding2023enhancing}, a multi-turns, high-quality instruction dataset consisting of 1.4 million dialogues. We further filter the sequences with response length greater than 512 and prompt length greater3k. In the experiment, we split the generated tokens into 4 context windows, each spanning8 tokens, to compute the averaged hit rates of these windows versus the observation window with size 32. According to the findings presented in Fig.\\ref{fig: hit_rate}, important keys in prefixes obtained from voting in observation windows exhibit remarkable consistency throughout the generation process, as evidenced by high hit rates. \n",
      "\n",
      "\\begin{figure}[ht]\n",
      "    \\centering\n",
      "    \\includ[width=0.7\\width]{figures/different_answer_pairs.pdf}\n",
      "    \\caption{The layer-wise overlap of important positions utilized by different question-answer pairs in the same dataset.\n",
      "    }\n",
      "    \\label{fig: qa_pairs}\n",
      "\\end{figure}\n",
      "\\subsection{Observations in Long Document QA}\n",
      "To further validate this finding, we also observe on multiple long documents QA datasets including QMSum \\cite{zhong2021q a collection of papers fromite{angelidis2V} it is the prompt phase weight along the query dimension based on the pooled weights # Expand to match the head dimension # Gather the compressed past based on the selected indices}\n",
      "\n",
      "\n",
      "\\sub{Efficient Clustering via Pooling} clustering}\n",
      "In LLMs, information retrieval and generation rely on attention weight and are supplemented by copying the rest in induction heads~\\2context}. Hence, naively selecting the top features only portions of details and then losing the completeness of the information. For example, such compression might cause the LLMs to retrieve only the country code of phone number and hallucinate the rest. Our experiment also revealed that only selecting the features with the highest weights is insufficient (Sec.~lation}).  Such sparse selection risks compromising the contextual integrity encapsulated in between features, thereby reducing accuracy. Based on the insights, We propose a fine-grained clustering algorithm utilizing a pooling layer shown in Lineing}.\n",
      "\\section{Experiments}\n",
      "In our experimental setup, we explore the performance of \\kv across models that can handle extended sequence contexts. First, we deliver a pressure test and benchmark the speed of \\textttLWM-Text-Chat-1M}~\\4world}, which is state-the-art regarding its context length.\n",
      "We then conduct an ablation study on \\textistralv0.2} to understand the influence of pooling on the model's information retrieval performance. We assess model performances using the LongBench~cbench} dataset. Further, we dive into a comprehensive examination of the \\texttt{Command-R}r} model, another leading open-source model in the field. Lastly, we show that \\kv can be utilized with other acceleration strategies such as parallel decoding.\n",
      "\n",
      "\\subsection{Benchmarks on LWM-Text-Chat-1M}\n",
      "\n",
      "\\texttt{LWM-Text-Chat-1M}~\\cite{liu24world} is a 7B instruction-finetuned model with up to one million context length. In this section, we conduct a pressure test on this model and examine its algorithmic efficiencies through the lens of hardware optimization.\n",
      "\n",
      "\n",
      "\\subsubsection{Needle-in-a-Haystack} \n",
      "The Needle-in-a-Haystack test \\cite{kamradt2023needle} challenges the model to accurately retrieve information from a specific\"), with the sentence placed we extended the document length which is the longest content We configured from prompt using our algorithm for answer generation,} from the Needle to precisely manage small details on extremely long input contexts ratio. \n",
      "\\ comparison speed of the baseline implementation update during the inference. time for the baseline model, with the consumption. Study of Effectiveness of Pooling}\\ation study of pooling on LongEval-Lines}\", where the key is. The model needs to retrieve the value based key. \n",
      "    Theaxis denotes the length of the input; position of the groundtruth. With the pooling, the better than the one without pooling.\n",
      "    }fig: ablation}\n",
      "\n",
      "We perform an ablation study to assess the impact of our pooling technique, a straightforward but efficient method for consolidating information through clustering. Our evaluation utilizes the modified LongEval-Lines benchmark~\\clongchat23}, incorporating random generated pairs and averaged scores. LongEval- presents a greater challenge compared to Needle- because it involves in noisy contexts of the same format, while in the relevant other contexts. We apply max pooling with kernel size 5 and use the observation window. The findings, illustrated in our results (Fig.: ablation}), indicate that pooling significantly enhances retrieval accuracy compared to methods not utilizing pooling. We hypothesize that this is due to the ability of strong attention mechanisms to focus on the initial portion of tokens. Without information compression, large language models tend to replicate the subsequent tokens, leading to retrieved partially correct results when the KV cache is compressed as we observed. Note that throughout our experiments, the choice between max pooling and average pooling did not yield significant differences in performance.\n",
      "\\begin{table*}[t]\n",
      "\n",
      "\\fontsselectfont\n",
      "\\set\\centering\n",
      "\\caption{Performance comparison of \\kv and H2O across various LLMs on LongBench.}\\label{tab:longbench}\n",
      "\\begintable}\n",
      "\\scale3}{\n",
      "\\\n",
      "\\special}\n",
      "&\\multi*}{~~~LLMs\\tnote{a}} &c}{Single-Document QA} &-Document QASummarization}&Few-shot Learning}&}{Synthetic} &c}{Code} \\\\\n",
      "\\cmidrule(lr){3-(lr){6lr){9)11)18}\n",
      "&& \\rotatebox[origin=c0}{NrtvQA} & \\rotate}{Qasper} & \\MF-en} & \\0}{HotpotQA} & \\}{2WikiMQA} & \\0}{Musique} & \\rotate0}{GovReport} & \\rotate=0}{QMSum} & \\rotatebox[origin=c]{30}{MultiNews} & \\rotatebox[origin=c]{30}{TREC} & \\rotatebox[origin=c]{30}{TriviaQA} & \\rotatebox[origin=c]{30}{SAMSum} & \\rotatebox[origin=c]{30}{PCount} & \\rotatebox[origin=c]{30}{PRe} & \\rotatebox[origin=c]{30}{L]{3[origin=c]{WMChat}}\n",
      "\n",
      "&\\cell~~~All KV &8.kv: 100\\\\\n",
      "\n",
      "\n",
      "\n",
      "&kv: 20\\\\\n",
      "\n",
      "\n",
      "&\\kv: 408\\\\\n",
      "\n",
      "&\\~H2O: 4016230.97\\\\\n",
      "\n",
      "\n",
      "\\specialrule{1pt}{2pt}{10pt}\\specialrule2pt}\n",
      "\n",
      "\\multirow{5}{*}{\\rotatebox[origin=c]{90}{\\fontsize{selectfont LongChat}}\n",
      "\n",
      "& \\cellcolor{blue!10}~~~All KV & \\cell20.88} & \\29.36} &43.2}33.0524814.6630.8922.7626.6166.5}83.99} & \\40.83} & \\blue!10}0.0 & \\cellblue!10}30.5 & \\cell{blue!10}54.89 & \\cellcolor{blue!10}\\textbf{59.05} \\\\\n",
      "\\cline{2-18}\n",
      "\n",
      "& \\cellcolor{blue!10}~~~\\kv: 1024 & \\cellcolor{blue!10}19.32 & \\cellcolor{blue!10}26.6 & \\cellcolor{blue!10}37.93 & \\.1}.color{blue!10} & \\cellcolor{ \\\\\n",
      "\n",
      "& \\\\kv: 0\n",
      "\n",
      "kv: 40H2O: 40 \\\\\n",
      "\n",
      "\\specialrule{1pt}\n",
      "\n",
      "\n",
      "\n",
      "\\multirow{5*}{\\rotatebox[originselectfont Mistral}}\n",
      "\n",
      "& \\cellcolor{~~~All KV & \\cell8} \\\\\n",
      "\\cline{218}\n",
      "\n",
      "& \\cell}~~~\\kv: 1024 & \\5187 \\\\\n",
      "\n",
      "\n",
      "& \\cellcolor{red!10}~~~\\kv: 2048 & \\cell25.8932.4748.6 &41.7127.3118.6928.8110}\\textbf24.5} & \\026.6 &10}70.0 &red!10}86.27 &cell{red!10}42.47 & \\cell{red!10}3.09 & \\cellcolor{red!10}87.43 & \\cellcolor{red!10}\\textbf{55.93} & \\cellcolor{red!10}52.01 \\\\\n",
      "\n",
      "\n",
      "& \\cellcolor{red!10}~~~\\kv: 4096 & \\cellcolor{red!10}26.41 & \\}\\.color{red!10}4 & \\cellcolor{ \\\\\n",
      "\n",
      "&O: \n",
      "\n",
      "\\specialrule{}\n",
      "\n",
      "\n",
      "\\multirow{ixtral}}\n",
      "\n",
      "& \\~~~All KV &\n",
      "\\}\n",
      "\n",
      "&\\kv: 108 \\\\\n",
      "\n",
      "& \\~\\kv: 201} \\\\\n",
      "\n",
      "& \\~~~\\kv: 4096262527.9475.590.7147.14}}5.5 &00.0} & \\068.81069.56 \\\\\n",
      "\n",
      "& \\cellcolor{cyan!10}~~~H2O: 4096 & \\cellcolor{cyan!10}20.45 & \\cellcolorcyan!10}32.09 & \\cellcolor{cyan!10}48.02 & \\cellcolor{cyan!10}34.76 & \\cellcolor{cyan!10}25.69 & \\cellcolor{cyan!10}16.5 & \\cellcolor{cyan!10}29.76 & \\cellcolor{cyan!10}23.53 & \\cellcolor{cyan!1!10}4O on the LongBen further assess the performance of NarrativeQA and modified version of the Needle we limit the KV depending on the sequence length.\n",
      "\n",
      "\\ previous influenced by the specific context used. To address, we modify the evaluation by each length and depth combination. This approach robust results. We observe across all under compared to the original setup shuffling. For simplicity we lengths for the baseline model and seen in in performanceV cache.\n",
      "\n",
      "\\begin{caption{Needles-inHaystack Test Results}\n",
      "Score & \\label{section{Retrieval Augmented Generation (RAG)}\n",
      "We assess \\kv's effectiveness in RAG tasks, which are more intricate than synthetic long-context tasks like NeedHaystack and closer to real use cases compared to tasks like NarrativeQA. RAG tasks require selecting pertinent documents from an indexed corpus based on the given prompt. An expanded context window enables the retrieval of additional documents, which can lead to improved model performance. However, this also increases memory requirements and latency, highlighting the delicate balance between retrieval scope and system resources. \\kv proves beneficial in these tasks by reducing memory usage while enhancing the performance. We evaluated \\kv's impact on RAG tasks with sequence lengths up to approximately40 tokens.\n",
      "\n",
      "\\paragraph{RAG Citation}\n",
      "We begin by assessing \\kv's impact on the model's ability to select relevant documents, a crucial aspect of effective RAG. We evaluate on an internal benchmarks from Cohere. The setup of the benchmark is as follow: for each prompt, we gathered a set of topic-related documents that included ground truth answers along with a sample of negative documents ensuring a total of 1 per prompt. We measured the model's performance by calculating the F1-score when the model successfully retrieved the ground truth documents. The dataset employed in this experiment spanned context lengths from 200400 tokens. Given our KV cache size of 496, we achieve a compression of 5-10x. As observed in Table \\ref{table:commandr_cite}, \\kv demonstrates a remarkable ability to retain nearly 98.8\\% of \\texttt{Command-R}'s performance.\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{RAG Test Results}\n",
      "\\begin{tabular}{lcc}\n",
      "\\toprule\n",
      "\\text{Evaluation Task} & \\text{Metric} & \\text{\\% Difference} \\\\\n",
      "\\midrule\n",
      "RAG Citation & F1 score & -1.2\\% \\\\\n",
      "RAG End-to-end & F1 score & -2.1\\% \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\label{table:commandr\n",
      "As the quality of generation isAG capability, we evaluate \\text half of the candidates, version of the Hotpot LL. concern given the autoreg, with the transfer of}.\n",
      "\n",
      "Our investigation incorporates \\- parallel decoding framework multiple classifiers and tree attention drafting tokens, subsequently verified LLMs. One of the challenges identified is the issue of speculative decoding size for the KV during generation \\kv generation efficiency.\n",
      "\n",
      "Empirical results shown in} highlight the performance across various prompt lengths, with a maximum ofively halted. The experiments utilized a subset of the QASdataset}, with a fixed prompt instructing the LLM to summarize the paper. The truncation adopted aligns with} standards, by removing the context in the middle to achieve the desired sequence length for benchmarking.\n",
      "\n",
      "The findings indicate a slowdown in \\textttusa}'s performance as sequence lengths extend, a challenge effectively mitigated by \\kv's intervention, which achieved a 1.3x speedup for sequences with 1 length comparedusa} and a  compared to the native decoding. This improvement underscores the potential of combining KV cache compression with parallel decoding frameworks to enhance LLM efficiency, particularly in long-context scenarios.\n",
      "\\section{Discussions}\n",
      "\\kv emerges as a potent yet straightforward solution, adeptly compressing the KV caches of models to mitigate the computational and memory burdens associated with processing extensive inputs. Originating from a nuanced observation that specific tokens within prompts garner consistent attention from each head during generation, our methodology not only conserves crucial information but also enhances processing efficiency. \n",
      "% limitation\n",
      "Despite its strengths, \\kv's scope is primarily confined to the generative aspect of models, specifically targeting the KV caches during the generation. This limitation implies that \\kv cannot extend a model's long context capability if the model inherently struggles with long contexts or exhibits poor performance. Additionally, \\kv's design does not cover the processing of the prompt inference, which limits its effectiveness in scenarios where the system cannot handle prompts\n",
      "Time taken:  0.8325202689738944\n"
     ]
    }
   ],
   "source": [
    "from einops import repeat\n",
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "top_tokens = []\n",
    "\n",
    "encoded_tokens = tokenizer.encode(sample_text, return_tensors=\"pt\")[0]  \n",
    "print(\"Encoding tokens took: \", time.perf_counter() - start_time)\n",
    "print(\"Encoded tokens shape: \", encoded_tokens.shape)\n",
    "\n",
    "chunk_count = math.ceil((encoded_tokens.shape[-1] - template_window - query_window) / (max_tokens - query_window - template_window))\n",
    "print(encoded_tokens[template_window:-query_window])\n",
    "split_tensors = encoded_tokens[template_window:-query_window].chunk(chunks=chunk_count, dim=-1)\n",
    "\n",
    "last_chunk_size = split_tensors[-1].shape[-1]\n",
    "print(\"Last chunk size: \", last_chunk_size)\n",
    "\n",
    "# split_tensors = torch.split(encoded_tokens[template_adjustment:-query_window], max_tokens - query_window - template_adjustment)\n",
    "max_split_tokens = 0\n",
    "for split_tensor in split_tensors:\n",
    "    max_split_tokens = max(max_split_tokens, split_tensor.shape[-1])\n",
    "padded_tensors = []\n",
    "attention_masks = []\n",
    "\n",
    "template_tokens = encoded_tokens[:template_window]\n",
    "query_tokens = encoded_tokens[-query_window:]\n",
    "\n",
    "print(\"Query tokens shape: \", query_tokens.shape)\n",
    "print(\"Chunks shape: \", [t.shape for t in split_tensors])\n",
    "print(\"Template tokens shape: \", template_tokens.shape)\n",
    "\n",
    "max_tokens_length = split_tensors[0].shape[-1] + template_window + query_window\n",
    "\n",
    "for i, chunk_tensor in enumerate(split_tensors):\n",
    "    joined_tensor = torch.cat((template_tokens, chunk_tensor, query_tokens), dim=-1) # -> take care of template tensor later\n",
    "    pad_tensor = torch.tensor(tokenizer.pad_token_id).expand(max_tokens - joined_tensor.shape[-1])\n",
    "    chunk_attention_mask = torch.IntTensor([0] * (max_tokens - chunk_tensor.shape[-1]) + [1] * (chunk_tensor.shape[-1]))\n",
    "    padded_tensors.append(torch.cat((pad_tensor, joined_tensor), dim=-1))\n",
    "    attention_masks.append(chunk_attention_mask)\n",
    "\n",
    "attention_masks = torch.stack(attention_masks, dim=0)\n",
    "padded_tensors = torch.stack(padded_tensors, dim=0)\n",
    "print(\"Generated padded tensors: \", padded_tensors.shape, \" and attention masks: \", attention_masks.shape, \" in \", time.perf_counter() - start_time)\n",
    "\n",
    "padded_tensors = padded_tensors.to(model.device)\n",
    "attention_masks = attention_masks.to(model.device)\n",
    "\n",
    "# batch run \n",
    "outputs = model(padded_tensors, attention_mask=attention_masks, output_attentions=True)#, return_dict_in_generate=True, output_attentions=True)\n",
    "for key in outputs:\n",
    "    print(\"Output key: \", key)\n",
    "print(\"Forward pass completed in: \", time.perf_counter() - start_time)\n",
    "\n",
    "print(\"Output attentions: \", outputs.attentions)\n",
    "print(\"First dim length: \", len(outputs.attentions))\n",
    "for x in outputs.attentions:\n",
    "    print(\"Sublen: \", len(x))\n",
    "\n",
    "summed_weights = torch.stack(outputs.attentions, dim=0).sum((0, 2))\n",
    "print(\"original summed weights shape: \", summed_weights.shape)\n",
    "\n",
    "# Isolate the final tensor\n",
    "# TODO: do the math to debug this...\n",
    "# print(\"Starting template window chunk size: \", summed_weights[0, :template_window].shape)\n",
    "# print(\"Main portion: \", summed_weights[:-1, template_window:].flatten().shape)\n",
    "# print(\"End portion: \", summed_weights[-1, -last_chunk_size:].shape)\n",
    "\n",
    "first_chunk_size = split_tensors[0].shape[-1]\n",
    "flat_weights = summed_weights[0, - first_chunk_size - template_window : - first_chunk_size]\n",
    "\n",
    "\n",
    "for split_chunk, attn_weight_chunk in zip(split_tensors, summed_weights):\n",
    "    print(\"Flat weights shape: \", flat_weights.shape)\n",
    "    print(\"Attention weight chunk: \", attn_weight_chunk[-split_chunk.shape[-1]:].shape)\n",
    "    flat_weights = torch.cat((flat_weights, attn_weight_chunk[-split_chunk.shape[-1]:]), dim=-1)\n",
    "\n",
    "# Adding the template, the summed weights in the middle, and only the relevant portion at the end\n",
    "\n",
    "# summed_weights = torch.stack(summed_weights, dim=0)\n",
    "# summed_weights = summed_weights.sum(dim=0)\n",
    "print(\"Flat weights shape: \", flat_weights.shape)\n",
    "print(flat_weights)\n",
    "\n",
    "indices = torch.sort(flat_weights.topk(max_tokens * 2, dim=-1).indices.cpu()).values\n",
    "print(indices)\n",
    "selected_tokens = encoded_tokens[indices]\n",
    "decoded_text = tokenizer.decode(selected_tokens)\n",
    "print(\"Decoded text: \", decoded_text)\n",
    "\n",
    "# get the top tokens attended to by the query tokens\n",
    "# create one single list of \"most important tokens\"\n",
    "# return this string.\n",
    "end_time = time.perf_counter()\n",
    "print(\"Time taken: \", end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
