{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d448e8dd-54df-457a-b0e6-a08d27d18317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52885a962df45a6a89aae8df315c0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.float16, attn_implementation='sdpa').cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4bbafda-78b6-42b1-afd5-b51d1ca67c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b56af5e8-c81e-4b5a-9067-0cfe11216980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17256 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17256])\n"
     ]
    }
   ],
   "source": [
    "sample_text = open(\"snapkv.txt\", \"r\").read()\n",
    "encoded_tokens = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "for key in encoded_tokens:\n",
    "    encoded_tokens[key] = encoded_tokens[key].cuda()\n",
    "print(encoded_tokens.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4831a57a-0037-41a5-acda-4107cc712580",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_window = 48\n",
    "template_window = 48\n",
    "max_tokens = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c32c0de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# perform qk calculation and get indices\n",
    "# this version will not update in inference mode\n",
    "class SnapKVCluster():\n",
    "    def __init__(self, query_window = 64, template_window = 64, max_capacity_prompt = 256 + 64, kernel_size = 5, pooling = 'avgpool'):\n",
    "        self.query_window = query_window\n",
    "        self.template_window = template_window\n",
    "        self.max_capacity_prompt = max_capacity_prompt\n",
    "        assert self.max_capacity_prompt - self.query_window > 0\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def update_kv(self, key_states, query_states, value_states, attention_mask, num_key_value_groups):\n",
    "        # check if prefix phase\n",
    "        assert key_states.shape[-2] == query_states.shape[-2]\n",
    "        bsz, num_heads, q_len, head_dim = query_states.shape\n",
    "        if q_len < self.max_capacity_prompt:\n",
    "            return key_states, value_states\n",
    "        else:\n",
    "            attn_weights = torch.matmul(query_states[..., -self.query_window:, :], key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "            mask = torch.full((self.query_window, self.query_window), torch.finfo(attn_weights.dtype).min, device=attn_weights.device)\n",
    "            mask_cond = torch.arange(mask.size(-1), device=attn_weights.device)\n",
    "            mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "            mask = mask.to(attn_weights.device)\n",
    "            attention_mask = mask[None, None, :, :]\n",
    "\n",
    "            attn_weights[:, :, -self.query_window:, -self.query_window:] += attention_mask\n",
    "            attn_weights = torch.where(torch.isinf(attn_weights), torch.tensor(-1e9, dtype=attn_weights.dtype), attn_weights)\n",
    "            # print(\"Weights before softmax: \", attn_weights)\n",
    "            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "            attn_weights_sum = attn_weights[:, :, -self.query_window:, :-self.query_window].sum(dim = -2)\n",
    "            if self.pooling == 'avgpool':\n",
    "                attn_cache = F.avg_pool1d(attn_weights_sum, kernel_size = self.kernel_size, padding=self.kernel_size//2, stride=1)\n",
    "            elif self.pooling == 'maxpool':\n",
    "                attn_cache = F.max_pool1d(attn_weights_sum, kernel_size = self.kernel_size, padding=self.kernel_size//2, stride=1)\n",
    "            else:\n",
    "                raise ValueError('Pooling method not supported')\n",
    "            return attn_cache\n",
    "\n",
    "snapkv_cluster = SnapKVCluster(query_window=query_window, template_window=template_window, max_capacity_prompt=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "224651ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "747fb760-2028-4933-b5ed-1719bac27b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from transformers.cache_utils import Cache\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, repeat_kv\n",
    "\n",
    "def sdpa_forward(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_value: Optional[Cache] = None,\n",
    "    output_attentions: bool = False,\n",
    "    use_cache: bool = False,\n",
    ") -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "    # print(\"Running custom forward function\")\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "    query_states = self.q_proj(hidden_states)\n",
    "    key_states = self.k_proj(hidden_states)\n",
    "    value_states = self.v_proj(hidden_states)\n",
    "\n",
    "    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    kv_seq_len = key_states.shape[-2]\n",
    "    if past_key_value is not None:\n",
    "        kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n",
    "    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
    "\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "\n",
    "    if past_key_value is not None:\n",
    "        cache_kwargs = {\"sin\": sin, \"cos\": cos}  # Specific to RoPE models\n",
    "        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "    key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "    value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "    if attention_mask is not None:\n",
    "        if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
    "            )\n",
    "\n",
    "    # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n",
    "    # Reference: https://github.com/pytorch/pytorch/issues/112577.\n",
    "    if query_states.device.type == \"cuda\" and attention_mask is not None:\n",
    "        query_states = query_states.contiguous()\n",
    "        key_states = key_states.contiguous()\n",
    "        value_states = value_states.contiguous()\n",
    "\n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query_states,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        attn_mask=attention_mask,\n",
    "        dropout_p=self.attention_dropout if self.training else 0.0,\n",
    "        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "        is_causal=self.is_causal and attention_mask is None and q_len > 1,\n",
    "    )\n",
    "\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
    "\n",
    "    attn_output = self.o_proj(attn_output)\n",
    "\n",
    "    attn_cache = snapkv_cluster.update_kv(key_states, query_states, value_states, attention_mask, self.num_key_value_groups)\n",
    "\n",
    "    return attn_output, attn_cache, past_key_value\n",
    "\n",
    "for i in range(len(model.model.layers)):\n",
    "    model.model.layers[i].self_attn.forward = sdpa_forward.__get__(model.model.layers[i].self_attn, type(model.model.layers[i].self_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4794931b-a04c-4e8c-b6f5-08d1b5e540fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding tokens took:  0.038643027015496045\n",
      "Encoded tokens shape:  torch.Size([17256])\n",
      "tensor([   70, 13842,   281,  ...,  2526,  5605, 19926])\n",
      "Query tokens shape:  torch.Size([48])\n",
      "Template tokens shape:  torch.Size([48])\n",
      "Generated padded tensors:  torch.Size([19, 1024])  and attention masks:  torch.Size([19, 1024])  in  0.04112430999521166\n",
      "Output key:  logits\n",
      "Output key:  past_key_values\n",
      "Output key:  attentions\n",
      "Forward pass completed in:  0.6328249570215121\n",
      "Summed weights shape:  torch.Size([17680])\n",
      "tensor([ 0.3438,  0.4636,  0.5820,  ..., 39.0938, 31.0156, 24.6875],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<CatBackward0>)\n",
      "tensor([10329, 10330,  9402,  ...,  1851,  3685,  5495])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 17610 is out of bounds for dimension 0 with size 17256",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m indices \u001b[38;5;241m=\u001b[39m summed_weights\u001b[38;5;241m.\u001b[39mtopk(max_tokens, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(indices)\n\u001b[0;32m---> 59\u001b[0m selected_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mencoded_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m decoded_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(selected_tokens)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoded text: \u001b[39m\u001b[38;5;124m\"\u001b[39m, decoded_text)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 17610 is out of bounds for dimension 0 with size 17256"
     ]
    }
   ],
   "source": [
    "from einops import repeat\n",
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "top_tokens = []\n",
    "\n",
    "encoded_tokens = tokenizer.encode(sample_text, return_tensors=\"pt\")[0]  \n",
    "print(\"Encoding tokens took: \", time.perf_counter() - start_time)\n",
    "print(\"Encoded tokens shape: \", encoded_tokens.shape)\n",
    "\n",
    "chunk_count = math.ceil((encoded_tokens.shape[-1] - template_window - query_window) / (max_tokens - query_window - template_window))\n",
    "print(encoded_tokens[template_window:-query_window])\n",
    "split_tensors = encoded_tokens[template_window:-query_window].chunk(chunks=chunk_count, dim=-1)\n",
    "# split_tensors = torch.split(encoded_tokens[template_adjustment:-query_window], max_tokens - query_window - template_adjustment)\n",
    "max_split_tokens = 0\n",
    "for split_tensor in split_tensors:\n",
    "    max_split_tokens = max(max_split_tokens, split_tensor.shape[-1])\n",
    "padded_tensors = []\n",
    "attention_masks = []\n",
    "\n",
    "template_tokens = encoded_tokens[:template_window]\n",
    "query_tokens = encoded_tokens[-query_window:]\n",
    "\n",
    "print(\"Query tokens shape: \", query_tokens.shape)\n",
    "print(\"Template tokens shape: \", template_tokens.shape)\n",
    "\n",
    "for i, chunk_tensor in enumerate(split_tensors):\n",
    "    joined_tensor = torch.cat((template_tokens, chunk_tensor, query_tokens), dim=-1) # -> take care of template tensor later\n",
    "    pad_tensor = torch.tensor(tokenizer.pad_token_id).expand(max_tokens - joined_tensor.shape[-1])\n",
    "    chunk_attention_mask = torch.IntTensor([0] * (max_tokens - chunk_tensor.shape[-1]) + [1] * (chunk_tensor.shape[-1]))\n",
    "    padded_tensors.append(torch.cat((pad_tensor, joined_tensor), dim=-1))\n",
    "    attention_masks.append(chunk_attention_mask)\n",
    "\n",
    "attention_masks = torch.stack(attention_masks, dim=0)\n",
    "padded_tensors = torch.stack(padded_tensors, dim=0)\n",
    "print(\"Generated padded tensors: \", padded_tensors.shape, \" and attention masks: \", attention_masks.shape, \" in \", time.perf_counter() - start_time)\n",
    "\n",
    "padded_tensors = padded_tensors.to(model.device)\n",
    "attention_masks = attention_masks.to(model.device)\n",
    "\n",
    "# batch run \n",
    "outputs = model(padded_tensors, attention_mask=attention_masks, output_attentions=True)#, return_dict_in_generate=True, output_attentions=True)\n",
    "for key in outputs:\n",
    "    print(\"Output key: \", key)\n",
    "print(\"Forward pass completed in: \", time.perf_counter() - start_time)\n",
    "\n",
    "summed_weights = torch.stack(outputs.attentions, dim=0).sum((0, 2))\n",
    "first_template_window = summed_weights[0, :template_window]\n",
    "summed_weights = summed_weights[:, template_window:]\n",
    "summed_weights = torch.cat((first_template_window, summed_weights.flatten()), dim=0)\n",
    "\n",
    "# summed_weights = torch.stack(summed_weights, dim=0)\n",
    "# summed_weights = summed_weights.sum(dim=0)\n",
    "print(\"Summed weights shape: \", summed_weights.shape)\n",
    "print(summed_weights)\n",
    "\n",
    "indices = summed_weights.topk(max_tokens, dim=-1).indices.cpu()\n",
    "print(indices)\n",
    "selected_tokens = encoded_tokens[indices]\n",
    "decoded_text = tokenizer.decode(selected_tokens)\n",
    "print(\"Decoded text: \", decoded_text)\n",
    "\n",
    "# get the top tokens attended to by the query tokens\n",
    "# create one single list of \"most important tokens\"\n",
    "# return this string.\n",
    "end_time = time.perf_counter()\n",
    "print(\"Time taken: \", end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
