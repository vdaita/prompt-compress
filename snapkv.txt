\subsection{Benchmarks on LWM-Text-Chat-1M}

\texttt{LWM-Text-Chat-1M}~\cite{liu2024world} is a 7B instruction-finetuned model with up to one million context length. In this section, we conduct a pressure test on this model and examine its algorithmic efficiencies through the lens of hardware optimization.

\begin{abstract}
Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces \kv, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications.
We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an `observation' window located at the end of the prompts. Drawing on this insight, \kv automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, \kv achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to baseline models across 16 long sequence datasets. Moreover, \kv can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest \kv's potential for practical applications. Our code is available at ~\url{https://github.com/FasterDecoding/SnapKV}.
\end{abstract}
\section{Introduction}

\subsubsection{Needle-in-a-Haystack} 
The Needle-in-a-Haystack test \cite{kamradt2023needle} challenges the model to accurately retrieve information from a specific sentence("needle") hidden within a lengthy document (the "haystack"), with the sentence placed at a random location. To rigorously evaluate \kv's capabilities, we extended the document length to 380k tokens which is the longest content that can be processed by a single A100-80GB GPU. We configured the prompt KV cache size to 1024, enabling \kv to select the most crucial 1024 attention features from the prompt using our algorithm for answer generation, with a maximum pooling kernel size of 5 and a observation window size of 16. The compelling outcomes in Fig. \ref{fig: needle} from the Needle-in-a-Haystack test underscore \kv's potential to precisely manage small details on extremely long input contexts with a 380x compression ratio. 
\begin{figure}[ht]
    \centering
        \includegraphics[width=0.9\textwidth]{figures/LWM-Text-Chat-1M_compress.pdf}
    \caption{Needle-in-a-Haystack test performance comparison on single A100-80GB GPU, native HuggingFace implementation with only a few lines of code changed. The x-axis denotes the length of the document (the “haystack”); the y-axis indicates the position that the “needle” (a short sentence) is located within the document, from 1K to 380K tokens. For example, 50\% indicates that the needle is placed in the middle of the document. Here LWMChat with \kv is able to retrieve the needle correctly before 160k and with only a little accuracy drop after. Meanwhile, the original implementation encounters OOM error with 33k input tokens.
    }
    \label{fig: needle}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/benchmark.pdf}
    \caption{Deconding speed comparison of baseline implementation and \kv optimized solutions on various batch sizes. The x-axis denotes the input sequence length; the y-axis indicates decoding speed (ms/token). All experiments are conducted on an A100 80GB GPU. The red dotted line denotes the current state-of-the-art open-sourced models' context length.}
    \label{fig: speed}
\end{figure}
\subsubsection{Decoding Speed and Memory Bound}
We further benchmark the speed of \texttt{LWM-Text-Chat-1M} under different batch-size settings using \kv. We set the maximum prompt KV cache size as 2048 for \kv. There are two main takeaways from our experiment on decoding speed and input sequence length on various batch sizes, as shown in Fig. \ref{fig: speed}. First, as the input sequence length increases, the decoding speed of the baseline implementation escalates exponentially. Conversely, the \kv-optimized model maintains a constant decoding speed since the KV cache stays the same and there is no extra update during the inference. For instance, at a sequence length of 16k and a batch size of 2, the decoding time for the baseline model surpasses 0.1 seconds, whereas the \kv-optimized model consistently remains below 0.04 seconds, achieving approximately a 3.6x speedup. Second, with the same batch size, the model optimized with \kv can decode significantly longer sequences. For example, at a batch size of 2, the baseline model encounters an OOM issue beyond 16k input tokens, whereas the \kv-enhanced model extends this limit to 131k input tokens, indicating an approximately 8.2x improvement. This demonstrates \kv's effectiveness in minimizing memory consumption.
\subsection{Ablation Study of Effectiveness of Pooling}\label{sec:ablation}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/Mistral-7B-Instruct-v0.2_nopool.pdf}
    \includegraphics[width=0.9\textwidth]{figures/Mistral-7B-Instruct-v0.2_maxpool.pdf}
    \caption{Ablation study of pooling on LongEval-Lines. The evaluation includes inputs, each comprised of lines formatted as "\texttt{line makeshift-penguin: REGISTER\_CONTENT is <10536>}", where the key is an adjective-noun pair and the value is a random 5-digit number. The model needs to retrieve the value based on a given key. 
    The x-axis denotes the length of the input; the y-axis indicates the position of the groundtruth, from 5K to 30K tokens. With the pooling, the model can retrieve correct values before 16k and performs significantly better than the one without pooling.
    }
    \label{fig: ablation}
\end{figure}
We perform an ablation study to assess the impact of our pooling technique, a straightforward but efficient method for consolidating information through clustering. Our evaluation utilizes the modified LongEval-Lines benchmark~\cite{longchat2023}, incorporating random generated pairs and averaged scores. LongEval-Lines presents a greater challenge compared to Needle-in-a-Haystack because it involves identifying key-value pairs in noisy contexts of the same format, while in Needle-in-a-Haystack, the relevant information is more distinctly separated from other contexts. We apply max pooling with a kernel size of 5 and use the observation window with a size of 16. The findings, illustrated in our results (Fig.~\ref{fig: ablation}), indicate that pooling significantly enhances retrieval accuracy compared to methods not utilizing pooling. We hypothesize that this is due to the ability of strong attention mechanisms to focus on the initial portion of tokens. Without information compression, large language models tend to replicate the subsequent tokens, leading to retrieved partially correct results when the KV cache is compressed as we observed. Note that throughout our experiments, the choice between max pooling and average pooling did not yield significant differences in performance.
\begin{table*}[t]

\fontsize{18}{24}\selectfont
\setlength{\tabcolsep}{5pt}
\centering
\caption{Performance comparison of \kv and H2O across various LLMs on LongBench.}\label{tab:longbench}
\begin{threeparttable}
\scalebox{0.3}{
\begin{tabular}{l|lcccccccccccccccc}
\specialrule{1pt}{0pt}{2pt}
&\multirow{4}{*}{~~~LLMs\tnote{a}} & \multicolumn{3}{c}{Single-Document QA} & \multicolumn{3}{c}{Multi-Document QA}& \multicolumn{3}{c}{Summarization}& \multicolumn{3}{c}{Few-shot Learning}& \multicolumn{2}{c}{Synthetic} & \multicolumn{2}{c}{Code} \\
\cmidrule(lr){3-5}\cmidrule(lr){6-8}\cmidrule(lr){9-11}\cmidrule(lr){12-14}\cmidrule(lr){15-16}\cmidrule(lr){17-18}
&& \rotatebox[origin=c]{30}{NrtvQA} & \rotatebox[origin=c]{30}{Qasper} & \rotatebox[origin=c]{30}{MF-en} & \rotatebox[origin=c]{30}{HotpotQA} & \rotatebox[origin=c]{30}{2WikiMQA} & \rotatebox[origin=c]{30}{Musique} & \rotatebox[origin=c]{30}{GovReport} & \rotatebox[origin=c]{30}{QMSum} & \rotatebox[origin=c]{30}{MultiNews} & \rotatebox[origin=c]{30}{TREC} & \rotatebox[origin=c]{30}{TriviaQA} & \rotatebox[origin=c]{30}{SAMSum} & \rotatebox[origin=c]{30}{PCount} & \rotatebox[origin=c]{30}{PRe} & \rotatebox[origin=c]{30}{Lcc} & \rotatebox[origin=c]{30}{RB-P} \\

\specialrule{1pt}{2pt}{2pt}

\multirow{5}{*}{\rotatebox[origin=c]{90}{\fontsize{18}{100}\selectfont LWMChat}}

&\cellcolor{green!10}~~~All KV & \cellcolor{green!10}\textbf{18.18}&\cellcolor{green!10}\textbf{25.56}&\cellcolor{green!10} 40.94 &\cellcolor{green!10} 24.57 &\cellcolor{green!10} 19.39&\cellcolor{green!10} 10.49 &\cellcolor{green!10} \textbf{27.97} & \cellcolor{green!10}24.9 &\cellcolor{green!10} \textbf{24.81} &\cellcolor{green!10}71.0&\cellcolor{green!10} 60.9 & \cellcolor{green!10} 39.73 &\cellcolor{green!10} 3.17 &\cellcolor{green!10}3.5 & \cellcolor{green!10}44.4 & \cellcolor{green!10}43.82\\
\cline{2-18}

&\cellcolor{green!10}~~~\kv: 1024 & \cellcolor{green!10}18.02&\cellcolor{green!10}23.73&\cellcolor{green!10} 40.25 &\cellcolor{green!10} 24.61 &\cellcolor{green!10} \textbf{19.84}&\cellcolor{green!10} 10.77 &\cellcolor{green!10} 19.79 & \cellcolor{green!10}24.44 &\cellcolor{green!10} 23.53 &\cellcolor{green!10} 70.0 &\cellcolor{green!10} \textbf{61.42} & \cellcolor{green!10} 39.64 &\cellcolor{green!10} 1.67 &\cellcolor{green!10}3.0 & \cellcolor{green!10}43.34 & \cellcolor{green!10}44.0\\

What is the GitHub repository for SnapKV?