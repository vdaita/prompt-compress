{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea here is that I want to guess what are the most important keys ahead of time ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# For boba\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf60b376f764e7b9331a87841e995af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.float16, attn_implementation='sdpa', device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from transformers.cache_utils import Cache\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, repeat_kv\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.compile # probablty shouldn't include this the first few times?\n",
    "def low_dim_get_attention_scores(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, block_size: int = 32, dim: int = 32, top_ratio = 4): # then, reduce this again to 8 and 64 after determining stuff and calculate the real deal\n",
    "    num_tokens = k.shape[-2]\n",
    "    \n",
    "    Bk, Hk, Tk, Dk = q.shape\n",
    "    print(\"Queries shape: \", q.shape, \" Keys shape: \", k.shape, \" Values shape: \", v.shape)\n",
    "    # Dimension B, H, T, D for everything\n",
    "    q = q.transpose(-1, -2)\n",
    "    k = k.transpose(-1, -2)\n",
    "\n",
    "    q = q.view(Bk * Hk, Dk, Tk)\n",
    "    k = k.view(Bk * Hk, Dk, Tk)\n",
    "\n",
    "    q_chunks = F.avg_pool1d(q, kernel_size=block_size, stride=block_size)\n",
    "    k_chunks = F.avg_pool1d(k, kernel_size=block_size, stride=block_size) \n",
    "    q_chunks = q_chunks.transpose(-1, -2)\n",
    "    k_chunks = k_chunks.transpose(-1, -2)\n",
    "\n",
    "    # q_chunks = q_chunks.view(Bk, Hk, Tk // block_size, Dk)\n",
    "    # k_chunks = q_chunks.view(Bk, Hk, Tk // block_size, Dk) -> reviewing them can happen later\n",
    "\n",
    "    print(\"Q chunks shape: \", q_chunks.shape, \" K chunks shape: \", k_chunks.shape)\n",
    "\n",
    "    # Move q, k into lower dimension with random matrices\n",
    "    random_matrix = torch.randn(Dk, dim, device=q_chunks.device)\n",
    "    q_ldim = torch.matmul(q_chunks, random_matrix)\n",
    "    k_ldim = torch.matmul(k_chunks, random_matrix)\n",
    "    print(\"Lower dim matrix shapes: \", q_ldim.shape, k_ldim.shape)\n",
    "\n",
    "    # Attention scores\n",
    "    approx_scores = F.softmax(torch.bmm(q_ldim, k_ldim.transpose(-1, -2)), dim=-1) # Add causal attention mask\n",
    "    approx_scores = torch.tril(approx_scores)\n",
    "    print(\"Attention map: \", approx_scores.shape)\n",
    "    # The keys should be in dim -2\n",
    "\n",
    "    num_top_block_count = (num_tokens // block_size // top_ratio)\n",
    "    print(\"Top block count: \", num_top_block_count)\n",
    "    top_blocks = torch.sort(torch.topk(approx_scores, num_top_block_count, dim=-2).indices).values\n",
    "    print(\"Top blocks shape: \", top_blocks.shape)\n",
    "    print(\"Top blocks: \", top_blocks)\n",
    "    repeated_indices = top_blocks.unsqueeze(-1).expand(-1, -1, -1, block_size)\n",
    "    block_offsets = torch.arange(block_size, device=top_blocks.device).view(1, 1, 1, block_size)\n",
    "    expanded_top_blocks = repeated_indices + block_offsets\n",
    "    print(\"Expanded top blocks shape: \", expanded_top_blocks)\n",
    "\n",
    "    k_blocks = torch.gather(k, dim=-2, indices=expanded_top_blocks) # -2 says that this is in the token_length portion\n",
    "    v_blocks = torch.gather(v, dim=-2, indices=expanded_top_blocks)\n",
    "    print(\"K blocks initial shape : \", k_blocks.shape, \" V blocks initial shape: \", k_blocks.shape)\n",
    "    # expanded_top_blocks = expanded_top_blocks.view(*top_blocks.shape[:-1], -1) # the same number of blocks are being viewed, no dynamic length\n",
    "    q_blocks = torch.reshape(q, (Bk, Hk, -1, block_size, Dk))\n",
    "    k_blocks = torch.reshape(k_blocks, (Bk, Hk, -1, num_top_block_count, Dk))\n",
    "    v_blocks = torch.reshape(v_blocks, (Bk, Hk, -1, num_top_block_count, Dk))\n",
    "\n",
    "    print(\"Reshaped blocks: \", q_blocks.shape, k_blocks.shape, v_blocks.shape)\n",
    "    \n",
    "    return q_blocks, k_blocks, v_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdpa_forward(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_value: Optional[Cache] = None,\n",
    "    output_attentions: bool = False,\n",
    "    use_cache: bool = False,\n",
    ") -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "    # print(\"Running custom forward function\")\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "    query_states = self.q_proj(hidden_states)\n",
    "    key_states = self.k_proj(hidden_states)\n",
    "    value_states = self.v_proj(hidden_states)\n",
    "\n",
    "    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    kv_seq_len = key_states.shape[-2]\n",
    "    if past_key_value is not None:\n",
    "        kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n",
    "    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
    "\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "\n",
    "    if past_key_value is not None:\n",
    "        cache_kwargs = {\"sin\": sin, \"cos\": cos}  # Specific to RoPE models\n",
    "        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "    key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "    value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "    if attention_mask is not None:\n",
    "        if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
    "            )\n",
    "\n",
    "    # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n",
    "    # Reference: https://github.com/pytorch/pytorch/issues/112577.\n",
    "    if query_states.device.type == \"cuda\" and attention_mask is not None:\n",
    "        query_states = query_states.contiguous()\n",
    "        key_states = key_states.contiguous()\n",
    "        value_states = value_states.contiguous()\n",
    "\n",
    "    # generate weights for which tokens are important or not\n",
    "\n",
    "    # Apply padding to q and k\n",
    "    max_block_size = 32\n",
    "    k_len = key_states.shape[-2]\n",
    "    pad_len_q = 0 if (q_len % max_block_size) == 0 else (max_block_size - (q_len % max_block_size))\n",
    "    pad_len_k = 0 if (k_len % max_block_size) == 0 else (max_block_size - (k_len % max_block_size))\n",
    "    if pad_len_q > 0:\n",
    "        bq, hq, tq, dq = query_states.shape\n",
    "        pad_matrix = torch.zeros((bq, hq, pad_len_q, dq), device=query_states.device)\n",
    "        query_states = torch.cat((query_states, pad_matrix), dim=-2)  # Padding on the last dimension\n",
    "    if pad_len_k > 0:\n",
    "        bk, hk, tk, dk = key_states.shape\n",
    "        pad_matrix = torch.zeros((bk, hk, pad_len_k, dk), device=query_states.device)\n",
    "        key_states = torch.cat((key_states, pad_matrix), dim=-2)  # Padding on the last dimension\n",
    "        value_states = torch.cat((value_states, pad_matrix), dim=-2)\n",
    "    print(\"Padding length: \", pad_len_q, pad_len_k)\n",
    "    q_blocks, k_blocks, v_blocks = low_dim_get_attention_scores(query_states, key_states, value_states)\n",
    "\n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        q_blocks,\n",
    "        k_blocks,\n",
    "        v_blocks,\n",
    "        attn_mask=attention_mask,\n",
    "        dropout_p=self.attention_dropout if self.training else 0.0,\n",
    "        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "        is_causal=self.is_causal and attention_mask is None and q_len > 1,\n",
    "    )\n",
    "\n",
    "    # TODO: rejoin the output to fit the desired size.\n",
    "\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size) # q_len is padded to be of the same length for everything\n",
    "\n",
    "    attn_output = self.o_proj(attn_output)\n",
    "\n",
    "    return attn_output, None, past_key_value\n",
    "\n",
    "for i in range(len(model.model.layers)):\n",
    "    model.model.layers[i].self_attn.forward = sdpa_forward.__get__(model.model.layers[i].self_attn, type(model.model.layers[i].self_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding length:  15 15\n",
      "Queries shape:  torch.Size([1, 32, 17600, 128])  Keys shape:  torch.Size([1, 32, 17600, 128])  Values shape:  torch.Size([1, 32, 17600, 128])\n",
      "Q chunks shape:  torch.Size([32, 550, 128])  K chunks shape:  torch.Size([32, 550, 128])\n",
      "Lower dim matrix shapes:  torch.Size([32, 550, 32]) torch.Size([32, 550, 32])\n",
      "Attention map:  torch.Size([32, 550, 550])\n",
      "Top block count:  137\n",
      "Top blocks shape:  torch.Size([32, 137, 550])\n",
      "Top blocks:  tensor([[[  0,   0,   0,  ..., 549, 549, 549],\n",
      "         [  0,   0,   0,  ..., 549, 549, 549],\n",
      "         [  0,   0,   0,  ..., 547, 549, 549],\n",
      "         ...,\n",
      "         [115, 118, 120,  ..., 134, 134, 134],\n",
      "         [116, 119, 121,  ..., 135, 135, 135],\n",
      "         [117, 120, 122,  ..., 136, 136, 136]],\n",
      "\n",
      "        [[  0,   0,   0,  ..., 548, 549, 549],\n",
      "         [  0,   0,   0,  ..., 549, 549, 549],\n",
      "         [  0,   0,   0,  ..., 549, 549, 549],\n",
      "         ...,\n",
      "         [ 71,  77,  82,  ..., 134, 134, 134],\n",
      "         [ 72,  78,  83,  ..., 135, 135, 135],\n",
      "         [ 73,  79,  84,  ..., 136, 136, 136]],\n",
      "\n",
      "        [[  2,   5,   5,  ..., 549, 549, 549],\n",
      "         [  0,   0,   6,  ..., 549, 549, 549],\n",
      "         [  0,   0,   1,  ..., 549, 549, 549],\n",
      "         ...,\n",
      "         [  0,   0,   1,  ..., 475, 489, 531],\n",
      "         [  0,   1,   1,  ..., 482, 504, 540],\n",
      "         [  0,   0,   1,  ..., 482, 500, 526]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  0,   0,   0,  ..., 549, 549, 549],\n",
      "         [  0,   0,   0,  ..., 539, 545, 548],\n",
      "         [  0,   0,   0,  ..., 535, 535, 538],\n",
      "         ...,\n",
      "         [ 97, 117, 117,  ..., 134, 134, 134],\n",
      "         [ 98, 118, 118,  ..., 135, 135, 135],\n",
      "         [ 99, 119, 119,  ..., 136, 136, 136]],\n",
      "\n",
      "        [[  0,   0,   0,  ..., 549, 549, 549],\n",
      "         [  0,   0,   0,  ..., 547, 549, 549],\n",
      "         [  0,   0,   0,  ..., 545, 547, 547],\n",
      "         ...,\n",
      "         [ 89,  97,  98,  ..., 134, 134, 134],\n",
      "         [ 90,  98,  99,  ..., 135, 135, 135],\n",
      "         [ 91,  99, 100,  ..., 136, 136, 136]],\n",
      "\n",
      "        [[  0,   0,   0,  ..., 548, 549, 549],\n",
      "         [  0,   0,   0,  ..., 549, 549, 549],\n",
      "         [  0,   0,   0,  ..., 549, 549, 549],\n",
      "         ...,\n",
      "         [  2,   7,   9,  ..., 134, 134, 134],\n",
      "         [  3,   8,  10,  ..., 135, 135, 135],\n",
      "         [  4,   9,  11,  ..., 136, 136, 136]]], device='cuda:0')\n",
      "Expanded top blocks shape:  tensor([[[[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          ...,\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580]],\n",
      "\n",
      "         [[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          ...,\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580]],\n",
      "\n",
      "         [[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          ...,\n",
      "          [547, 548, 549,  ..., 576, 577, 578],\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[115, 116, 117,  ..., 144, 145, 146],\n",
      "          [118, 119, 120,  ..., 147, 148, 149],\n",
      "          [120, 121, 122,  ..., 149, 150, 151],\n",
      "          ...,\n",
      "          [134, 135, 136,  ..., 163, 164, 165],\n",
      "          [134, 135, 136,  ..., 163, 164, 165],\n",
      "          [134, 135, 136,  ..., 163, 164, 165]],\n",
      "\n",
      "         [[116, 117, 118,  ..., 145, 146, 147],\n",
      "          [119, 120, 121,  ..., 148, 149, 150],\n",
      "          [121, 122, 123,  ..., 150, 151, 152],\n",
      "          ...,\n",
      "          [135, 136, 137,  ..., 164, 165, 166],\n",
      "          [135, 136, 137,  ..., 164, 165, 166],\n",
      "          [135, 136, 137,  ..., 164, 165, 166]],\n",
      "\n",
      "         [[117, 118, 119,  ..., 146, 147, 148],\n",
      "          [120, 121, 122,  ..., 149, 150, 151],\n",
      "          [122, 123, 124,  ..., 151, 152, 153],\n",
      "          ...,\n",
      "          [136, 137, 138,  ..., 165, 166, 167],\n",
      "          [136, 137, 138,  ..., 165, 166, 167],\n",
      "          [136, 137, 138,  ..., 165, 166, 167]]],\n",
      "\n",
      "\n",
      "        [[[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          ...,\n",
      "          [548, 549, 550,  ..., 577, 578, 579],\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580]],\n",
      "\n",
      "         [[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          ...,\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580]],\n",
      "\n",
      "         [[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          ...,\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 71,  72,  73,  ..., 100, 101, 102],\n",
      "          [ 77,  78,  79,  ..., 106, 107, 108],\n",
      "          [ 82,  83,  84,  ..., 111, 112, 113],\n",
      "          ...,\n",
      "          [134, 135, 136,  ..., 163, 164, 165],\n",
      "          [134, 135, 136,  ..., 163, 164, 165],\n",
      "          [134, 135, 136,  ..., 163, 164, 165]],\n",
      "\n",
      "         [[ 72,  73,  74,  ..., 101, 102, 103],\n",
      "          [ 78,  79,  80,  ..., 107, 108, 109],\n",
      "          [ 83,  84,  85,  ..., 112, 113, 114],\n",
      "          ...,\n",
      "          [135, 136, 137,  ..., 164, 165, 166],\n",
      "          [135, 136, 137,  ..., 164, 165, 166],\n",
      "          [135, 136, 137,  ..., 164, 165, 166]],\n",
      "\n",
      "         [[ 73,  74,  75,  ..., 102, 103, 104],\n",
      "          [ 79,  80,  81,  ..., 108, 109, 110],\n",
      "          [ 84,  85,  86,  ..., 113, 114, 115],\n",
      "          ...,\n",
      "          [136, 137, 138,  ..., 165, 166, 167],\n",
      "          [136, 137, 138,  ..., 165, 166, 167],\n",
      "          [136, 137, 138,  ..., 165, 166, 167]]],\n",
      "\n",
      "\n",
      "        [[[  2,   3,   4,  ...,  31,  32,  33],\n",
      "          [  5,   6,   7,  ...,  34,  35,  36],\n",
      "          [  5,   6,   7,  ...,  34,  35,  36],\n",
      "          ...,\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580]],\n",
      "\n",
      "         [[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  6,   7,   8,  ...,  35,  36,  37],\n",
      "          ...,\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580]],\n",
      "\n",
      "         [[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  1,   2,   3,  ...,  30,  31,  32],\n",
      "          ...,\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  1,   2,   3,  ...,  30,  31,  32],\n",
      "          ...,\n",
      "          [475, 476, 477,  ..., 504, 505, 506],\n",
      "          [489, 490, 491,  ..., 518, 519, 520],\n",
      "          [531, 532, 533,  ..., 560, 561, 562]],\n",
      "\n",
      "         [[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  1,   2,   3,  ...,  30,  31,  32],\n",
      "          [  1,   2,   3,  ...,  30,  31,  32],\n",
      "          ...,\n",
      "          [482, 483, 484,  ..., 511, 512, 513],\n",
      "          [504, 505, 506,  ..., 533, 534, 535],\n",
      "          [540, 541, 542,  ..., 569, 570, 571]],\n",
      "\n",
      "         [[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  1,   2,   3,  ...,  30,  31,  32],\n",
      "          ...,\n",
      "          [482, 483, 484,  ..., 511, 512, 513],\n",
      "          [500, 501, 502,  ..., 529, 530, 531],\n",
      "          [526, 527, 528,  ..., 555, 556, 557]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          ...,\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580]],\n",
      "\n",
      "         [[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          ...,\n",
      "          [539, 540, 541,  ..., 568, 569, 570],\n",
      "          [545, 546, 547,  ..., 574, 575, 576],\n",
      "          [548, 549, 550,  ..., 577, 578, 579]],\n",
      "\n",
      "         [[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          ...,\n",
      "          [535, 536, 537,  ..., 564, 565, 566],\n",
      "          [535, 536, 537,  ..., 564, 565, 566],\n",
      "          [538, 539, 540,  ..., 567, 568, 569]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 97,  98,  99,  ..., 126, 127, 128],\n",
      "          [117, 118, 119,  ..., 146, 147, 148],\n",
      "          [117, 118, 119,  ..., 146, 147, 148],\n",
      "          ...,\n",
      "          [134, 135, 136,  ..., 163, 164, 165],\n",
      "          [134, 135, 136,  ..., 163, 164, 165],\n",
      "          [134, 135, 136,  ..., 163, 164, 165]],\n",
      "\n",
      "         [[ 98,  99, 100,  ..., 127, 128, 129],\n",
      "          [118, 119, 120,  ..., 147, 148, 149],\n",
      "          [118, 119, 120,  ..., 147, 148, 149],\n",
      "          ...,\n",
      "          [135, 136, 137,  ..., 164, 165, 166],\n",
      "          [135, 136, 137,  ..., 164, 165, 166],\n",
      "          [135, 136, 137,  ..., 164, 165, 166]],\n",
      "\n",
      "         [[ 99, 100, 101,  ..., 128, 129, 130],\n",
      "          [119, 120, 121,  ..., 148, 149, 150],\n",
      "          [119, 120, 121,  ..., 148, 149, 150],\n",
      "          ...,\n",
      "          [136, 137, 138,  ..., 165, 166, 167],\n",
      "          [136, 137, 138,  ..., 165, 166, 167],\n",
      "          [136, 137, 138,  ..., 165, 166, 167]]],\n",
      "\n",
      "\n",
      "        [[[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          ...,\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580]],\n",
      "\n",
      "         [[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          ...,\n",
      "          [547, 548, 549,  ..., 576, 577, 578],\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580]],\n",
      "\n",
      "         [[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          ...,\n",
      "          [545, 546, 547,  ..., 574, 575, 576],\n",
      "          [547, 548, 549,  ..., 576, 577, 578],\n",
      "          [547, 548, 549,  ..., 576, 577, 578]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 89,  90,  91,  ..., 118, 119, 120],\n",
      "          [ 97,  98,  99,  ..., 126, 127, 128],\n",
      "          [ 98,  99, 100,  ..., 127, 128, 129],\n",
      "          ...,\n",
      "          [134, 135, 136,  ..., 163, 164, 165],\n",
      "          [134, 135, 136,  ..., 163, 164, 165],\n",
      "          [134, 135, 136,  ..., 163, 164, 165]],\n",
      "\n",
      "         [[ 90,  91,  92,  ..., 119, 120, 121],\n",
      "          [ 98,  99, 100,  ..., 127, 128, 129],\n",
      "          [ 99, 100, 101,  ..., 128, 129, 130],\n",
      "          ...,\n",
      "          [135, 136, 137,  ..., 164, 165, 166],\n",
      "          [135, 136, 137,  ..., 164, 165, 166],\n",
      "          [135, 136, 137,  ..., 164, 165, 166]],\n",
      "\n",
      "         [[ 91,  92,  93,  ..., 120, 121, 122],\n",
      "          [ 99, 100, 101,  ..., 128, 129, 130],\n",
      "          [100, 101, 102,  ..., 129, 130, 131],\n",
      "          ...,\n",
      "          [136, 137, 138,  ..., 165, 166, 167],\n",
      "          [136, 137, 138,  ..., 165, 166, 167],\n",
      "          [136, 137, 138,  ..., 165, 166, 167]]],\n",
      "\n",
      "\n",
      "        [[[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          ...,\n",
      "          [548, 549, 550,  ..., 577, 578, 579],\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580]],\n",
      "\n",
      "         [[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          ...,\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580]],\n",
      "\n",
      "         [[  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          [  0,   1,   2,  ...,  29,  30,  31],\n",
      "          ...,\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580],\n",
      "          [549, 550, 551,  ..., 578, 579, 580]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  2,   3,   4,  ...,  31,  32,  33],\n",
      "          [  7,   8,   9,  ...,  36,  37,  38],\n",
      "          [  9,  10,  11,  ...,  38,  39,  40],\n",
      "          ...,\n",
      "          [134, 135, 136,  ..., 163, 164, 165],\n",
      "          [134, 135, 136,  ..., 163, 164, 165],\n",
      "          [134, 135, 136,  ..., 163, 164, 165]],\n",
      "\n",
      "         [[  3,   4,   5,  ...,  32,  33,  34],\n",
      "          [  8,   9,  10,  ...,  37,  38,  39],\n",
      "          [ 10,  11,  12,  ...,  39,  40,  41],\n",
      "          ...,\n",
      "          [135, 136, 137,  ..., 164, 165, 166],\n",
      "          [135, 136, 137,  ..., 164, 165, 166],\n",
      "          [135, 136, 137,  ..., 164, 165, 166]],\n",
      "\n",
      "         [[  4,   5,   6,  ...,  33,  34,  35],\n",
      "          [  9,  10,  11,  ...,  38,  39,  40],\n",
      "          [ 11,  12,  13,  ...,  40,  41,  42],\n",
      "          ...,\n",
      "          [136, 137, 138,  ..., 165, 166, 167],\n",
      "          [136, 137, 138,  ..., 165, 166, 167],\n",
      "          [136, 137, 138,  ..., 165, 166, 167]]]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "gather() received an invalid combination of arguments - got (Tensor, indices=Tensor, dim=int), but expected one of:\n * (Tensor input, int dim, Tensor index, *, bool sparse_grad = False, Tensor out = None)\n * (Tensor input, name dim, Tensor index, *, bool sparse_grad = False, Tensor out = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# find a way to start by chunking this portion\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# there shouuld be a mix between the global attention and the attention that only the last few tokens (query) pay\u001b[39;00m\n\u001b[1;32m     10\u001b[0m past_key_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/snapkv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1525\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1518\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1519\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1520\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1521\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1522\u001b[0m     )\n\u001b[1;32m   1524\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1542\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1543\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1548\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1549\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2622\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2619\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2621\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2622\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2623\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2625\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2630\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/snapkv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/snapkv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1183\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1180\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/snapkv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/snapkv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1070\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1061\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1062\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         use_cache,\n\u001b[1;32m   1068\u001b[0m     )\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1070\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1079\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/snapkv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/snapkv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:798\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/snapkv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/snapkv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 65\u001b[0m, in \u001b[0;36msdpa_forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m     63\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((value_states, pad_matrix), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPadding length: \u001b[39m\u001b[38;5;124m\"\u001b[39m, pad_len_q, pad_len_k)\n\u001b[0;32m---> 65\u001b[0m q_blocks, k_blocks, v_blocks \u001b[38;5;241m=\u001b[39m \u001b[43mlow_dim_get_attention_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[1;32m     68\u001b[0m     q_blocks,\n\u001b[1;32m     69\u001b[0m     k_blocks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_causal \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m q_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# TODO: rejoin the output to fit the desired size.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m, in \u001b[0;36mlow_dim_get_attention_scores\u001b[0;34m(q, k, v, block_size, dim, top_ratio)\u001b[0m\n\u001b[1;32m     43\u001b[0m expanded_top_blocks \u001b[38;5;241m=\u001b[39m repeated_indices \u001b[38;5;241m+\u001b[39m block_offsets\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpanded top blocks shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, expanded_top_blocks)\n\u001b[0;32m---> 46\u001b[0m k_blocks \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpanded_top_blocks\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# -2 says that this is in the token_length portion\u001b[39;00m\n\u001b[1;32m     47\u001b[0m v_blocks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(v, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, indices\u001b[38;5;241m=\u001b[39mexpanded_top_blocks)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK blocks initial shape : \u001b[39m\u001b[38;5;124m\"\u001b[39m, k_blocks\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m V blocks initial shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, k_blocks\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: gather() received an invalid combination of arguments - got (Tensor, indices=Tensor, dim=int), but expected one of:\n * (Tensor input, int dim, Tensor index, *, bool sparse_grad = False, Tensor out = None)\n * (Tensor input, name dim, Tensor index, *, bool sparse_grad = False, Tensor out = None)\n"
     ]
    }
   ],
   "source": [
    "sample_text = open(\"snapkv_full.txt\", \"r\", encoding=\"utf-8\").read()\n",
    "encoded_tokens = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "for key in encoded_tokens:\n",
    "    encoded_tokens[key] = encoded_tokens[key].cuda()\n",
    "\n",
    "# find a way to start by chunking this portion\n",
    "\n",
    "# there shouuld be a mix between the global attention and the attention that only the last few tokens (query) pay\n",
    "\n",
    "past_key_values = None\n",
    "outputs = model.generate(**encoded_tokens, output_attentions=True, return_dict_in_generate=True, max_new_tokens=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snapkv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
